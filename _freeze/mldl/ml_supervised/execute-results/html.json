{
  "hash": "5b1e918d7eb9072f947343eceaeddcd7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Feature Engineering and Selection\nexecute:\n  warning: false\n  error: false\nsidebar:\n  contents: auto\nnumber-sections: true\n---\n\n\n\n\n\n\n\n\n# Library and Data\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\ntheme_set(theme_bw())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncolor_RUB_blue <- \"#17365c\"\ncolor_RUB_green <- \"#8dae10\"\ncolor_TUD_pink <- \"#EC008D\"\n\ncolor_DRESDEN <- c(\"#03305D\", \"#28618C\", \"#539DC5\", \"#84D1EE\", \"#009BA4\", \"#13A983\", \"#93C356\", \"#BCCF02\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_Bochum_KL <- read_csv(\"../data_share/df_Bochum_KL.csv\")\n# Train–test split (75% / 25%)\nsplit_Bochum <- initial_split(df_Bochum_KL, prop = 3/4)\ndf_Train <- training(split_Bochum)\ndf_Test  <- testing(split_Bochum)\n\n\n# Preprocessing recipe (normalize + PCA)\n\nrcp_Norm <- \n  recipe(evapo_r ~ ., data = df_Train) |>\n  step_YeoJohnson(all_predictors()) |> \n  step_normalize(all_predictors())\n```\n:::\n\n\n\n\n\n\n\n# Tuning parameter\n\ngrid search and iterative search.\n\n\n## Grid Search\n\nGrid search is when we predefine a set of parameter values to evaluate. The main choices involved in grid search are how to make the grid and how many parameter combinations to evaluate. Grid search is often judged as inefficient since the number of grid points required to cover the parameter space can become unmanageable with the curse of dimensionality. There is truth to this concern, but it is most true when the process is not optimized. This is discussed more in Chapter 13.\n\nIterative search or sequential search is when we sequentially discover new parameter combinations based on previous results. Almost any nonlinear optimization method is appropriate, although some are more efficient than others. In some cases, an initial set of results for one or more parameter combinations is required to start the optimization process. Iterative search is discussed more in Chapter 14.\n\n\n\n\n\n# Linear regression\n\nLinear regression is a classical supervised learning method used to estimate the linear relationship between one or more predictors and a continuous outcome variable.  \nIn practice, most applications involve several predictors, so the standard approach is the *multiple linear regression* model, where the response is modeled as a linear combination of all predictors.\n\nFor predictors $x_1, x_2, ..., x_p$, the model is:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\varepsilon\n$$\n\nwhere  \n- $y$ is the outcome,  \n- $\\beta_0$ is the intercept,  \n- $\\beta_j$ are the coefficients showing the contribution of each predictor,  \n- $\\varepsilon$ is the random error.\n\nThe goal is to estimate the coefficients $\\beta$ so that the model predicts $y$ as well as possible.\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# linear regression --------------\n\nmdl_Mlm <- \n  linear_reg() |>\n  set_engine(\"lm\")\n\nwflow_Norm_Mlm <- \n  workflow() |>\n  add_recipe(rcp_Norm) |>\n  add_model(mdl_Mlm)\n\nfit_Norm_Mlm <- \n  wflow_Norm_Mlm |>\n  fit(data = df_Train)\n\npred_Mlm <- \n  predict(fit_Norm_Mlm, df_Test) |>\n  bind_cols(df_Test |> select(evapo_r))\n\nmetrics(pred_Mlm, truth = evapo_r, estimate = .pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4.04 \n2 rsq     standard       0.813\n3 mae     standard       3.05 \n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(pred_Mlm, aes(x = evapo_r, y = .pred)) +\n  geom_point(alpha = 0.6, color = color_RUB_blue) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = color_TUD_pink) +\n  labs(x = \"Observed\", y = \"Predicted\")\n```\n\n::: {.cell-output-display}\n![](ml_supervised_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Hyperparameter\n\nFor ordinary linear regression (using the `lm` engine), there are no hyperparameters to tune.  \nHowever, when we introduce regularization (ridge, lasso, elastic net), the model includes an additional penalty term in the loss function to prevent overfitting.  \nRegularization requires the model to control *how strong* the penalty is and *what type* of penalty is used.  \nThese controls are called **hyperparameters**, and two of them appear:\n\n- `penalty` (lambda): determines the overall strength of the regularization  \n- `mixture` (alpha): determines the type of regularization (0 = ridge, 1 = lasso, values in between = elastic net)\n\nWithout regularization (ordinary linear regression), no hyperparameters are needed.\n\nThese hyperparameters must usually be tuned using cross-validation.\n\n\n\n\nDifferent regression methods use different *loss functions* and *regularization strategies*.  \nThe key difference is whether the model introduces **hyperparameters** (`penalty` and `mixture`) that control the amount and type of regularization.\n\n### Ordinary Least Squares (OLS)\n- No regularization → **no hyperparameters**.  \n- The solution is obtained analytically by minimizing the sum of squared residuals.\n\n$$\n\\min_{\\beta} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n$$\n\nBecause there is no penalty term, the model can overfit when predictors are highly correlated.\n\n### Ridge Regression (L2 penalty)\n- Introduces **one hyperparameter**:  \n  - `penalty` (lambda): strength of coefficient shrinkage  \n- `mixture` is fixed to **0**, meaning pure L2 regularization.\n\n$$\n\\min_{\\beta} \\left(\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n+ \\lambda \\sum_{j=1}^p \\beta_j^2\n\\right)\n$$\n\nLarge lambda → strong shrinkage.  \nCoefficients approach zero but never become exactly zero.\n\n### Lasso Regression (L1 penalty)\n- Also uses **one hyperparameter**:  \n  - `penalty` (lambda): controls how strongly coefficients are pushed toward zero  \n- `mixture` is fixed to **1**, meaning pure L1 regularization.\n\n$$\n\\min_{\\beta} \\left(\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n+ \\lambda \\sum_{j=1}^p |\\beta_j|\n\\right)\n$$\n\nLarge lambda → many coefficients become exactly zero → feature selection.\n\n### Elastic Net Regression (combined L1 + L2)\n- Uses **two hyperparameters**:\n  - `penalty` (lambda): global strength of shrinkage  \n  - `mixture` (alpha): balance between L1 and L2  \n    - alpha = 0 → ridge  \n    - alpha = 1 → lasso  \n    - 0 < alpha < 1 → mixture\n\n$$\n\\min_{\\beta} \\left(\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n+ \\lambda \\left[\n\\alpha \\sum_{j=1}^p |\\beta_j|\n+ (1-\\alpha) \\sum_{j=1}^p \\beta_j^2\n\\right]\n\\right)\n$$\n\nElastic Net is useful when predictors are correlated and when both shrinkage and feature selection are desired.\n\n### Summary of Hyperparameter Settings\n\n| Method        | penalty (lambda) | mixture (alpha) | Regularization |\n|---------------|------------------|------------------|----------------|\n| OLS           | not used         | not used         | none |\n| Ridge         | yes              | 0                | L2 |\n| Lasso         | yes              | 1                | L1 |\n| Elastic Net   | yes              | 0 < α < 1        | L1 + L2 |\n\nIn practice, `penalty` and `mixture` are usually selected using **cross-validation**, since the optimal values depend on the dataset.\n\n\n\n\n\n\n",
    "supporting": [
      "ml_supervised_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}