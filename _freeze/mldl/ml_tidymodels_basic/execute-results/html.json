{
  "hash": "efb9222041e2e67e3588eed34cddeb6a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: ML Workflow with `tidymodels`\nexecute:\n  warning: false\n  error: false\nsidebar:\n  contents: auto\nnumber-sections: true\n---\n\n\n\n\n\n\n# Libraries\n\nIn this article we use the `tidymodels` ecosystem to illustrate a complete machine learning workflow in a tidy, coherent, and reproducible manner.  \nThroughout the article, we revisit the fundamental components of the ML workflow:\n\n1. Data split \n2. Feature engineering  \n3. Model building  \n4. Model fitting \n5. Model prediction \n6. Model evaluation\n\nWe begin with the most basic workflow and later extend it using the `recipe` framework for more advanced preprocessing.\n\nThe material in this article is based on the case study provided at: [https://www.tidymodels.org/start/case-study/](https://www.tidymodels.org/start/case-study/).  All exercises use the dataset included in that example.\n\nThe following packages are required:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\nlibrary(tidymodels)  # parsnip + other tidymodels packages\n\n# Helper packages\nlibrary(readr)       # for importing data\nlibrary(broom.mixed) # for tidying Bayesian model output\nlibrary(dotwhisker)  # for visualizing regression results\nlibrary(skimr)       # for variable summaries\n```\n:::\n\n\n\n\n\n\n\n# Basic Model Building and Fitting\n\nIn the first case, we will focus on three steps:\n\n1. **Exploring the data** – Understanding the structure, distributions, and relationships within the dataset to inform subsequent modeling decisions.  \n2. **Specifying a model** – Selecting an appropriate algorithm and defining its parameters based on the task and data characteristics.  \n3. **Fitting the model to the data** – Training the model using the available data to estimate its parameters and capture underlying patterns.\n\n## Data\n\nIn this exercise we explore the relationship between:\n\n* the outcome variable: **sea urchin width**\n* feature 1: **initial volume** (continuous)\n* feature 2: **food regime** (categorical)\n\nA basic model for these variables can be written as:\n\n```r\nwidth ~ initial_volume * food_regime\n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Data import ----------\ndf_Urchins <-\n  read_csv(\"https://tidymodels.org/start/models/urchins.csv\") |>\n  setNames(c(\"food_regime\", \"initial_volume\", \"width\")) |>\n  mutate(food_regime = factor(food_regime, levels = c(\"Initial\", \"Low\", \"High\")))\n\n## Explore the relationships ----------\nggplot(df_Urchins,\n       aes(x = initial_volume,\n           y = width,\n           group = food_regime,\n           col = food_regime)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)\n```\n\n::: {.cell-output-display}\n![](ml_tidymodels_basic_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Building the Model\n\nWe start with a simple linear regression model using `linear_reg()`.\nThe `tidymodels` ecosystem separates *model specification* (what model we want) from the *computational engine* (how it is estimated).\nThe `parsnip` package handles this abstraction.\n\nAvailable engines for `linear_reg()` are listed here:\n[https://parsnip.tidymodels.org/reference/linear_reg.html](https://parsnip.tidymodels.org/reference/linear_reg.html)\n\n1. Choose the model type:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n\n\n2. Choose the engine:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n\n\n3. Save the model specification:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmdl_Linear <- linear_reg()\n```\n:::\n\n\n\n\n\n\n## Training (Fitting) the Model\n\nTo train the model, we use the `fit()` function, which requires:\n\n1. the model specification\n2. a formula\n3. the training dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_Linear <-\n  mdl_Linear |>\n  fit(width ~ initial_volume * food_regime, data = df_Urchins)\n\nfit_Linear\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = width ~ initial_volume * food_regime, data = data)\n\nCoefficients:\n                   (Intercept)                  initial_volume  \n                     0.0331216                       0.0015546  \n                food_regimeLow                 food_regimeHigh  \n                     0.0197824                       0.0214111  \n initial_volume:food_regimeLow  initial_volume:food_regimeHigh  \n                    -0.0012594                       0.0005254  \n```\n\n\n:::\n:::\n\n\n\n\nWe can inspect the fitted model coefficients:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(fit_Linear)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  term                            estimate std.error statistic  p.value\n  <chr>                              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)                     0.0331    0.00962      3.44  0.00100 \n2 initial_volume                  0.00155   0.000398     3.91  0.000222\n3 food_regimeLow                  0.0198    0.0130       1.52  0.133   \n4 food_regimeHigh                 0.0214    0.0145       1.47  0.145   \n5 initial_volume:food_regimeLow  -0.00126   0.000510    -2.47  0.0162  \n6 initial_volume:food_regimeHigh  0.000525  0.000702     0.748 0.457   \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Predicting with the Fitted Model\n\nOnce a model is fitted, we can use it for prediction.\nTo make predictions we need:\n\n1. a dataset containing the feature variables (with matching names)\n2. the fitted model object\n\nBy default, `predict()` returns the mean prediction:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_NewData <- expand.grid(\n  initial_volume = 20,\n  food_regime = c(\"Initial\", \"Low\", \"High\")\n)\n\npred_Mean <- predict(fit_Linear, new_data = df_NewData)\npred_Mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 1\n   .pred\n   <dbl>\n1 0.0642\n2 0.0588\n3 0.0961\n```\n\n\n:::\n:::\n\n\n\n\nWe can also request confidence intervals using `type = \"conf_int\"`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_Conf <- predict(\n  fit_Linear,\n  new_data = df_NewData,\n  type = \"conf_int\"\n)\npred_Conf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  .pred_lower .pred_upper\n        <dbl>       <dbl>\n1      0.0555      0.0729\n2      0.0499      0.0678\n3      0.0870      0.105 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n# Data Processing with `recipes`\n\nThe `recipes` package provides a structured and extensible framework for data preprocessing and feature engineering within the `tidymodels` ecosystem.  \nIt allows us to define all preprocessing steps in a reproducible, model-agnostic workflow before model fitting.\n\nIn this exercise, we will focus on the following steps:\n\n1. **Data splitting** – Dividing the dataset into training, validation, and testing sets.  \n2. **Recipe defining**  \n   2.1 Basic definition – Specifying the target variable, predictors, and initial preprocessing steps.  \n   2.2 Feature engineering – Applying transformations, scaling, encoding, and other preprocessing techniques to enhance model performance.  \n3. **Model building** – Selecting and configuring the appropriate algorithm for the task.  \n4. **Combining recipe and model into a workflow** – Integrating preprocessing steps with the model into a unified pipeline.  \n5. **Model fitting** – Training the model on the training data using the defined workflow.  \n6. **Prediction** – Generating predictions on new or unseen data.  \n7. **Evaluation** – Assessing model performance using suitable metrics and interpreting the results to inform potential improvements.\n\n\n\n\n\n\n\n## Data\n\nIn this example we work with flight information from the United States (package `nycflights13`).  \nOur goal is to predict whether an arriving flight is **late** (arrival delay ≥ 30 minutes) or **on time**.\n\nThe modeling relationship can be summarized as:\n\n```r\narr_delay ~ (date, airport, distance, ...)\n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nycflights13)\nset.seed(123)\n\ndf_Flight <- \n  flights |> \n  mutate(\n    arr_delay = ifelse(arr_delay >= 30, \"late\", \"on_time\"),\n    arr_delay = factor(arr_delay),\n    date = lubridate::as_date(time_hour)\n  ) |> \n  inner_join(weather, by = c(\"origin\", \"time_hour\")) |> \n  select(dep_time, flight, origin, dest, air_time, distance,\n         carrier, date, arr_delay, time_hour) |> \n  na.omit() |> \n  mutate_if(is.character, as.factor)\n\ndf_Flight |> \n  skimr::skim(dest, carrier)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |          |\n|:------------------------|:---------|\n|Name                     |df_Flight |\n|Number of rows           |325819    |\n|Number of columns        |10        |\n|_______________________  |          |\n|Column type frequency:   |          |\n|factor                   |2         |\n|________________________ |          |\n|Group variables          |None      |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                                     |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------------------------------------|\n|dest          |         0|             1|FALSE   |      104|ATL: 16771, ORD: 16507, LAX: 15942, BOS: 14948 |\n|carrier       |         0|             1|FALSE   |       16|UA: 57489, B6: 53715, EV: 50868, DL: 47465     |\n\n\n:::\n:::\n\n\n\n\n\n\n## Data Split\n\nAs usual, we divide the dataset into **training** and **validation** sets.\nThis allows us to fit the model on one portion of the data and evaluate performance on unseen observations.\n\nUnder the `tidymodels` framework:\n\n1. `initial_split()` performs the random split.\n2. `training()` and `testing()` extract the corresponding sets.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(222)\nsplit_Flight <- initial_split(df_Flight, prop = 3/4)\n\ndf_Train <- training(split_Flight)\ndf_Validate <- testing(split_Flight)\n```\n:::\n\n\n\n\n\n\n## Creating a Recipe\n\n\n\n### Basical Recipe\n\nA **recipe** in `tidymodels` is a structured, pre-defined plan for **data preprocessing and feature engineering**.  \nIt allows us to declare, in advance, all the steps needed to transform raw data into model-ready features.\n\nWhen creating a recipe, two fundamental components must be specified:\n\n1. **Formula (`~`)**:  \n   The variable on the left-hand side is the **target/outcome**, and the variables on the right-hand side are the **predictors/features**.\n\n2. **Data**:  \n   The dataset is used to identify variable names and types. At this stage, no transformations are applied.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a basic recipe\nrcp_Flight <- recipe(arr_delay ~ ., data = df_Train)\n```\n:::\n\n\n\n\nUsing `.` on the right-hand side indicates that **all remaining variables** in the dataset should be used as predictors.\n\n\n\n\n### Roles\n\nIn addition to the standard roles (**outcome** and **predictor**), `recipes` allows defining custom roles using `update_role()`.\n\nCommon roles include:\n\n* `predictor`: variables used to predict the outcome\n* `outcome`: target variable\n* `ID`: identifiers not used for modeling\n* `case_weights`: observation weights\n* `datetime`, `latitude`, `longitude`: for temporal or spatial feature engineering\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrcp_Flight <- \n  recipe(arr_delay ~ ., data = df_Train) |> \n  update_role(flight, time_hour, new_role = \"ID\")\n\nsummary(rcp_Flight)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n   variable  type      role      source  \n   <chr>     <list>    <chr>     <chr>   \n 1 dep_time  <chr [2]> predictor original\n 2 flight    <chr [2]> ID        original\n 3 origin    <chr [3]> predictor original\n 4 dest      <chr [3]> predictor original\n 5 air_time  <chr [2]> predictor original\n 6 distance  <chr [2]> predictor original\n 7 carrier   <chr [3]> predictor original\n 8 date      <chr [1]> predictor original\n 9 time_hour <chr [1]> ID        original\n10 arr_delay <chr [3]> outcome   original\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Feature Engineering with `step_*`\n\n### Date Features\n\nFeature engineering transforms raw data into model-ready variables.\n`recipes` expresses each transformation as a `step_*()` function.\n\nBelow, we extract:\n\n- **day of week** (`dow`)\n- **month**\n- **holiday indicators** (using U.S. holidays)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrcp_Flight <- \n  recipe(arr_delay ~ ., data = df_Train) |> \n  update_role(flight, time_hour, new_role = \"ID\") |> \n  step_date(date, features = c(\"dow\", \"month\")) |>               \n  step_holiday(\n    date,\n    holidays = timeDate::listHolidays(\"US\"),\n    keep_original_cols = FALSE\n  )\n```\n:::\n\n\n\n\nThis replaces the original date with engineered calendar features.\n\n\n\n### Categorical Variables (Dummy Encoding)\n\nMost models require converting categorical variables into **dummy/one-hot encoded** predictors.\n`recipes` provides:\n\n- `step_novel()`: handles unseen factor levels at prediction time\n- `step_dummy()`: converts categorical predictors into numerical dummy variables\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrcp_Flight <- \n  recipe(arr_delay ~ ., data = df_Train) |> \n  update_role(flight, time_hour, new_role = \"ID\") |> \n  step_date(date, features = c(\"dow\", \"month\")) |>               \n  step_holiday(\n    date, \n    holidays = timeDate::listHolidays(\"US\"), \n    keep_original_cols = FALSE\n  ) |> \n  step_novel(all_nominal_predictors()) |>  \n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\n\n\n\n\n### Removing Zero-Variance Predictors\n\nPredictors with constant values provide no information and may cause issues in some algorithms.\nWe remove them using `step_zv()`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrcp_Flight <- \n  recipe(arr_delay ~ ., data = df_Train) |> \n  update_role(flight, time_hour, new_role = \"ID\") |> \n  step_date(date, features = c(\"dow\", \"month\")) |>               \n  step_holiday(\n    date, \n    holidays = timeDate::listHolidays(\"US\"), \n    keep_original_cols = FALSE\n  ) |> \n  step_novel(all_nominal_predictors()) |>  \n  step_dummy(all_nominal_predictors()) |> \n  step_zv(all_predictors())\n```\n:::\n\n\n\n\n\n\n## Combining the Recipe with a Model\n\n### Workflow Definition\n\nIn this section, we formalize the integration of data preprocessing and model specification by constructing a **workflow**. This approach ensures that all preprocessing operations defined in the recipe are applied consistently during model training and prediction, thereby maintaining a coherent pipeline.\n\n\nA `workflow` allows bundling:\n\n1. the **recipe** (preprocessing)\n2. the **model** (statistical/ML algorithm)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmdl_LogReg <- \n  logistic_reg() |> \n  set_engine(\"glm\")\n\nwflow_Flight <- \n  workflow() |> \n  add_model(mdl_LogReg) |> \n  add_recipe(rcp_Flight)\n```\n:::\n\n\n\n\n\n\n## Model Fitting\n\nWe proceed by fitting the complete preprocessing and modeling pipeline to the training data using the `fit()` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_Flight <- \n  wflow_Flight |> \n  fit(data = df_Train)\n```\n:::\n\n\n\n\nInspect model coefficients:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_Flight |> \n  extract_fit_parsnip() |> \n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 157 × 5\n   term                         estimate std.error statistic  p.value\n   <chr>                           <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)                   7.28    2.73           2.67 7.64e- 3\n 2 dep_time                     -0.00166 0.0000141   -118.   0       \n 3 air_time                     -0.0440  0.000563     -78.2  0       \n 4 distance                      0.00507 0.00150        3.38 7.32e- 4\n 5 date_USChristmasDay           1.33    0.177          7.49 6.93e-14\n 6 date_USColumbusDay            0.724   0.170          4.25 2.13e- 5\n 7 date_USCPulaskisBirthday      0.807   0.139          5.80 6.57e- 9\n 8 date_USDecorationMemorialDay  0.585   0.117          4.98 6.32e- 7\n 9 date_USElectionDay            0.948   0.190          4.98 6.25e- 7\n10 date_USGoodFriday             1.25    0.167          7.45 9.40e-14\n# ℹ 147 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Prediction\n\nUpon fitting, the workflow is employed to generate predictions on the validation dataset.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_Flight <- predict(fit_Flight, df_Validate)\n```\n:::\n\n\n\n\nFor evaluation we need both class predictions and class probabilities:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_Flight <- \n  predict(fit_Flight, df_Validate, type = \"prob\") |> \n  bind_cols(predict(fit_Flight, df_Validate, type = \"class\")) |> \n  bind_cols(df_Validate |> select(arr_delay))\n```\n:::\n\n\n\n\n\n\n## Evaluation\n\n\nThe predictive performance of the model is evaluated using appropriate metrics, including ROC AUC and accuracy. Additionally, visualization of the ROC curve facilitates an assessment of the classifier’s discriminative capability.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_Flight |> \n  roc_auc(truth = arr_delay, .pred_late)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.764\n```\n\n\n:::\n\n```{.r .cell-code}\npred_Flight |> \n  accuracy(truth = arr_delay, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.849\n```\n\n\n:::\n\n```{.r .cell-code}\naug_Flight <- augment(fit_Flight, df_Validate)\n\naug_Flight |> \n  roc_curve(truth = arr_delay, .pred_late) |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](ml_tidymodels_basic_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Clean Version of the Code\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(222)  # Set seed for reproducibility\n\n# Split the data into 75% training and 25% validation\nsplit_Flight <- initial_split(df_Flight, prop = 3/4)\ndf_Train <- training(split_Flight)      # Extract training set\ndf_Validate <- testing(split_Flight)    # Extract validation set\n\n# -------------------------------\n# Create a preprocessing recipe\n# -------------------------------\nrcp_Flight <- \n  recipe(arr_delay ~ ., data = df_Train) |>    # Define target and predictors\n  update_role(flight, time_hour, new_role = \"ID\") |>  # Assign ID role to identifier columns\n  step_date(date, features = c(\"dow\", \"month\")) |>    # Extract day of week & month from date\n  step_holiday(\n    date, \n    holidays = timeDate::listHolidays(\"US\"), \n    keep_original_cols = FALSE\n  ) |>                                             # Add holiday indicator features\n  step_novel(all_nominal_predictors()) |>         # Handle new factor levels in prediction\n  step_dummy(all_nominal_predictors()) |>         # Convert categorical predictors to dummy variables\n  step_zv(all_predictors())                       # Remove predictors with zero variance\n\n# -------------------------------\n# Define the logistic regression model\n# -------------------------------\nmdl_LogReg <- \n  logistic_reg() |> \n  set_engine(\"glm\")  # Use generalized linear model as backend\n\n# -------------------------------\n# Combine recipe and model into a workflow\n# -------------------------------\nwflow_Flight <- \n  workflow() |> \n  add_model(mdl_LogReg) |> \n  add_recipe(rcp_Flight)\n\n# -------------------------------\n# Fit the workflow on the training data\n# -------------------------------\nfit_Flight <- \n  wflow_Flight |> \n  fit(data = df_Train)\n\n# -------------------------------\n# Predict on the validation set (class labels)\n# -------------------------------\npred_Flight <- predict(fit_Flight, df_Validate)\n\n# -------------------------------\n# Predict on validation set (probabilities and class labels)\n# -------------------------------\npred_Flight <- \n  predict(fit_Flight, df_Validate, type = \"prob\") |>   # Predicted probabilities\n  bind_cols(predict(fit_Flight, df_Validate, type = \"class\")) |>  # Predicted classes\n  bind_cols(df_Validate |> select(arr_delay))          # Add true labels for evaluation\n\n# -------------------------------\n# Evaluate model performance\n# -------------------------------\npred_Flight |> \n  roc_auc(truth = arr_delay, .pred_late)   # ROC AUC for \"late\" class\n\npred_Flight |> \n  accuracy(truth = arr_delay, .pred_class) # Classification accuracy\n\n# -------------------------------\n# Visualize ROC curve\n# -------------------------------\naug_Flight <- augment(fit_Flight, df_Validate)  # Add predictions and residuals to dataset\n\naug_Flight |> \n  roc_curve(truth = arr_delay, .pred_late) |>  # Compute ROC curve\n  autoplot()                                   # Plot ROC curve\n```\n:::\n\n\n\n\n\n\n\n# Modelling with Resampling\n\nIn contrast to the previous exercise, this section introduces model evaluation using resampling techniques, specifically **cross-validation**. This approach incorporates an additional step in which the training dataset is repeatedly partitioned into multiple training–validation splits. By fitting and evaluating the model across these resampled subsets, cross-validation provides a more reliable and robust estimate of the model’s generalization performance than a single train–validate split.\n\n## Evaluating Training and Validation Sets\n\nA fitted model can be evaluated on both the **training** dataset and the **validation** (test) dataset.  \nAfter generating predictions, we can compute performance metrics such as `roc_auc` and `accuracy`.  \nFor both metrics, **higher values indicate better predictive performance**.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(modeldata) \n\n# ----------------------------------\n# Dataset\n# ----------------------------------\n\n## Split the data\nset.seed(123)\nsplit_Cell <- initial_split(\n  modeldata::cells |> select(-case), \n  strata = class\n)\n\ndf_Train_Cell <- training(split_Cell)\ndf_Validate_Cell  <- testing(split_Cell)\n\n# ----------------------------------\n# Model\n# ----------------------------------\n\n## Specify a random forest classifier\nmdl_RandomForest <- \n  rand_forest(trees = 1000) |> \n  set_engine(\"ranger\") |> \n  set_mode(\"classification\")\n\n## Fit the model\nset.seed(234)\nfit_RandomForest <- \n  mdl_RandomForest |> \n  fit(class ~ ., data = df_Train_Cell)\nfit_RandomForest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      1514 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1187479 \n```\n\n\n:::\n\n```{.r .cell-code}\n# ----------------------------------\n# Predictions\n# ----------------------------------\n\n## Training set predictions\npred_Train_Cell <- \n  predict(fit_RandomForest, df_Train_Cell) |> \n  bind_cols(predict(fit_RandomForest, df_Train_Cell, type = \"prob\")) |> \n  bind_cols(df_Train_Cell |> select(class))\n\n## Validation set predictions\npred_Validate_Cell <- \n  predict(fit_RandomForest, df_Validate_Cell) |> \n  bind_cols(predict(fit_RandomForest, df_Validate_Cell, type = \"prob\")) |> \n  bind_cols(df_Validate_Cell |> select(class))\n\n# ----------------------------------\n# Performance\n# ----------------------------------\n\n## Training\npred_Train_Cell |> roc_auc(truth = class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         1.000\n```\n\n\n:::\n\n```{.r .cell-code}\npred_Train_Cell |> accuracy(truth = class, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.990\n```\n\n\n:::\n\n```{.r .cell-code}\n## Validation\npred_Validate_Cell |> roc_auc(truth = class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.891\n```\n\n\n:::\n\n```{.r .cell-code}\npred_Validate_Cell |> accuracy(truth = class, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.814\n```\n\n\n:::\n:::\n\n\n\n\nThe results typically show a **large gap** between training and validation performance.\nThis is normal: the model tends to overfit the training data, leading to reduced predictive ability on new, unseen data.\nThis is precisely why a simple train/test split is often insufficient.\nTo obtain more reliable and stable performance estimates, we use **resampling methods**.\n\n\n\n## Resampling (Cross-Validation)\n\nCross-validation improves model evaluation by repeatedly splitting the training data into several **folds**.\nEach fold is used once as an **assessment** (test) set, while the remaining folds form the **analysis** (training) set.\n\nThis process reduces the variance in performance estimation and helps detect overfitting.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(345)\nfolds_Cell <- vfold_cv(df_Train_Cell, v = 10)\n\nwflow_CrossValidate <- \n  workflow() |>\n  add_model(mdl_RandomForest) |>\n  add_formula(class ~ .)\n\nset.seed(456)\nfit_CrossValidate <- \n  wflow_CrossValidate |> \n  fit_resamples(folds_Cell)\n\ncollect_metrics(fit_CrossValidate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.833    10 0.00876 Preprocessor1_Model1\n2 brier_class binary     0.120    10 0.00406 Preprocessor1_Model1\n3 roc_auc     binary     0.905    10 0.00627 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n\nCompared with normal model fitting, resampling uses the specialized function:\n\n* `fit_resamples()`\n  which fits the model on each fold and aggregates the performance results.\n\nCross-validation therefore provides a more robust estimate of the model’s generalization ability and reduces the risk of overfitting, compared with a single train/test split.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "ml_tidymodels_basic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}