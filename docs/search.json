[
  {
    "objectID": "modelling/model_linearReservoir.html",
    "href": "modelling/model_linearReservoir.html",
    "title": "Linear Reservoir",
    "section": "",
    "text": "Linear Reservoir is a method and just assuming that the watershed behaves like a linear reservoir, where the outflow is proportional to the water storage within the reservoir.\n\\[\nQ_{out} = \\frac{1}{K}S(t)\n\\tag{1}\\]\nIn addition to their relationship with output and storage, linear reservoir models also adhere to the continuity equation, often referred to as the water balance equation.\n\\[\n\\frac{\\mathrm{d}S(t)}{\\mathrm{d}t} = Q_{in} - Q_{out}\n\\tag{2}\\]\nBy combining both equations, we obtain a differential equation (DGL).\n\\[\nQ_{in} = Q_{out} + K\\frac{\\mathrm{d}Q_{out}(t)}{\\mathrm{d}t}\n\\tag{3}\\]\n\\[\nQ_{out}(t)=\\int_{\\tau=t0}^{t}Q_{in}(\\tau)\\frac{1}{K}e^{-\\frac{t-\\tau}{K}}\\mathrm{d}\\tau + Q_{out}(t_0)\\frac{1}{K}e^{-\\frac{t-t0}{K}}\n\\tag{4}\\]\nWhere:\n\n\\(Q_{in}\\) is the inflow of the reservoir\n\\(Q_{out}\\) is the outflow of the reservoir\n\\(S\\) is the storage of the reservoir\n\\(K\\) is the parameter that defines the relationship between \\(Q_{out}\\) and \\(S\\)",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#analytical-solution",
    "href": "modelling/model_linearReservoir.html#analytical-solution",
    "title": "Linear Reservoir",
    "section": "2.1 Analytical Solution",
    "text": "2.1 Analytical Solution\nThe final form of the equation under the simplifying hypothesis of linear input looks like this:\n\\[\nQ_{out}(t_1) = Q_{out}(t_0) + (Q_{in}(t_1) - Q_{out}(t_0))\\cdot (1-e^{-\\frac{1}{K}}) + (Q_{in}(t_1) - Q_{in}(t_0))\\cdot [1-K(1-e^{-\\frac{1}{K}})]\n\\]\n\nlinear_reservoir_Ana &lt;- function(Q_In, Q_Out0 = 0, param_K = 1) {\n  n_Step &lt;- length(Q_In)\n  Q_Out &lt;- c(Q_Out0, rep(0, n_Step - 1))\n  \n  for (i in 2:n_Step) {\n    Q_Out[i] &lt;- Q_Out[i-1] + (Q_In[i] - Q_Out[i-1]) * (1 - exp(-1 / param_K)) + (Q_In[i] - Q_In[i-1]) * (1 - param_K * (1 - exp(-1 / param_K)))\n  }\n  \n  Q_Out\n  \n}",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#numerical-solution",
    "href": "modelling/model_linearReservoir.html#numerical-solution",
    "title": "Linear Reservoir",
    "section": "2.2 Numerical Solution",
    "text": "2.2 Numerical Solution\nWhen we simplify the difficult continuous form into a discrete form using \\(\\Delta S / \\Delta t\\) to replace \\(\\mathrm{d}S/\\mathrm{d}t\\), we can obtain the numerical (discrete) format:\n\\[\nQ_{out}(t_1) = Q_{out}(t_0) + (Q_{in}(t_0) - Q_{out}(t_0)) \\frac{1}{K + 0.5} + (Q_{in}(t_1) - Q_{in}(t_0)) \\frac{0.5}{K + 0.5}\n\\]\n\nlinear_reservoir_Num &lt;- function(Q_In, Q_Out0 = 0, param_K = 1) {\n  n_Step &lt;- length(Q_In)\n  Q_Out &lt;- c(Q_Out0, rep(0, n_Step - 1))\n  \n  for (i in 2:n_Step) {\n    Q_Out[i] &lt;- Q_Out[i-1] + (Q_In[i-1] - Q_Out[i-1]) / (param_K + 0.5) + (Q_In[i] - Q_In[i-1]) * .5 / (param_K + 0.5)\n  }\n  \n  Q_Out\n  \n}",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#compare-the-results-of-both-functions",
    "href": "modelling/model_linearReservoir.html#compare-the-results-of-both-functions",
    "title": "Linear Reservoir",
    "section": "2.3 Compare the results of both Functions",
    "text": "2.3 Compare the results of both Functions\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\nload(\"../data_share/color.Rdata\")\n\n\n\nCode\nnum_TestIn &lt;- c(rep(100, 100), 0:100, rep(0,99))\nnum_Out_Ana &lt;- linear_reservoir_Ana(num_TestIn, param_K = 60)\nnum_Out_Num &lt;- linear_reservoir_Num(num_TestIn, param_K = 60)\n\ngg_Test &lt;- ggplot() +\n  geom_line(aes(1:300, num_TestIn, color = \"Input\")) +\n  geom_line(aes(1:300, num_Out_Ana, color = \"Output\\n(Analytical)\")) +\n  geom_line(aes(1:300, num_Out_Num, color = \"Output\\n(Numerical)\")) +\n  scale_color_manual(values = c(\"cyan\", \"red\", \"orange\"))+\n  labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"\") \n\nggplotly(gg_Test)\n\n\n\n\n\n\nIn the test forcing data, there are constant, linear, and null input scenarios. In all three situations, the analytical and numerical solutions yield almost the same results. Therefore, we can use either of them for subsequent analysis.",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#boundaray-condition-forcing",
    "href": "modelling/model_linearReservoir.html#boundaray-condition-forcing",
    "title": "Linear Reservoir",
    "section": "3.1 Boundaray Condition Forcing",
    "text": "3.1 Boundaray Condition Forcing\nFor the single linear reservoir, the boundary condition is the time series of the inflow \\(Q_{in}(t)\\) (Q_In).\nIn the one-variable experiment of the boundary condition, we will consider five boundary conditions, including three constants at 10, 50, and 100 [V/L], as well as an increasing (0 to 100 [V/L]) and decreasing (100 to 0 [V/L]) series.\nThe\n\n\nCode\nnum_BC10 &lt;- rep(c(10,0), each = 100)\nnum_BC50 &lt;- rep(c(50,0), each = 100)\nnum_BC100 &lt;- rep(c(100,0), each = 100)\nnum_BCin &lt;- c(0:100, rep(0,99))\nnum_BCde &lt;- c(100:0, rep(0,99))\n\nlst_BC_in &lt;- list(num_BC10, num_BC50, num_BC100, num_BCin, num_BCde)\ndf_BC_in &lt;- bind_cols(lst_BC_in) |&gt; as.data.frame()\nnames(df_BC_in) &lt;- c(\"BC10\", \"BC50\", \"BC100\", \"BCin\", \"BCde\")\ngdf_BC_in &lt;- reshape2::melt(df_BC_in)\ngdf_BC_in$time &lt;- 1:200\ngdf_BC_in$facet &lt;- \"Q_In\"\n\nlst_BC_out &lt;- map(lst_BC_in, linear_reservoir_Num, param_K = 60)\n\ndf_BC_out &lt;- bind_cols(lst_BC_out) |&gt; as.data.frame()\nnames(df_BC_out) &lt;- c(\"BC10\", \"BC50\", \"BC100\", \"BCin\", \"BCde\")\ngdf_BC_out &lt;- reshape2::melt(df_BC_out)\ngdf_BC_out$time &lt;- 1:200\ngdf_BC_out$facet &lt;- \"Q_Out\"\ngdf_BC &lt;- rbind(gdf_BC_in, gdf_BC_out)\ngg_BC &lt;- ggplot(gdf_BC) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_manual(values = color_TUD_diskrete)+\n  facet_grid(cols = vars(facet))+\n  scale_alpha_manual(values = c(.6,1)) +\n    labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari (BC):\") \nggplotly(gg_BC)\n\n\n\n\n\n\n\n\nFigure 1: The facet labeled Q_In displays five input time series (\\(Q_{in}\\)) scenarios. In scenarios BC10, BC50, and BC100, the input remains constant for the initial 100 timesteps. BCin and BCde represent scenarios where the input increases and decreases, respectively, during the first 100 timesteps. Notably, BC50, BCin, and BCde scenarios all have the same total volume of 5000 [V]. The facet labeled Q_Out presents the corresponding simulated results, showcasing the model’s responses to different boundary conditions (\\(Q_{in}\\)).",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#innitial-condition-forcing",
    "href": "modelling/model_linearReservoir.html#innitial-condition-forcing",
    "title": "Linear Reservoir",
    "section": "3.2 Innitial Condition Forcing",
    "text": "3.2 Innitial Condition Forcing\nNormally, the initial condition represents the initial state of state variables, such as the water content of the soil or the storage of the reservoir. However, in the case of a single linear reservoir, the storage of the reservoir is simplified as the variable \\(Q_{out}\\) (Q_Out). For this one-variable experiment, \\(Q_{out}\\) will vary from 10 to 90 [V/L].\n\n\nCode\nlst_IC_in &lt;- as.list(seq(10, 90, 10))\nlst_IC_out &lt;- map(lst_IC_in, linear_reservoir_Ana, Q_In = num_BC100, param_K = 60)\n\ndf_IC_out &lt;- bind_cols(lst_IC_out) |&gt; as.data.frame()\ngdf_IC_out &lt;- reshape2::melt(df_IC_out)\ngdf_IC_out$time &lt;- 1:200\ngdf_IC_out$variable &lt;- rep(seq(10, 90, 10), each = 200)\ngg_BC &lt;- ggplot(gdf_IC_out) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_gradientn(colours = color_DRESDEN)+\n    labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari\\n(IC):\") \nggplotly(gg_BC)\n\n\n\n\n\n\n\n\nFigure 2: The different initial condition \\(Q_{out}(t_0)\\) values result in distinct outflow time series. The line colors correspond to the values of the initial condition.",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#parameter",
    "href": "modelling/model_linearReservoir.html#parameter",
    "title": "Linear Reservoir",
    "section": "3.3 Parameter",
    "text": "3.3 Parameter\nIn the case of the single linear reservoir, there is only one parameter, denoted as \\(K\\) (param_K). The parameter \\(K\\) can vary widely due to differences in the scale of the simulation domain. It has physical units of time, which can be specified in units such as seconds, hours, or days depending on the scale of the hydrological model.\n\n\nCode\nlst_Param_in &lt;- as.list(seq(10, 90, 10))\nlst_Param_out &lt;- map(lst_Param_in, linear_reservoir_Ana, Q_In = num_BC100, Q_Out0 = 0)\n\ndf_Param_out &lt;- bind_cols(lst_Param_out) |&gt; as.data.frame()\ngdf_Param_out &lt;- reshape2::melt(df_Param_out)\ngdf_Param_out$time &lt;- 1:200\ngdf_Param_out$variable &lt;- rep(seq(10, 90, 10), each = 200)\ngg_BC &lt;- ggplot(gdf_Param_out) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_gradientn(colours = color_DRESDEN)+\n  labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari\\n(Param):\") \nggplotly(gg_BC)\n\n\n\n\n\n\n\n\nFigure 3: The different parameter \\(K\\) values result in distinct outflow time series. The line colors correspond to the values of the parameter.",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/index.html",
    "href": "modelling/index.html",
    "title": "Hydrological Modelling",
    "section": "",
    "text": "Hydrological modeling is a scientific approach aimed at simulating and comprehending the dynamics of the Earth’s water cycle. On this page, we will compile information on hydrological models, as well as the processes involved in their application. This includes running the model, evaluating its performance, calibrating and conducting sensitivity analyses, and ultimately validating the model’s results. Through this comprehensive exploration, we aim to provide valuable insights into the world of hydrological modeling.",
    "crumbs": [
      "Modelling"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html",
    "href": "mldl/ml_tidymodels_basic.html",
    "title": "ML Workflow with tidymodels",
    "section": "",
    "text": "In this article we use the tidymodels ecosystem to illustrate a complete machine learning workflow in a tidy, coherent, and reproducible manner.\nThroughout the article, we revisit the fundamental components of the ML workflow:\n\nData split\nFeature engineering\n\nModel building\n\nModel fitting\nModel prediction\nModel evaluation\n\nWe begin with the most basic workflow and later extend it using the recipe framework for more advanced preprocessing.\nThe material in this article is based on the case study provided at: https://www.tidymodels.org/start/case-study/. All exercises use the dataset included in that example.\nThe following packages are required:\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\nlibrary(tidymodels)  # parsnip + other tidymodels packages\n\n# Helper packages\nlibrary(readr)       # for importing data\nlibrary(broom.mixed) # for tidying Bayesian model output\nlibrary(dotwhisker)  # for visualizing regression results\nlibrary(skimr)       # for variable summaries",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#data",
    "href": "mldl/ml_tidymodels_basic.html#data",
    "title": "ML Workflow with tidymodels",
    "section": "2.1 Data",
    "text": "2.1 Data\nIn this exercise we explore the relationship between:\n\nthe outcome variable: sea urchin width\nfeature 1: initial volume (continuous)\nfeature 2: food regime (categorical)\n\nA basic model for these variables can be written as:\nwidth ~ initial_volume * food_regime\n\n## Data import ----------\ndf_Urchins &lt;-\n  read_csv(\"https://tidymodels.org/start/models/urchins.csv\") |&gt;\n  setNames(c(\"food_regime\", \"initial_volume\", \"width\")) |&gt;\n  mutate(food_regime = factor(food_regime, levels = c(\"Initial\", \"Low\", \"High\")))\n\n## Explore the relationships ----------\nggplot(df_Urchins,\n       aes(x = initial_volume,\n           y = width,\n           group = food_regime,\n           col = food_regime)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#building-the-model",
    "href": "mldl/ml_tidymodels_basic.html#building-the-model",
    "title": "ML Workflow with tidymodels",
    "section": "2.2 Building the Model",
    "text": "2.2 Building the Model\nWe start with a simple linear regression model using linear_reg(). The tidymodels ecosystem separates model specification (what model we want) from the computational engine (how it is estimated). The parsnip package handles this abstraction.\nAvailable engines for linear_reg() are listed here: https://parsnip.tidymodels.org/reference/linear_reg.html\n\nChoose the model type:\n\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nChoose the engine:\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nSave the model specification:\n\n\nmdl_Linear &lt;- linear_reg()",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#training-fitting-the-model",
    "href": "mldl/ml_tidymodels_basic.html#training-fitting-the-model",
    "title": "ML Workflow with tidymodels",
    "section": "2.3 Training (Fitting) the Model",
    "text": "2.3 Training (Fitting) the Model\nTo train the model, we use the fit() function, which requires:\n\nthe model specification\na formula\nthe training dataset\n\n\nfit_Linear &lt;-\n  mdl_Linear |&gt;\n  fit(width ~ initial_volume * food_regime, data = df_Urchins)\n\nfit_Linear\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = width ~ initial_volume * food_regime, data = data)\n\nCoefficients:\n                   (Intercept)                  initial_volume  \n                     0.0331216                       0.0015546  \n                food_regimeLow                 food_regimeHigh  \n                     0.0197824                       0.0214111  \n initial_volume:food_regimeLow  initial_volume:food_regimeHigh  \n                    -0.0012594                       0.0005254  \n\n\nWe can inspect the fitted model coefficients:\n\ntidy(fit_Linear)\n\n# A tibble: 6 × 5\n  term                            estimate std.error statistic  p.value\n  &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                     0.0331    0.00962      3.44  0.00100 \n2 initial_volume                  0.00155   0.000398     3.91  0.000222\n3 food_regimeLow                  0.0198    0.0130       1.52  0.133   \n4 food_regimeHigh                 0.0214    0.0145       1.47  0.145   \n5 initial_volume:food_regimeLow  -0.00126   0.000510    -2.47  0.0162  \n6 initial_volume:food_regimeHigh  0.000525  0.000702     0.748 0.457",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#predicting-with-the-fitted-model",
    "href": "mldl/ml_tidymodels_basic.html#predicting-with-the-fitted-model",
    "title": "ML Workflow with tidymodels",
    "section": "2.4 Predicting with the Fitted Model",
    "text": "2.4 Predicting with the Fitted Model\nOnce a model is fitted, we can use it for prediction. To make predictions we need:\n\na dataset containing the feature variables (with matching names)\nthe fitted model object\n\nBy default, predict() returns the mean prediction:\n\ndf_NewData &lt;- expand.grid(\n  initial_volume = 20,\n  food_regime = c(\"Initial\", \"Low\", \"High\")\n)\n\npred_Mean &lt;- predict(fit_Linear, new_data = df_NewData)\npred_Mean\n\n# A tibble: 3 × 1\n   .pred\n   &lt;dbl&gt;\n1 0.0642\n2 0.0588\n3 0.0961\n\n\nWe can also request confidence intervals using type = \"conf_int\":\n\npred_Conf &lt;- predict(\n  fit_Linear,\n  new_data = df_NewData,\n  type = \"conf_int\"\n)\npred_Conf\n\n# A tibble: 3 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1      0.0555      0.0729\n2      0.0499      0.0678\n3      0.0870      0.105",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#data-1",
    "href": "mldl/ml_tidymodels_basic.html#data-1",
    "title": "ML Workflow with tidymodels",
    "section": "3.1 Data",
    "text": "3.1 Data\nIn this example we work with flight information from the United States (package nycflights13).\nOur goal is to predict whether an arriving flight is late (arrival delay ≥ 30 minutes) or on time.\nThe modeling relationship can be summarized as:\narr_delay ~ (date, airport, distance, ...)\n\nlibrary(nycflights13)\nset.seed(123)\n\ndf_Flight &lt;- \n  flights |&gt; \n  mutate(\n    arr_delay = ifelse(arr_delay &gt;= 30, \"late\", \"on_time\"),\n    arr_delay = factor(arr_delay),\n    date = lubridate::as_date(time_hour)\n  ) |&gt; \n  inner_join(weather, by = c(\"origin\", \"time_hour\")) |&gt; \n  select(dep_time, flight, origin, dest, air_time, distance,\n         carrier, date, arr_delay, time_hour) |&gt; \n  na.omit() |&gt; \n  mutate_if(is.character, as.factor)\n\ndf_Flight |&gt; \n  skimr::skim(dest, carrier)\n\n\nData summary\n\n\nName\ndf_Flight\n\n\nNumber of rows\n325819\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndest\n0\n1\nFALSE\n104\nATL: 16771, ORD: 16507, LAX: 15942, BOS: 14948\n\n\ncarrier\n0\n1\nFALSE\n16\nUA: 57489, B6: 53715, EV: 50868, DL: 47465",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#data-split",
    "href": "mldl/ml_tidymodels_basic.html#data-split",
    "title": "ML Workflow with tidymodels",
    "section": "3.2 Data Split",
    "text": "3.2 Data Split\nAs usual, we divide the dataset into training and validation sets. This allows us to fit the model on one portion of the data and evaluate performance on unseen observations.\nUnder the tidymodels framework:\n\ninitial_split() performs the random split.\ntraining() and testing() extract the corresponding sets.\n\n\nset.seed(222)\nsplit_Flight &lt;- initial_split(df_Flight, prop = 3/4)\n\ndf_Train &lt;- training(split_Flight)\ndf_Validate &lt;- testing(split_Flight)",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#creating-a-recipe",
    "href": "mldl/ml_tidymodels_basic.html#creating-a-recipe",
    "title": "ML Workflow with tidymodels",
    "section": "3.3 Creating a Recipe",
    "text": "3.3 Creating a Recipe\n\n3.3.1 Basical Recipe\nA recipe in tidymodels is a structured, pre-defined plan for data preprocessing and feature engineering.\nIt allows us to declare, in advance, all the steps needed to transform raw data into model-ready features.\nWhen creating a recipe, two fundamental components must be specified:\n\nFormula (~):\nThe variable on the left-hand side is the target/outcome, and the variables on the right-hand side are the predictors/features.\nData:\nThe dataset is used to identify variable names and types. At this stage, no transformations are applied.\n\n\n# Create a basic recipe\nrcp_Flight &lt;- recipe(arr_delay ~ ., data = df_Train)\n\nUsing . on the right-hand side indicates that all remaining variables in the dataset should be used as predictors.\n\n\n3.3.2 Roles\nIn addition to the standard roles (outcome and predictor), recipes allows defining custom roles using update_role().\nCommon roles include:\n\npredictor: variables used to predict the outcome\noutcome: target variable\nID: identifiers not used for modeling\ncase_weights: observation weights\ndatetime, latitude, longitude: for temporal or spatial feature engineering\n\n\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Train) |&gt; \n  update_role(flight, time_hour, new_role = \"ID\")\n\nsummary(rcp_Flight)\n\n# A tibble: 10 × 4\n   variable  type      role      source  \n   &lt;chr&gt;     &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 dep_time  &lt;chr [2]&gt; predictor original\n 2 flight    &lt;chr [2]&gt; ID        original\n 3 origin    &lt;chr [3]&gt; predictor original\n 4 dest      &lt;chr [3]&gt; predictor original\n 5 air_time  &lt;chr [2]&gt; predictor original\n 6 distance  &lt;chr [2]&gt; predictor original\n 7 carrier   &lt;chr [3]&gt; predictor original\n 8 date      &lt;chr [1]&gt; predictor original\n 9 time_hour &lt;chr [1]&gt; ID        original\n10 arr_delay &lt;chr [3]&gt; outcome   original",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#feature-engineering-with-step_",
    "href": "mldl/ml_tidymodels_basic.html#feature-engineering-with-step_",
    "title": "ML Workflow with tidymodels",
    "section": "3.4 Feature Engineering with step_*",
    "text": "3.4 Feature Engineering with step_*\n\n3.4.1 Date Features\nFeature engineering transforms raw data into model-ready variables. recipes expresses each transformation as a step_*() function.\nBelow, we extract:\n\nday of week (dow)\nmonth\nholiday indicators (using U.S. holidays)\n\n\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Train) |&gt; \n  update_role(flight, time_hour, new_role = \"ID\") |&gt; \n  step_date(date, features = c(\"dow\", \"month\")) |&gt;               \n  step_holiday(\n    date,\n    holidays = timeDate::listHolidays(\"US\"),\n    keep_original_cols = FALSE\n  )\n\nThis replaces the original date with engineered calendar features.\n\n\n3.4.2 Categorical Variables (Dummy Encoding)\nMost models require converting categorical variables into dummy/one-hot encoded predictors. recipes provides:\n\nstep_novel(): handles unseen factor levels at prediction time\nstep_dummy(): converts categorical predictors into numerical dummy variables\n\n\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Train) |&gt; \n  update_role(flight, time_hour, new_role = \"ID\") |&gt; \n  step_date(date, features = c(\"dow\", \"month\")) |&gt;               \n  step_holiday(\n    date, \n    holidays = timeDate::listHolidays(\"US\"), \n    keep_original_cols = FALSE\n  ) |&gt; \n  step_novel(all_nominal_predictors()) |&gt;  \n  step_dummy(all_nominal_predictors())\n\n\n\n3.4.3 Removing Zero-Variance Predictors\nPredictors with constant values provide no information and may cause issues in some algorithms. We remove them using step_zv():\n\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Train) |&gt; \n  update_role(flight, time_hour, new_role = \"ID\") |&gt; \n  step_date(date, features = c(\"dow\", \"month\")) |&gt;               \n  step_holiday(\n    date, \n    holidays = timeDate::listHolidays(\"US\"), \n    keep_original_cols = FALSE\n  ) |&gt; \n  step_novel(all_nominal_predictors()) |&gt;  \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors())",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#combining-the-recipe-with-a-model",
    "href": "mldl/ml_tidymodels_basic.html#combining-the-recipe-with-a-model",
    "title": "ML Workflow with tidymodels",
    "section": "3.5 Combining the Recipe with a Model",
    "text": "3.5 Combining the Recipe with a Model\n\n3.5.1 Workflow Definition\nIn this section, we formalize the integration of data preprocessing and model specification by constructing a workflow. This approach ensures that all preprocessing operations defined in the recipe are applied consistently during model training and prediction, thereby maintaining a coherent pipeline.\nA workflow allows bundling:\n\nthe recipe (preprocessing)\nthe model (statistical/ML algorithm)\n\n\nmdl_LogReg &lt;- \n  logistic_reg() |&gt; \n  set_engine(\"glm\")\n\nwflow_Flight &lt;- \n  workflow() |&gt; \n  add_model(mdl_LogReg) |&gt; \n  add_recipe(rcp_Flight)",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#model-fitting",
    "href": "mldl/ml_tidymodels_basic.html#model-fitting",
    "title": "ML Workflow with tidymodels",
    "section": "3.6 Model Fitting",
    "text": "3.6 Model Fitting\nWe proceed by fitting the complete preprocessing and modeling pipeline to the training data using the fit() function.\n\nfit_Flight &lt;- \n  wflow_Flight |&gt; \n  fit(data = df_Train)\n\nInspect model coefficients:\n\nfit_Flight |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy()\n\n# A tibble: 157 × 5\n   term                         estimate std.error statistic  p.value\n   &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                   7.28    2.73           2.67 7.64e- 3\n 2 dep_time                     -0.00166 0.0000141   -118.   0       \n 3 air_time                     -0.0440  0.000563     -78.2  0       \n 4 distance                      0.00507 0.00150        3.38 7.32e- 4\n 5 date_USChristmasDay           1.33    0.177          7.49 6.93e-14\n 6 date_USColumbusDay            0.724   0.170          4.25 2.13e- 5\n 7 date_USCPulaskisBirthday      0.807   0.139          5.80 6.57e- 9\n 8 date_USDecorationMemorialDay  0.585   0.117          4.98 6.32e- 7\n 9 date_USElectionDay            0.948   0.190          4.98 6.25e- 7\n10 date_USGoodFriday             1.25    0.167          7.45 9.40e-14\n# ℹ 147 more rows",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#prediction",
    "href": "mldl/ml_tidymodels_basic.html#prediction",
    "title": "ML Workflow with tidymodels",
    "section": "3.7 Prediction",
    "text": "3.7 Prediction\nUpon fitting, the workflow is employed to generate predictions on the validation dataset.\n\npred_Flight &lt;- predict(fit_Flight, df_Validate)\n\nFor evaluation we need both class predictions and class probabilities:\n\npred_Flight &lt;- \n  predict(fit_Flight, df_Validate, type = \"prob\") |&gt; \n  bind_cols(predict(fit_Flight, df_Validate, type = \"class\")) |&gt; \n  bind_cols(df_Validate |&gt; select(arr_delay))",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#evaluation",
    "href": "mldl/ml_tidymodels_basic.html#evaluation",
    "title": "ML Workflow with tidymodels",
    "section": "3.8 Evaluation",
    "text": "3.8 Evaluation\nThe predictive performance of the model is evaluated using appropriate metrics, including ROC AUC and accuracy. Additionally, visualization of the ROC curve facilitates an assessment of the classifier’s discriminative capability.\n\npred_Flight |&gt; \n  roc_auc(truth = arr_delay, .pred_late)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.764\n\npred_Flight |&gt; \n  accuracy(truth = arr_delay, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.849\n\naug_Flight &lt;- augment(fit_Flight, df_Validate)\n\naug_Flight |&gt; \n  roc_curve(truth = arr_delay, .pred_late) |&gt; \n  autoplot()",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#clean-version-of-the-code",
    "href": "mldl/ml_tidymodels_basic.html#clean-version-of-the-code",
    "title": "ML Workflow with tidymodels",
    "section": "3.9 Clean Version of the Code",
    "text": "3.9 Clean Version of the Code\n\nset.seed(222)  # Set seed for reproducibility\n\n# Split the data into 75% training and 25% validation\nsplit_Flight &lt;- initial_split(df_Flight, prop = 3/4)\ndf_Train &lt;- training(split_Flight)      # Extract training set\ndf_Validate &lt;- testing(split_Flight)    # Extract validation set\n\n# -------------------------------\n# Create a preprocessing recipe\n# -------------------------------\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Train) |&gt;    # Define target and predictors\n  update_role(flight, time_hour, new_role = \"ID\") |&gt;  # Assign ID role to identifier columns\n  step_date(date, features = c(\"dow\", \"month\")) |&gt;    # Extract day of week & month from date\n  step_holiday(\n    date, \n    holidays = timeDate::listHolidays(\"US\"), \n    keep_original_cols = FALSE\n  ) |&gt;                                             # Add holiday indicator features\n  step_novel(all_nominal_predictors()) |&gt;         # Handle new factor levels in prediction\n  step_dummy(all_nominal_predictors()) |&gt;         # Convert categorical predictors to dummy variables\n  step_zv(all_predictors())                       # Remove predictors with zero variance\n\n# -------------------------------\n# Define the logistic regression model\n# -------------------------------\nmdl_LogReg &lt;- \n  logistic_reg() |&gt; \n  set_engine(\"glm\")  # Use generalized linear model as backend\n\n# -------------------------------\n# Combine recipe and model into a workflow\n# -------------------------------\nwflow_Flight &lt;- \n  workflow() |&gt; \n  add_model(mdl_LogReg) |&gt; \n  add_recipe(rcp_Flight)\n\n# -------------------------------\n# Fit the workflow on the training data\n# -------------------------------\nfit_Flight &lt;- \n  wflow_Flight |&gt; \n  fit(data = df_Train)\n\n# -------------------------------\n# Predict on the validation set (class labels)\n# -------------------------------\npred_Flight &lt;- predict(fit_Flight, df_Validate)\n\n# -------------------------------\n# Predict on validation set (probabilities and class labels)\n# -------------------------------\npred_Flight &lt;- \n  predict(fit_Flight, df_Validate, type = \"prob\") |&gt;   # Predicted probabilities\n  bind_cols(predict(fit_Flight, df_Validate, type = \"class\")) |&gt;  # Predicted classes\n  bind_cols(df_Validate |&gt; select(arr_delay))          # Add true labels for evaluation\n\n# -------------------------------\n# Evaluate model performance\n# -------------------------------\npred_Flight |&gt; \n  roc_auc(truth = arr_delay, .pred_late)   # ROC AUC for \"late\" class\n\npred_Flight |&gt; \n  accuracy(truth = arr_delay, .pred_class) # Classification accuracy\n\n# -------------------------------\n# Visualize ROC curve\n# -------------------------------\naug_Flight &lt;- augment(fit_Flight, df_Validate)  # Add predictions and residuals to dataset\n\naug_Flight |&gt; \n  roc_curve(truth = arr_delay, .pred_late) |&gt;  # Compute ROC curve\n  autoplot()                                   # Plot ROC curve",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#evaluating-training-and-validation-sets",
    "href": "mldl/ml_tidymodels_basic.html#evaluating-training-and-validation-sets",
    "title": "ML Workflow with tidymodels",
    "section": "4.1 Evaluating Training and Validation Sets",
    "text": "4.1 Evaluating Training and Validation Sets\nA fitted model can be evaluated on both the training dataset and the validation (test) dataset.\nAfter generating predictions, we can compute performance metrics such as roc_auc and accuracy.\nFor both metrics, higher values indicate better predictive performance.\n\nlibrary(modeldata) \n\n# ----------------------------------\n# Dataset\n# ----------------------------------\n\n## Split the data\nset.seed(123)\nsplit_Cell &lt;- initial_split(\n  modeldata::cells |&gt; select(-case), \n  strata = class\n)\n\ndf_Train_Cell &lt;- training(split_Cell)\ndf_Validate_Cell  &lt;- testing(split_Cell)\n\n# ----------------------------------\n# Model\n# ----------------------------------\n\n## Specify a random forest classifier\nmdl_RandomForest &lt;- \n  rand_forest(trees = 1000) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"classification\")\n\n## Fit the model\nset.seed(234)\nfit_RandomForest &lt;- \n  mdl_RandomForest |&gt; \n  fit(class ~ ., data = df_Train_Cell)\nfit_RandomForest\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      1514 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1187479 \n\n# ----------------------------------\n# Predictions\n# ----------------------------------\n\n## Training set predictions\npred_Train_Cell &lt;- \n  predict(fit_RandomForest, df_Train_Cell) |&gt; \n  bind_cols(predict(fit_RandomForest, df_Train_Cell, type = \"prob\")) |&gt; \n  bind_cols(df_Train_Cell |&gt; select(class))\n\n## Validation set predictions\npred_Validate_Cell &lt;- \n  predict(fit_RandomForest, df_Validate_Cell) |&gt; \n  bind_cols(predict(fit_RandomForest, df_Validate_Cell, type = \"prob\")) |&gt; \n  bind_cols(df_Validate_Cell |&gt; select(class))\n\n# ----------------------------------\n# Performance\n# ----------------------------------\n\n## Training\npred_Train_Cell |&gt; roc_auc(truth = class, .pred_PS)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         1.000\n\npred_Train_Cell |&gt; accuracy(truth = class, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.990\n\n## Validation\npred_Validate_Cell |&gt; roc_auc(truth = class, .pred_PS)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.891\n\npred_Validate_Cell |&gt; accuracy(truth = class, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.814\n\n\nThe results typically show a large gap between training and validation performance. This is normal: the model tends to overfit the training data, leading to reduced predictive ability on new, unseen data. This is precisely why a simple train/test split is often insufficient. To obtain more reliable and stable performance estimates, we use resampling methods.",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_tidymodels_basic.html#resampling-cross-validation",
    "href": "mldl/ml_tidymodels_basic.html#resampling-cross-validation",
    "title": "ML Workflow with tidymodels",
    "section": "4.2 Resampling (Cross-Validation)",
    "text": "4.2 Resampling (Cross-Validation)\nCross-validation improves model evaluation by repeatedly splitting the training data into several folds. Each fold is used once as an assessment (test) set, while the remaining folds form the analysis (training) set.\nThis process reduces the variance in performance estimation and helps detect overfitting.\n\nset.seed(345)\nfolds_Cell &lt;- vfold_cv(df_Train_Cell, v = 10)\n\nwflow_CrossValidate &lt;- \n  workflow() |&gt;\n  add_model(mdl_RandomForest) |&gt;\n  add_formula(class ~ .)\n\nset.seed(456)\nfit_CrossValidate &lt;- \n  wflow_CrossValidate |&gt; \n  fit_resamples(folds_Cell)\n\ncollect_metrics(fit_CrossValidate)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.833    10 0.00876 Preprocessor1_Model1\n2 brier_class binary     0.120    10 0.00406 Preprocessor1_Model1\n3 roc_auc     binary     0.905    10 0.00627 Preprocessor1_Model1\n\n\nCompared with normal model fitting, resampling uses the specialized function:\n\nfit_resamples() which fits the model on each fold and aggregates the performance results.\n\nCross-validation therefore provides a more robust estimate of the model’s generalization ability and reduces the risk of overfitting, compared with a single train/test split.",
    "crumbs": [
      "ML / DL",
      "ML Workflow with `tidymodels`"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html",
    "href": "mldl/ml_feature_engineering.html",
    "title": "Feature Engineering and Selection",
    "section": "",
    "text": "library(tidyverse)\ntheme_set(theme_bw())\n\nlibrary(tidymodels)  # parsnip + other tidymodels packages\nlibrary(learntidymodels)\nlibrary(embed)\n# Helper packages\nlibrary(readr)       # for importing data\nlibrary(broom.mixed) # for tidying Bayesian model output\nlibrary(dotwhisker)  # for visualizing regression results\nlibrary(skimr)       # for variable summaries\nlibrary(nycflights13)\nlibrary(ggforce)\n\n\n\nCode\ncolor_RUB_blue &lt;- \"#17365c\"\ncolor_RUB_green &lt;- \"#8dae10\"\ncolor_TUD_pink &lt;- \"#EC008D\"\ncolor_DRESDEN &lt;- c(\"#03305D\", \"#28618C\", \"#539DC5\", \"#84D1EE\", \"#009BA4\", \"#13A983\", \"#93C356\", \"#BCCF02\")",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#dummy-variables-for-unordered-categories",
    "href": "mldl/ml_feature_engineering.html#dummy-variables-for-unordered-categories",
    "title": "Feature Engineering and Selection",
    "section": "3.1 Dummy Variables for Unordered Categories",
    "text": "3.1 Dummy Variables for Unordered Categories\nThe most common way to encode categorical variables is to create dummy (one-hot) indicator variables.\n\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Flight) |&gt;    \n  step_dummy(origin)\n\nprep_Flight &lt;- prep(rcp_Flight)\ndf_Back &lt;- bake(prep_Flight, new_data = NULL)\n\ndata.frame(df_Flight$origin, df_Back[setdiff(names(df_Back), names(df_Flight))]) |&gt; \n  head(100) |&gt; \n  DT::datatable()",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#creating-dummy-variables-for-many-categories",
    "href": "mldl/ml_feature_engineering.html#creating-dummy-variables-for-many-categories",
    "title": "Feature Engineering and Selection",
    "section": "3.2 Creating Dummy Variables for Many Categories",
    "text": "3.2 Creating Dummy Variables for Many Categories\nWhen a variable has many categories (high cardinality), simple one-hot encoding becomes inefficient because it produces a very large sparse matrix.\n\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Flight) |&gt;    \n  step_dummy(dest)\n\nprep_Flight &lt;- prep(rcp_Flight)\ndf_Back &lt;- bake(prep_Flight, new_data = NULL)\n\ndata.frame(df_Flight$dest, df_Back[setdiff(names(df_Back), names(df_Flight))]) |&gt; \n  head(100) |&gt; \n  DT::datatable()\n\n\n\n\n\nA better approach for high-cardinality variables is hash encoding:\n\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Flight) |&gt;    \n  textrecipes::step_dummy_hash(dest, num_terms = 16)\n\nprep_Flight &lt;- prep(rcp_Flight)\ndf_Back &lt;- bake(prep_Flight, new_data = NULL)\n\ndata.frame(df_Flight$dest, df_Back[setdiff(names(df_Back), names(df_Flight))]) |&gt; \n  head(100) |&gt; \n  DT::datatable()\n\n\n\n\n\nHash encoding maps categorical levels to a fixed number of numerical features using a hashing function. Instead of creating one column per category, each category is transformed into a set of hashed feature values.\nAdvantages:\n\nProduces a fixed feature dimensionality through num_terms\nScales well to hundreds or thousands of categories\nAutomatically handles unseen categories at prediction time\nAvoids imposing artificial ordering (unlike integer encoding)\nMore memory-efficient than one-hot encoding for large category sets\nWorks directly on character/factor variables without additional preprocessing\n\nDisadvantages:\n\nHash collisions may occur, causing different categories to map to the same bucket\nFeatures are not interpretable because the mapping is not reversible\nCollisions can degrade accuracy, especially for models sensitive to feature distortions (e.g., linear models)\nChoosing an appropriate value for num_terms is heuristic and problem-dependent\nNot advantageous for low-cardinality variables",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#approaches-for-novel-categories",
    "href": "mldl/ml_feature_engineering.html#approaches-for-novel-categories",
    "title": "Feature Engineering and Selection",
    "section": "3.3 Approaches for Novel Categories",
    "text": "3.3 Approaches for Novel Categories\nIn practical applications, test data often contain categories not seen during training. Recipes includes a dedicated step for this:\n\nrcp_Flight &lt;- \n  recipe(arr_delay ~ ., data = df_Flight) |&gt;    \n  step_novel() |&gt; \n  step_dummy(origin)\n\nprep_Flight &lt;- prep(rcp_Flight)\ndf_Back &lt;- bake(prep_Flight, new_data = NULL)\n\ndata.frame(df_Flight$origin, df_Back[setdiff(names(df_Back), names(df_Flight))]) |&gt; \n  head(100) |&gt; DT::datatable()\n\n\n\n\n\nstep_novel() must come before step_dummy() so that the training step learns the “new” category and creates a corresponding column. This does not improve prediction performance—it only prevents errors.",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#encodings-for-ordered-data",
    "href": "mldl/ml_feature_engineering.html#encodings-for-ordered-data",
    "title": "Feature Engineering and Selection",
    "section": "3.4 Encodings for Ordered Data",
    "text": "3.4 Encodings for Ordered Data\nFor ordinal categories (e.g., bad &lt; normal &lt; good or low &lt; medium &lt; high), the ordering contains useful information.\nPossible numeric encodings:\n\nLinear integer encoding (e.g. -1, 0, 1) → step_integer\nPolynomial or contrast encodings → step_poly\nMultiple orthogonal encodings to capture more structure",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#knowledge-based-mapping",
    "href": "mldl/ml_feature_engineering.html#knowledge-based-mapping",
    "title": "Feature Engineering and Selection",
    "section": "3.5 Knowledge-based Mapping",
    "text": "3.5 Knowledge-based Mapping\nSometimes a categorical variable corresponds to a meaningful numeric attribute. You can encode the category directly using domain knowledge.\nExamples:\n\nPlant species → root depth\nWeather condition (sunny, cloudy, rain) → sunshine hours\n\nThis requires a lookup table and domain understanding, so there is no universal function. You can simply use a join:\n\ndf_Main &lt;- tibble(\n  id = 1:6,\n  plant_type = c(\"Wheat\", \"Corn\", \"Soybean\", \"Wheat\", \"Corn\", \"Rice\")\n)\n\ndf_AttriTable &lt;- tibble(\n  plant_type = c(\"Wheat\", \"Corn\", \"Soybean\", \"Rice\"),\n  root_depth_cm = c(120, 150, 80, 130)\n)\n\ndf_Main |&gt; \n  left_join(df_AttriTable, by = \"plant_type\")\n\n# A tibble: 6 × 3\n     id plant_type root_depth_cm\n  &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;\n1     1 Wheat                120\n2     2 Corn                 150\n3     3 Soybean               80\n4     4 Wheat                120\n5     5 Corn                 150\n6     6 Rice                 130",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#distribution-shaping-transformations",
    "href": "mldl/ml_feature_engineering.html#distribution-shaping-transformations",
    "title": "Feature Engineering and Selection",
    "section": "4.1 Distribution-shaping Transformations",
    "text": "4.1 Distribution-shaping Transformations\nIn some variables, most values cluster within a very narrow range or display strong deviations from a normal-like distribution. Such skewed or compressed distributions may limit model performance. Similar to target transformation, applying transformations to minimize skewness in predictors can improve model fit. Box–Cox (step_BoxCox) and Yeo–Johnson (step_YeoJohnson) are commonly used approaches, although classical options such as logarithmic or square-root transformations also remain effective.\n\ndf_Bochum_KL &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_Bochum_KL.csv\")\ndf_FeatureEng &lt;- df_Bochum_KL[,c(\"evapo_r\", \"evapo_p\", \"RSK\", \"soil_moist\")]\nggplot_hist &lt;- function(df_Data) {\n  df_long &lt;- df_Data |&gt;\n    pivot_longer(\n      cols = everything(),     # all columns; or select specific columns\n      names_to = \"variable\",\n      values_to = \"value\"\n    )\n  \n  # Plot faceted histograms\n  ggplot(df_long, aes(x = value)) +\n    geom_histogram(bins = 30, fill = color_RUB_green, color = color_RUB_blue) +\n    facet_wrap(~ variable, scales = \"free\") +\n    labs(x = \"Value\", y = \"Frequency\") +\n    theme(axis.text.y = element_text(angle = 90, vjust = 0.5, hjust = 0.5))\n}\ngp_Ori &lt;- ggplot_hist(df_FeatureEng[,c(\"evapo_p\", \"RSK\", \"soil_moist\")])\ngp_Ori\n\n\n\n\n\n\n\n\n\n4.1.1 Yeo–Johnson Transformation\nA flexible transformation that accommodates positive, zero, and negative values.\nFormula\nFor a value \\(x\\) and transformation parameter \\(\\lambda\\):\n\\[\nT(x;\\lambda) =\n\\begin{cases}\n\\frac{(x+1)^{\\lambda}-1}{\\lambda}, & x \\ge 0,\\ \\lambda \\ne 0 \\\\\n\\ln(x+1), & x \\ge 0,\\ \\lambda = 0 \\\\\n-\\frac{(1-x)^{2-\\lambda}-1}{2-\\lambda}, & x &lt; 0,\\ \\lambda \\ne 2 \\\\\n-\\ln(1-x), & x &lt; 0,\\ \\lambda = 2\n\\end{cases}\n\\]\nProperties - Works with positive, zero, and negative values. - Estimates the parameter \\(\\lambda\\) to reduce skewness. - Often yields distributions closest to normality. - Less interpretable and slightly slower due to parameter estimation.\n\nrcp_YeoJohnson &lt;- recipe(evapo_r ~ ., data = df_FeatureEng) |&gt;\n  step_YeoJohnson(all_predictors())   # handles zeros and negatives\ndf_YeoJohnson &lt;- prep(rcp_YeoJohnson) |&gt; bake(new_data = NULL)\ngp_YeoJohnson &lt;- ggplot_hist(df_YeoJohnson[,c(\"evapo_p\", \"RSK\", \"soil_moist\")])\ngp_YeoJohnson\n\n\n\n\n\n\n\n\n\n\n4.1.2 Logarithmic Transformation\nA strong transformation for right-skewed positive variables.\nFormula\n\\[\nT(x) = \\log(x)\n\\]\nProperties - Only defined for strictly positive values. - Suitable for multiplicative processes and highly right-skewed data. - Strongly compresses large values. - Not applicable to zero or negative values.\n\nrcp_Log &lt;- recipe(evapo_r ~ ., data = df_FeatureEng) |&gt;\n  step_mutate(across(all_predictors(), ~ .x - min(.x) + 1e-6)) |&gt;\n  step_log(all_predictors())\ndf_Log &lt;- prep(rcp_Log) |&gt; bake(new_data = NULL)\ngp_Log &lt;- ggplot_hist(df_Log[,c(\"evapo_p\", \"RSK\", \"soil_moist\")])\ngp_Log\n\n\n\n\n\n\n\n\n\n\n4.1.3 Square-Root Transformation\nA mild transformation suitable for non-negative, moderately right-skewed data.\nFormula\n\\[\nT(x) = \\sqrt{x}\n\\]\nProperties - Defined for zero and positive values. - Less aggressive than the log transformation. - Reduces right-skewness but insufficient for heavy skewness. - Not applicable to negative values.\n\nrcp_Sqrt &lt;- recipe(evapo_r ~ ., data = df_FeatureEng) |&gt;\n  step_mutate(across(all_predictors(), ~ .x - min(.x) + 1e-6)) |&gt;\n  step_sqrt(all_predictors())\ndf_Sqrt &lt;- prep(rcp_Sqrt) |&gt; bake(new_data = NULL)\ngp_Sqrt &lt;- ggplot_hist(df_Sqrt[,c(\"evapo_p\", \"RSK\", \"soil_moist\")])\ngp_Sqrt\n\n\n\n\n\n\n\n\n\n\n4.1.4 Choosing a Transformation\n\nRight-skewed and strictly positive → Log\nRight-skewed and non-negative → Square-root\nMixed signs or unclear skewness → Yeo–Johnson",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#standardization-and-normalization",
    "href": "mldl/ml_feature_engineering.html#standardization-and-normalization",
    "title": "Feature Engineering and Selection",
    "section": "4.2 Standardization and Normalization",
    "text": "4.2 Standardization and Normalization\nScaling features to comparable magnitudes is crucial for algorithms that rely on linear functions or distance metrics, such as regression models, neural networks, support vector machines, k-means, and hierarchical clustering.\n\n4.2.1 Standardization (Z-Scaling)\nCenters each feature at zero mean and scales to unit variance.\nFormula\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nwhere\n\\(\\mu\\) = mean of the feature,\n\\(\\sigma\\) = standard deviation.\nProperties - Produces features with mean 0 and standard deviation 1. - Preserves distribution shape and skewness. - Outliers still influence the scale. - Widely used for algorithms assuming normal-like distributions.\n\nrcp_Normal &lt;- recipe(evapo_r ~ ., data = df_FeatureEng) |&gt;\n  step_normalize(all_predictors())    # (x − mean) / sd\ndf_Normal &lt;- prep(rcp_Normal) |&gt; bake(new_data = NULL)\ngp_Normal &lt;- ggplot_hist(df_Normal[,c(\"evapo_p\", \"RSK\", \"soil_moist\")])\ngp_Normal\n\n\n\n\n\n\n\n\n\n\n4.2.2 Normalization (Min–Max Scaling)\nRescales each feature to the interval \\([0, 1]\\).\nFormula\n\\[\nx' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n\\]\nProperties - Compresses all values into a fixed range. - Highly sensitive to outliers. - Preserves relative ordering but not distributional shape. - Useful when absolute scales matter or when features must lie in \\([0,1]\\), such as in neural networks and kNN.\n\nrcp_MinMax &lt;- recipe(evapo_r ~ ., data = df_FeatureEng)  |&gt; \n  step_range(all_predictors())\ndf_MinMax &lt;- prep(rcp_MinMax) |&gt; bake(new_data = NULL)\ngp_MinMax &lt;- ggplot_hist(df_MinMax[,c(\"evapo_p\", \"RSK\", \"soil_moist\")])\ngp_MinMax\n\n\n\n\n\n\n\n\n\n\n4.2.3 Choosing a Scaling Method\nUse standardization when\n- variables need equal weight,\n- algorithms assume normally scaled inputs,\n- outliers should not dominate the scaling range.\nUse normalization when\n- features must lie within \\([0,1]\\),\n- distance-based methods or neural networks are used,\n- absolute ranges are important.",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#principal-components-analysis",
    "href": "mldl/ml_feature_engineering.html#principal-components-analysis",
    "title": "Feature Engineering and Selection",
    "section": "6.1 Principal Components Analysis",
    "text": "6.1 Principal Components Analysis\nPrincipal components analysis (PCA) is a method for finding low-dimensional representations of a data set that retain as much of the original variation as possible.\nThe idea is that each of the n observations lives in p-dimensional space, but not all of these dimensions are equally interesting. In PCA we look for a smaller number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension.\nFor Principal Component Analysis (PCA), the first step is to decide how many dimensions (principal components) to retain. A common approach is to base this decision on the eigenvalues of the principal components:\n\nEigenvalue threshold criterion: Retain components with eigenvalues greater than 1. This approach assumes that any component accounting for more variance than an individual original variable is worth keeping.\nCumulative variance explained criterion: Retain enough components to explain a desired proportion of the total variance, such as 75% or 90%. This allows the analyst to select the minimum number of components that capture a substantial fraction of the original data’s variability. Metrics such as Proportion of Variance Explained (PVE) or Cumulative Variance Explained (CVE) are commonly used for this purpose.\n\n\nrcp_NormalAll &lt;- recipe(evapo_r ~ ., data = df_Bochum_KL) |&gt;\n  step_normalize(all_predictors())\ndf_NormalAll &lt;- prep(rcp_NormalAll) |&gt; bake(new_data = NULL)\n\npca_Noraml &lt;- prcomp(df_NormalAll |&gt; select(-evapo_r), scale = FALSE)\neigen_Normal &lt;- pca_Noraml$sdev^2\n\nggplot() +\n  geom_point(aes(x = 1:length(eigen_Normal), y = eigen_Normal)) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = color_TUD_pink) +\n  labs(x = \"PC\", y = \"Eigenvalue\")\n\n\n\n\n\n\n\n\n\nnum_CVE &lt;- cumsum(eigen_Normal / sum(eigen_Normal))\nggplot() +\n  geom_line(aes(x = 1:length(num_CVE), y = num_CVE), color = color_RUB_green) +\n  geom_point(aes(x = 1:length(num_CVE), y = num_CVE), color = color_RUB_green) +\n  geom_hline(yintercept = .9, linetype = \"dashed\", color = color_TUD_pink) +\n  labs(x = \"PC\", y = \"CVE\")\n\n\n\n\n\n\n\n\n\nSet the number of principal components (PCs) manually\n\n\nrcp_PCA4 &lt;- recipe(evapo_r ~ ., data = df_Bochum_KL) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 4)\ndf_PCA4 &lt;- prep(rcp_PCA4) |&gt; bake(new_data = NULL)\n\n\nSet the percentage of variability in the variables\n\n\nrcp_PCA75per &lt;- recipe(evapo_r ~ ., data = df_Bochum_KL) |&gt;\n  step_normalize(all_predictors()) |&gt; \n  step_pca(all_predictors(), threshold = .75)\n\ndf_PCA75per &lt;- prep(rcp_PCA75per) |&gt; bake(new_data = NULL)\n\n\nggplot(df_PCA4, aes(x = .panel_x, y = .panel_y, color = evapo_r, fill = evapo_r)) +\n  geom_point(size = 0.5) +\n  geom_autodensity(alpha = .8, fill = color_RUB_green, color = color_RUB_blue) +\n  facet_matrix(vars(-evapo_r), layer.diag = 2) + \n  scale_color_gradientn(colors = color_DRESDEN) + \n  scale_fill_gradientn(colors = color_DRESDEN)\n\n\n\n\n\n\n\n\nThe most influential variables for each principal component can also be inspected through their loadings.\nThese loadings (or coefficients) indicate how strongly each original feature contributes to a given PC,\nallowing us to interpret what each component represents in terms of the original variables.\n\nplot_top_loadings(\n  prep(rcp_PCA4),\n  component_number &lt;= 4,\n  n = 4,               # number of variables to show per PC\n  type = \"pca\"    # which PCs to plot\n) +\n  scale_fill_manual(values = c(\"TRUE\" = color_RUB_green, \"FALSE\" = color_RUB_blue))+\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(.99, .01),\n        legend.justification = c(1,0))",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#independent-component-analysis-ica",
    "href": "mldl/ml_feature_engineering.html#independent-component-analysis-ica",
    "title": "Feature Engineering and Selection",
    "section": "6.2 Independent Component Analysis (ICA)",
    "text": "6.2 Independent Component Analysis (ICA)\nIndependent Component Analysis (ICA) is a dimension reduction technique that seeks to represent multivariate data as a combination of statistically independent components. Unlike methods such as Principal Component Analysis (PCA), which maximize variance and produce uncorrelated components, ICA focuses on identifying underlying latent factors that are mutually independent.\n\nrcp_ICA4 &lt;- recipe(evapo_r ~ ., data = df_Bochum_KL) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_ica(all_predictors(), num_comp = 4)\ndf_ICA4 &lt;- prep(rcp_ICA4) |&gt; bake(new_data = NULL)\n\nggplot(df_ICA4, aes(x = .panel_x, y = .panel_y, color = evapo_r, fill = evapo_r)) +\n  geom_point(size = 0.5) +\n  geom_autodensity(alpha = .8, fill = color_RUB_green, color = color_RUB_blue) +\n  facet_matrix(vars(-evapo_r), layer.diag = 2) + \n  scale_color_gradientn(colors = color_DRESDEN) + \n  scale_fill_gradientn(colors = color_DRESDEN)\n\n\n\n\n\n\n\n\n\nplot_top_loadings(\n  prep(rcp_ICA4),\n  component_number &lt;= 4,\n  n = 4,               # number of variables to show per PC\n  type = \"ica\"    # which PCs to plot\n) +\n  scale_fill_manual(values = c(\"TRUE\" = color_RUB_green, \"FALSE\" = color_RUB_blue))+\n  theme(legend.position = \"inside\",\n        legend.position.inside = c(.99, .01),\n        legend.justification = c(1,0))",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#uniform-manifold-approximation-and-projection-umap",
    "href": "mldl/ml_feature_engineering.html#uniform-manifold-approximation-and-projection-umap",
    "title": "Feature Engineering and Selection",
    "section": "6.3 Uniform Manifold Approximation and Projection (UMAP)",
    "text": "6.3 Uniform Manifold Approximation and Projection (UMAP)\nUniform Manifold Approximation and Projection (UMAP) is a nonlinear dimension reduction technique designed to project high-dimensional data into a lower-dimensional space while preserving its intrinsic geometric structure. UMAP is particularly effective at maintaining both local relationships (i.e., neighborhood structure) and global patterns in the data. Unlike linear methods such as PCA, UMAP can capture complex, nonlinear structures in the data, making it well-suited for high-dimensional datasets with intricate patterns.\n\nrcp_UMAP4 &lt;- recipe(evapo_r ~ ., data = df_Bochum_KL) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_umap(all_predictors(), num_comp = 4)\ndf_UMAP4 &lt;- prep(rcp_UMAP4) |&gt; bake(new_data = NULL)\n\nggplot(df_UMAP4, aes(x = .panel_x, y = .panel_y, color = evapo_r, fill = evapo_r)) +\n  geom_point(size = 0.5) +\n  geom_autodensity(alpha = .8, fill = color_RUB_green, color = color_RUB_blue) +\n  facet_matrix(vars(-evapo_r), layer.diag = 2) + \n  scale_color_gradientn(colors = color_DRESDEN) + \n  scale_fill_gradientn(colors = color_DRESDEN)",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#generalized-low-rank-models-glrm",
    "href": "mldl/ml_feature_engineering.html#generalized-low-rank-models-glrm",
    "title": "Feature Engineering and Selection",
    "section": "6.4 Generalized Low Rank Models (GLRM)",
    "text": "6.4 Generalized Low Rank Models (GLRM)\nGeneralized Low Rank Models (GLRM) is a generalization of PCA and matrix factorization, which is more flexibel and useable for categorei variables. The the orginal variables iwll replace by a dimesnionreduced matrix X, which \\(A = X \\times Y\\). This approach is not aviable in recipes framework and more complex, so we do not give a excies. more details is in https://bradleyboehmke.github.io/HOML/GLRM.html aviable",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#distance",
    "href": "mldl/ml_feature_engineering.html#distance",
    "title": "Feature Engineering and Selection",
    "section": "7.1 Distance",
    "text": "7.1 Distance\nComputing the distance to a reference point is often helpful in spatial-related datasheet. With recipes, this can be achieved using steps such as step_geodist, allowing us to transform coordinates (e.g., latitude/longitude) into meaningful spatial distance features.\n\nrec_TimeSpat &lt;- recipe(target ~ ., data = df_TimeSpat)  |&gt; \n  step_geodist(lat, lon, ref_lat = 52.52, ref_lon = 13.40, name = \"dist_to_berlin\")\nbake(prep(rec_TimeSpat), new_data = df_TimeSpat) |&gt; head(100) |&gt; DT::datatable()",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#date-and-holiday",
    "href": "mldl/ml_feature_engineering.html#date-and-holiday",
    "title": "Feature Engineering and Selection",
    "section": "7.2 Date and holiday",
    "text": "7.2 Date and holiday\nDate variables can be expanded into more informative features such as weekday, weekend, month, or season. Additionally, holiday signatures can be added to capture special events or non-working days. Recipes provides steps like step_date and time-series extensions such as step_holiday.\n\nrec_TimeSpat &lt;- recipe(target ~ ., data = df_TimeSpat)  |&gt; \n  step_date(date) |&gt; \n  step_holiday(date)\nbake(prep(rec_TimeSpat), new_data = df_TimeSpat) |&gt; head(100) |&gt; DT::datatable()",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#lag",
    "href": "mldl/ml_feature_engineering.html#lag",
    "title": "Feature Engineering and Selection",
    "section": "7.3 Lag",
    "text": "7.3 Lag\nFor time-dependent data, the current value may depend on previous time steps. Lag transformations create new variables such as lag-1, lag-7, or lag-30. Recipes supports this through step_lag, which is especially useful in time-series prediction tasks.\n\nrec_TimeSpat &lt;- recipe(target ~ ., data = df_TimeSpat)  |&gt; \n  step_lag(target, lag = 3)\nbake(prep(rec_TimeSpat), new_data = df_TimeSpat) |&gt; head(100) |&gt; DT::datatable()",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_feature_engineering.html#missing-values-imputation",
    "href": "mldl/ml_feature_engineering.html#missing-values-imputation",
    "title": "Feature Engineering and Selection",
    "section": "7.4 Missing values imputation",
    "text": "7.4 Missing values imputation\nAlthough missing values—especially in time-series or spatial data—should ideally be treated during the EDA phase, recipes still offers convenient imputation options:\n\nstep_impute_knn: k-nearest neighbour imputation\nstep_impute_linear: linear interpolation\nstep_impute_roll: rolling window based filling\n\nThese steps integrate seamlessly into modelling pipelines, but it remains good practice to address outliers and missing values as early as possible during EDA for better control and transparency.",
    "crumbs": [
      "ML / DL",
      "Feature Engineering and Selection"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html",
    "href": "mldl/ml_basic_concept.html",
    "title": "Basic Concept",
    "section": "",
    "text": "Machine learning (ML) is a discipline concerned with developing computational methods that identify patterns in data and generate predictions or decisions without the need for task-specific programming. Modern ML comprises a wide range of approaches, from classical statistical learning techniques to contemporary deep learning methods based on multilayer neural networks. This text focuses exclusively on classical machine learning techniques and does not address deep learning.\nBefore examining specific algorithms or datasets, it is useful to outline the general workflow that characterizes most ML applications. Because machine learning is fundamentally data-driven, its problems are commonly divided into two primary categories:\n\nsupervised learning, which uses datasets containing labeled observations with known input features and corresponding target values;\nunsupervised learning, which addresses unlabeled datasets and aims to identify latent structures or patterns without predefined target information.\n\n\n\nSupervised learning methods rely on datasets composed of input–output pairs. The objective is to approximate a function that captures the relationship between the inputs and their associated outputs. Supervised learning tasks are typically grouped into two types:\n\nclassification, which concerns the prediction of categorical outcomes;\nregression, which focuses on predicting continuous numerical values.\n\nIn both cases, the model must learn systematic relationships that meaningfully link input features to their corresponding targets.\n\n\n\nUnsupervised learning is applied when no target variable is provided. Its purpose is to uncover underlying structures, regularities, or groupings within the data.\nA widely used unsupervised approach is clustering, which partitions observations into groups such that items within the same cluster exhibit greater similarity to one another than to items in different clusters.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#supervised-learning",
    "href": "mldl/ml_basic_concept.html#supervised-learning",
    "title": "Basic Concept",
    "section": "",
    "text": "Supervised learning methods rely on datasets composed of input–output pairs. The objective is to approximate a function that captures the relationship between the inputs and their associated outputs. Supervised learning tasks are typically grouped into two types:\n\nclassification, which concerns the prediction of categorical outcomes;\nregression, which focuses on predicting continuous numerical values.\n\nIn both cases, the model must learn systematic relationships that meaningfully link input features to their corresponding targets.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#unsupervised-learning",
    "href": "mldl/ml_basic_concept.html#unsupervised-learning",
    "title": "Basic Concept",
    "section": "",
    "text": "Unsupervised learning is applied when no target variable is provided. Its purpose is to uncover underlying structures, regularities, or groupings within the data.\nA widely used unsupervised approach is clustering, which partitions observations into groups such that items within the same cluster exhibit greater similarity to one another than to items in different clusters.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#target-engineering",
    "href": "mldl/ml_basic_concept.html#target-engineering",
    "title": "Basic Concept",
    "section": "2.1 Target engineering",
    "text": "2.1 Target engineering\nThe target is the variable treated as the outcome of the model, the quantity the model aims to predict. Target values in training datasets may contain noise, missing entries, or extreme outliers, any of which can impair model performance.\nCommon target-engineering operations include correcting inconsistent values, smoothing or aggregating observations, applying transformations to reduce skewness, or converting between continuous and categorical forms when appropriate. A clearly defined and well-processed target provides a meaningful objective for the learning procedure.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#feature-engineering",
    "href": "mldl/ml_basic_concept.html#feature-engineering",
    "title": "Basic Concept",
    "section": "2.2 Feature engineering",
    "text": "2.2 Feature engineering\nFeatures supply the information that the model uses to generate predictions. Effective feature engineering enhances the model’s ability to capture relevant structure in the data.\n\n2.2.1 Numeric feature engineering\nNumeric features often differ in both their distribution and their scale, and these issues require distinct preprocessing strategies. Transformations modify the shape of an individual feature’s distribution, while normalization (or standardization) adjust the scales of multiple features to make them comparable.\n\nTransformations (distribution shaping)\nTransformations are applied to single features to alter their distribution. They can reduce skewness, stabilize variance, or help satisfy model assumptions. Examples include\n\nlogarithmic,\nsquare-root,\nreciprocal, and\nBox–Cox transformations. These operations reshape a feature’s distribution but do not make different features comparable in scale.\n\nNormalization and standardization (scale adjustment)\nNormalization and standardization rescale features so they share similar or identical ranges, preventing any single feature from dominating the learning process due to its magnitude.\n\nStandardization rescales a feature to zero mean and unit variance.\n\nNormalization, such as min–max scaling, maps a feature to a fixed interval, typically [0, 1].\nThese methods do not change distributional shape but ensure comparability in scale, which is important for distance-based models or gradient-based optimization.\n\n\n\n\n2.2.2 Categorical feature engineering\nCategorical variables must be converted into numerical form, as most machine learning algorithms operate on numeric inputs. Unlike numeric features, categorical variables do not support distribution-shaping transformations. Consequently, categorical feature engineering focuses on encoding strategies that represent categories numerically while preserving their meaning.\nCommon encoding methods include:\n\nOne-hot encoding, which creates binary indicator variables for each category without imposing artificial ordering.\nLabel encoding, which assigns integer codes to categories and is suitable for models that do not interpret these codes as ordinals, such as tree-based methods.\n\nSome tree-based algorithms can handle categorical variables directly or interpret encoded integers in a non-ordinal way. Nonetheless, choosing an appropriate encoding method remains essential to ensure that the numerical representation aligns with model assumptions.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#feature-filtering-and-dimension-reduction",
    "href": "mldl/ml_basic_concept.html#feature-filtering-and-dimension-reduction",
    "title": "Basic Concept",
    "section": "2.3 Feature filtering and dimension reduction",
    "text": "2.3 Feature filtering and dimension reduction\nFeature filtering removes variables that provide little or no useful information for the learning task. Features with\n\nvery low variance,\nhigh proportions of missing values, or\nstrong multicollinearity\n\noften contribute minimally to predictive performance.\nEliminating such variables reduces noise, simplifies the modelling process, and can improve both accuracy and efficiency. Often, a smaller, more informative feature set is preferable to a high-dimensional collection of irrelevant or redundant variables.\nDimension reduction methods also decrease the number of features but take a different approach. Instead of discarding variables, they construct new, lower-dimensional representations that preserve essential information. These methods simplify high-dimensional data, reduce noise, and often improve computational efficiency and predictive performance. Conceptually, they identify the most informative directions or structures in the data while disregarding variation that contributes little to the modelling objective.\nPrincipal Component Analysis (PCA) creates new variables (principal components) that are linear combinations of the original features. These components are orthogonal and ranked according to the variance they explain. More flexible approaches, such as Generalized Low Rank Models (GLRM), extend this idea by allowing additional constraints or custom loss functions, making them adaptable to a wider range of data types and modelling requirements.\nIn summary, feature filtering removes uninformative variables directly, whereas dimension reduction retains information by reorganizing it into new combined features.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#data-splitting-and-resampling",
    "href": "mldl/ml_basic_concept.html#data-splitting-and-resampling",
    "title": "Basic Concept",
    "section": "3.1 Data splitting and resampling",
    "text": "3.1 Data splitting and resampling\nBefore model development begins, the dataset is usually divided into separate subsets. This division allows the model to be trained on one subset while another is reserved for evaluation, ensuring that performance reflects generalization rather than memorization.\nThe basic approach uses two groups:\n\ntraining dataset: used to estimate model parameters\n\ntesting dataset: used for final performance assessment\n\nWhen more rigorous tuning is required, a third group is added:\n\ntraining dataset: used for parameter estimation\n\nvalidation dataset: used for hyperparameter tuning, model selection, and early stopping\n\ntesting dataset: used exclusively for the final, unbiased assessment of model performance\n\nA key principle is that performance must not be evaluated solely on data used during training, as this can lead to overly optimistic results.\nResampling techniques provide more reliable estimates of model performance, particularly when data are limited. Instead of relying on a single train–test split, methods such as cross-validation repeatedly partition the data into training and validation sets, while bootstrapping generates samples with replacement to estimate variability. These techniques reduce sensitivity to any single split and yield more stable and trustworthy performance estimates.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#model-creation",
    "href": "mldl/ml_basic_concept.html#model-creation",
    "title": "Basic Concept",
    "section": "3.2 Model creation",
    "text": "3.2 Model creation\nModel creation involves selecting an appropriate algorithm and preparing it for training. This phase includes identifying relevant features, specifying the model structure, and configuring initial settings. The objective is to establish a suitable initial model that can be trained and refined.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#model-training",
    "href": "mldl/ml_basic_concept.html#model-training",
    "title": "Basic Concept",
    "section": "3.3 Model training",
    "text": "3.3 Model training\nDuring model training, the algorithm learns from the training dataset by iteratively adjusting its internal parameters. Through repeated exposure to input–output examples, the model minimizes the discrepancy between predictions and observed values. Training must balance complexity and generalization: excessive training may lead to overfitting, whereas insufficient training may result in underfitting.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#model-tuning",
    "href": "mldl/ml_basic_concept.html#model-tuning",
    "title": "Basic Concept",
    "section": "3.4 Model tuning",
    "text": "3.4 Model tuning\nModel tuning, or hyperparameter tuning, adjusts the external settings of the algorithm—parameters that are fixed before training and determine how the model learns.\nExamples include regularization strength, learning rates, tree depth, and the number of neighbors in k-nearest neighbors. Because hyperparameters have a substantial influence on performance, tuning is typically guided by validation data or cross-validation to identify configurations that yield optimal generalization.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#model-performance-assessment",
    "href": "mldl/ml_basic_concept.html#model-performance-assessment",
    "title": "Basic Concept",
    "section": "3.5 Model Performance Assessment",
    "text": "3.5 Model Performance Assessment\nDuring training and tuning, it is essential to monitor and compare model performance. Performance metrics provide quantitative indicators of a model’s predictive quality and guide decisions related to training, hyperparameter optimization, and model selection.\nWhile the model is being trained, metrics are computed on both the training and validation sets to track learning progress, identify overfitting, and adjust hyperparameters. After training, the model is evaluated on a separate testing set to obtain an unbiased estimate of its generalization ability, ensuring that performance reflects real-world applicability rather than memorization of the training data.\n\n3.5.1 Classification Metrics\nFor classification tasks, common metrics include accuracy, Brier score, and ROC AUC, each capturing different aspects of model performance:\n\nAccuracy: Measures the proportion of correct predictions relative to all predictions:\n\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\nValues range from 0 to 1, with higher values indicating better performance. Accuracy is intuitive but can be misleading for imbalanced datasets, where a model predicting only the majority class may appear highly accurate yet be uninformative.\n\nBrier Score: Evaluates the accuracy of probabilistic predictions. For a binary outcome:\n\n\\[\nBS = (p - o)^2\n\\]\nwhere \\(p\\) is the predicted probability and \\(o\\) the actual outcome (1 or 0). The mean Brier score across the dataset ranges from 0 to 1, with lower values indicating better calibration. Unlike accuracy, it accounts for both confidence and correctness of predictions.\n\nROC AUC: Measures a classifier’s ability to discriminate between classes across all possible thresholds. The ROC curve plots:\n\n\\[\n\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\qquad\\text{against}\\qquad\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n\\]\nThe area under the curve (AUC) ranges from 0.5 (random guessing) to 1.0 (perfect separation). ROC AUC is threshold-independent and particularly useful for imbalanced datasets.\n\n\n3.5.2 Regression Metrics\nFor regression tasks, metrics quantify the deviation between predicted and observed values:\n\nRoot Mean Squared Error (RMSE): Measures the average magnitude of prediction errors, penalizing large deviations:\n\n\\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}( \\hat{y}_i - y_i )^2}\n\\]\n\nMean Absolute Error (MAE): Computes the average absolute difference between predictions and observations, providing robustness to outliers:\n\n\\[\n\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|\\hat{y}_i - y_i|\n\\]\n\nCoefficient of Determination (R²): Indicates the proportion of variance in the observed data explained by the model:\n\n\\[\nR^2 = 1 - \\frac{\\sum_{i=1}^{n}( \\hat{y}_i - y_i )^2}{\\sum_{i=1}^{n}( y_i - \\bar{y} )^2}\n\\]\nWhere:\n\n$ _i $ is the predicted value for the \\(i\\)-th observation\n\n$ y_i $ is the observed (true) value for the \\(i\\)-th observation\n\n$ {y} $ is the mean of all observed values\n\n$ n $ is the total number of observations\n\n$ $ denotes summation over all observations from \\(i = 1\\) to \\(n\\)\n\nBy integrating performance metrics into both training and evaluation phases, practitioners can iteratively refine models, prevent overfitting, optimize hyperparameters, and select models that provide reliable predictions on unseen data. This combined approach of monitoring during training and assessing after training ensures that models are both accurate and generalizable.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#predict-using-trained-model",
    "href": "mldl/ml_basic_concept.html#predict-using-trained-model",
    "title": "Basic Concept",
    "section": "3.6 Predict Using Trained Model",
    "text": "3.6 Predict Using Trained Model\nOnce a model has been trained, it can be used to generate predictions on new or unseen data. This step applies the learned relationships from the training phase to make informed estimates of the target variable for previously unobserved samples. Predicting with a trained model is a critical part of the modeling workflow, as it enables evaluation of generalization performance, comparison across different models, and practical application of the model to real-world data.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#model-evaluation",
    "href": "mldl/ml_basic_concept.html#model-evaluation",
    "title": "Basic Concept",
    "section": "3.7 Model Evaluation",
    "text": "3.7 Model Evaluation\nModel evaluation involves systematically measuring how well a trained model performs on data that were not used during training. This process ensures that the model generalizes effectively to unseen data, facilitates comparison among alternative models, and identifies potential issues such as overfitting, underfitting, or insufficient feature quality. Performance metrics serve as quantitative indicators of the model’s predictive quality and guide both training decisions and model selection.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#model-selection",
    "href": "mldl/ml_basic_concept.html#model-selection",
    "title": "Basic Concept",
    "section": "3.8 Model selection",
    "text": "3.8 Model selection\nModel selection involves choosing the most appropriate model from a set of candidates, which may differ in algorithmic approach, hyperparameter settings, or feature engineering choices. Selection relies on fair comparison using consistent metrics and validation procedures. The preferred model is the one that demonstrates strong and stable generalization, not merely the best training performance.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#modelling-workflow",
    "href": "mldl/ml_basic_concept.html#modelling-workflow",
    "title": "Basic Concept",
    "section": "3.9 Modelling workflow",
    "text": "3.9 Modelling workflow\nThe development of an effective model is both iterative and heuristic. It is rarely clear in advance what a dataset requires, and it is common to evaluate and modify multiple approaches before finalizing a model.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "mldl/ml_basic_concept.html#overfitting",
    "href": "mldl/ml_basic_concept.html#overfitting",
    "title": "Basic Concept",
    "section": "3.10 Overfitting",
    "text": "3.10 Overfitting\nOverfitting occurs when a model captures patterns that are specific to the training data but do not generalize to new samples. This typically arises when the model is overly complex or when it learns from noise or idiosyncratic patterns in the training set. A well-designed modelling process incorporates safeguards—such as validation, regularization, and resampling—to reduce the risk of overfitting and promote generalizable performance.",
    "crumbs": [
      "ML / DL",
      "Basic Concept"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcom Page",
    "section": "",
    "text": "Welcome to HydroSimul: Your Gateway to Understanding Hydrological Simulation\nWe’re thrilled to have you at HydroSimul, your premier resource for delving into the realm of hydrological simulation. Here, we curate a vast collection of datasets and offer cutting-edge techniques to empower your exploration.\nAt HydroSimul, we believe in the power of collaboration. If you want to share your knowledge, experience, and skills in the field, please contact us at hydro.simul@gmail.com. We also welcome your advice.\nOur website serves as a comprehensive hub for your hydrological journey:"
  },
  {
    "objectID": "index.html#dataset-collection",
    "href": "index.html#dataset-collection",
    "title": "Welcom Page",
    "section": "1. Dataset Collection:",
    "text": "1. Dataset Collection:\nNumerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "index.html#data-processing",
    "href": "index.html#data-processing",
    "title": "Welcom Page",
    "section": "2. Data Processing:",
    "text": "2. Data Processing:\nOn this page, we provide you with a wealth of data processing tools and techniques. Discover how to adeptly clean, preprocess, and convert raw hydrological data into a valuable format suitable for analysis and modeling."
  },
  {
    "objectID": "index.html#data-analysis",
    "href": "index.html#data-analysis",
    "title": "Welcom Page",
    "section": "3. Data Analysis:",
    "text": "3. Data Analysis:\nUncover concealed statistical insights and trends within your data. Our comprehensive guides and tutorials are designed to empower you with the skills to effectively analyze hydrological data."
  },
  {
    "objectID": "index.html#hydrological-modeling",
    "href": "index.html#hydrological-modeling",
    "title": "Welcom Page",
    "section": "4. Hydrological Modeling:",
    "text": "4. Hydrological Modeling:\nTake your understanding to the next level with hydrological modeling. Explore various models and model frameworks and acquire hands-on experience in simulating complex hydrological processes. Additionally, we provide extensive resources on calibration algorithms and strategies to significantly improve your modeling outcomes."
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Dataset",
    "section": "",
    "text": "Numerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "dataset/geoph.html",
    "href": "dataset/geoph.html",
    "title": "Geophysical",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/geoph.html#h2",
    "href": "dataset/geoph.html#h2",
    "title": "Geophysical",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html",
    "href": "dataprocess/visual_ggplot2Basic.html",
    "title": "ggplot2 Basic",
    "section": "",
    "text": "The work of visualization involves the process of mapping the data into visualized geometry. There are three basic components to this: data, geometry, and aesthetic mappings. In this article, we will step-by-step learn about these three components.\n\nlibrary(tidyverse) # or: library(ggplot2)\n\n\n\nThe data structure in ggplot2 is organized in a data.frame or a similar structure like tibble. As an example, we will use the mpg dataset from ggplot2.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nThe dataframe can (should) contain all the information that you want to visualize.\n\n\n\nThe aesthetic mappings define the connection (mapping) between variables in the data and visual properties of geometry. The properties of the geometry are derived from the chosen geometry. For example, almost every plot maps a variable to x and y to determine the position, and color or size provides additional details.\nThis mapping is established with the aes() function. In the aes() function, you need to use the format property name = variable name to connect the data and geometry, like aes(x = displ, y = hwy).\nThe aes() function simply defines which variable is connected to which aesthetics, but for more detailed modifications of aesthetics, you can use the scale_*() functions. More details are provided in Section 2.3.\n\n\n\nWith the same dataset, we can also choose different geometries, such as scatter points, lines, or bars. In ggplot2, all the geometries are defined using the geom_*() functions, like geom_point() and geom_line(). These functions specify how the data should be visually represented.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#data",
    "href": "dataprocess/visual_ggplot2Basic.html#data",
    "title": "ggplot2 Basic",
    "section": "",
    "text": "The data structure in ggplot2 is organized in a data.frame or a similar structure like tibble. As an example, we will use the mpg dataset from ggplot2.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nThe dataframe can (should) contain all the information that you want to visualize.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#aesthetic-mappings",
    "href": "dataprocess/visual_ggplot2Basic.html#aesthetic-mappings",
    "title": "ggplot2 Basic",
    "section": "",
    "text": "The aesthetic mappings define the connection (mapping) between variables in the data and visual properties of geometry. The properties of the geometry are derived from the chosen geometry. For example, almost every plot maps a variable to x and y to determine the position, and color or size provides additional details.\nThis mapping is established with the aes() function. In the aes() function, you need to use the format property name = variable name to connect the data and geometry, like aes(x = displ, y = hwy).\nThe aes() function simply defines which variable is connected to which aesthetics, but for more detailed modifications of aesthetics, you can use the scale_*() functions. More details are provided in Section 2.3.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#geometry",
    "href": "dataprocess/visual_ggplot2Basic.html#geometry",
    "title": "ggplot2 Basic",
    "section": "",
    "text": "With the same dataset, we can also choose different geometries, such as scatter points, lines, or bars. In ggplot2, all the geometries are defined using the geom_*() functions, like geom_point() and geom_line(). These functions specify how the data should be visually represented.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#element_",
    "href": "dataprocess/visual_ggplot2Basic.html#element_",
    "title": "ggplot2 Basic",
    "section": "2.1 element_*()",
    "text": "2.1 element_*()\nIn conjunction with the theme system, the element_ functions specify the display of how non-data components of the plot are drawn (Wickham 2009).\nThere are four main elements to specify the appearance of elements for the plot, axis, and more (details in ggplot2 Theme elements):\n\nelement_blank(): draws nothing and assigns no space.\nelement_rect(): used for borders and backgrounds.\nelement_line(): defines the appearance of lines.\nelement_text(): controls the appearance of text.\n\n\nelement_blank()\n\nelement_rect(\n  fill = NULL,\n  colour = NULL,\n  linewidth = NULL,\n  linetype = NULL,\n  color = NULL,\n  inherit.blank = FALSE,\n  size = deprecated()\n)\n\nelement_line(\n  colour = NULL,\n  linewidth = NULL,\n  linetype = NULL,\n  lineend = NULL,\n  color = NULL,\n  arrow = NULL,\n  inherit.blank = FALSE,\n  size = deprecated()\n)\n\nelement_text(\n  family = NULL,\n  face = NULL,\n  colour = NULL,\n  size = NULL,\n  hjust = NULL,\n  vjust = NULL,\n  angle = NULL,\n  lineheight = NULL,\n  color = NULL,\n  margin = NULL,\n  debug = NULL,\n  inherit.blank = FALSE\n)\n\nSome common characters include:\n\nfill: fill color\ncolour or color: Line/border/text color. color is an alias for colour.\nlinewidth: Line/border size in mm.\nsize: Text size in pts.\n\nYou can find more scripts and detailed examples in the article The elements of a plot.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#title-modify",
    "href": "dataprocess/visual_ggplot2Basic.html#title-modify",
    "title": "ggplot2 Basic",
    "section": "2.2 Title Modify",
    "text": "2.2 Title Modify\nThe element_*() functions specifically define the style, but for certain text elements like titles or axis titles, you need to provide specific names. By default, these elements are named after the variables you have mapped them to.\n\nlabs(): This function allows you to modify all text related to titles, subtitles, captions, and tags for the plot, as well as aesthetic names (e.g., x, y, color, fill, etc.).\n\nFor individual elements, you can use specific functions:\n\nxlab(): Set the x-axis label.\nylab(): Set the y-axis label.\nggtitle(): Set the main plot title.\n\nBy using these functions, you can precisely control the naming and appearance of text elements in your plot.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#sec-scale",
    "href": "dataprocess/visual_ggplot2Basic.html#sec-scale",
    "title": "ggplot2 Basic",
    "section": "2.3 scale_*()",
    "text": "2.3 scale_*()\nThey take your data and turn it into something that you can see, like size, colour, position or shape. They also provide the tools that let you interpret the plot: the axes and legends. You can generate plots with ggplot2 without knowing how scales work, but understanding scales and learning how to manipulate them will give you much more control (Wickham 2009).\n\n2.3.1 Axis scale_x_*() and scale_y_*()\nThere are two main types of axes: discrete, which represents values not in numerical order, and continuous axes.\n\nscale_*_discrete()\nscale_*_continuous()\n\nscale_*_log10()\nscale_*_sqrt()\nscale_*_reverse()\n\n\nAdditionally, there are specific axes (based on continuous) for dates (scale_*_date()) and bins (scale_*_binned()), commonly used in histograms.\nThe common attributes for axes include:\n\nname: axis title\nbreaks / minor_breaks: break points for axis ticks and panel grid lines\nlabels: axis text of break points\nlimits: axis ranges\n\n\n\n2.3.2 Colour scale_color_*() and scale_fill_*()\nLike axes, color mapping also has challenges with discrete and continuous data.\nFor continuous color mapping, there are:\n\nContinuous: scale_colour_continuous()\n\ntype: custom\n\nGradient:\n\nscale_colour_gradient() for two (low and high) colors\nscale_colour_gradient2() for three (low, mid, and high) colors\nscale_colour_gradientn() for defined colors\n\nBinned: assigns discrete color bins to the continuous values\n\nscale_colour_binned()\n\n\nFor discrete data, use scale_colour_manual() to define the colors for values.\n\n\n2.3.3 Other scales\nExcept for axes and color, other aesthetics can be simpler with scale_*_manual() to define:\n\nscale_size_manual()\nscale_shape_manual()\nscale_linetype_manual()\nscale_linewidth_manual()\nscale_alpha_manual()",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#fixed-elements-for-specific-geometry",
    "href": "dataprocess/visual_ggplot2Basic.html#fixed-elements-for-specific-geometry",
    "title": "ggplot2 Basic",
    "section": "2.4 Fixed elements for specific geometry",
    "text": "2.4 Fixed elements for specific geometry\nThe scale_*() functions primarily control the attributes of elements using data. However, there are situations where you may want to set fixed values for a specific element directly. For example, you can set the linewidth for colored lines: geom_line(aes(color = dis), linewidth = 2). The settings outside the aes() function will be treated as fixed values for that particular geometry.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#syntax",
    "href": "dataprocess/visual_ggplot2Basic.html#syntax",
    "title": "ggplot2 Basic",
    "section": "3.1 Syntax",
    "text": "3.1 Syntax\nThe syntax of ggplot2 follows a layered approach, where you start with the base layer of data and progressively add additional layers to create a complex plot.\nThe basic syntax involves using the ggplot() function to initiate the plot, specifying the data and aesthetics using the aes() function, and then adding (use +) geometric elements with functions like geom_point() or geom_line(). Each added layer enhances the plot, and you can further customize it using various options. The syntax is intuitive and modular, allowing for flexible and expressive visualizations.\n\nggplot(data = my_Data, aes(x = my_X, y = my_Y)) +\n  geom_point()\n\nIn ggplot2, the aes() function (also data), which defines the aesthetic mappings, can be placed either within the ggplot() function for all geoms or inside the specific geom_*() function to apply mappings only to that geometry. This flexibility allows for clear and concise syntax, as aesthetic mappings can be specified globally for the entire plot or tailored for individual geometric layers.\n\nggplot() +\n  geom_point(data = my_Data, aes(x = my_X, y = my_Y))\n\nNot only that, but you can also divide the mapping into several parts, with common mappings in the initial ggplot() function and other specific mappings in the given geom_*() functions. This allows for flexibility and customization in defining aesthetic mappings for different geometries in the same plot.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#first-ggplot",
    "href": "dataprocess/visual_ggplot2Basic.html#first-ggplot",
    "title": "ggplot2 Basic",
    "section": "3.2 First ggplot()",
    "text": "3.2 First ggplot()\n\n3.2.1 Task\nWith the above three short introductions, we can now try the first plot with the mpg dataset.\nThe task is as follows:\n\nData: Using the mpg dataset.\n\nVariables: displ, hwy, class\n\nGeometry:\n\nColored scatter plot: geom_point()\nSmoothed line: geom_smooth()\n\nAesthetic Mappings:\n\nx-dimension with displ (applied to both geoms): x = displ\ny-dimension with hwy (applied to both geoms): y = hwy\nColored with class (only for scatter plot): color = class\n\n\n\n\n3.2.2 Codes\n\nggplot(data = mpg, aes(x = displ, y = hwy))+\n  geom_point(aes(colour = class)) + \n  geom_smooth()",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#customize-the-elements",
    "href": "dataprocess/visual_ggplot2Basic.html#customize-the-elements",
    "title": "ggplot2 Basic",
    "section": "3.3 Customize the elements",
    "text": "3.3 Customize the elements\nFor the continuous axis, we will store the default plot in a variable gp_Default, and then we can use + to add future custom settings.\n\ngp_Default &lt;- ggplot(data = mpg, aes(x = displ, y = hwy))+\n  geom_point(aes(colour = class)) + \n  geom_smooth() \n\n\n3.3.1 Label and Text\n\nX-Axis in \"Engine displacement (L)\"\nY-Axis in \"MPG in highway  (mi / gal)\"\nTitle in \"Fuel economy data\"\nLegend (color) title in \"Car type\"\n\n\ngp_Default +\n  labs(x = \"Engine displacement (L)\", \n       y = \"MPG in highway (mi/gal)\", \n       colour = \"Car type\",\n       title = \"Fuel economy data\")\n\n\n\n\n\n\n\n\n\n\n3.3.2 Axes\n\nX-limits: 2 to 5\nY-breaks: from 15 to 45 every 5\n\n\ngp_Default +\n  scale_x_continuous(limits = c(2, 5)) +\n  scale_y_continuous(breaks = seq(15, 45, 5)) \n\n\n\n\n\n\n\n\n\n\n3.3.3 Colour\n\nSmooth line in red\npoints color in rainbow color\n\n\ngp_Default +\n  geom_smooth(color = \"red\") +\n  scale_color_manual(values = rainbow(7))",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_process.html",
    "href": "dataprocess/timeserises_process.html",
    "title": "Basic Processing",
    "section": "",
    "text": "In this article, we will cover fundamental techniques for manipulating and analyzing time series data. This includes tasks such as creating time series, summarizing data based on time indices, identifying trends, and more.\nTime series data often comes with specific considerations related to time zones, varying numbers of days in months, and leap years.\n\nTime Zones: Time series data collected from different regions or sources may be recorded in various time zones. Converting data to a consistent time zone is crucial to ensure accurate analysis and visualization, especially for data with hourly resolution.\nVarying Days in Months: Some months have 30 days, while others have 31, and February can have 28 or 29 days in leap years. This variation should be considered when performing calculations based on monthly or daily data.\nLeap Years: Leap years, which occur every four years, add an extra day (February 29) to the calendar. Analysts need to account for leap years when working with annual time series data to avoid inconsistencies.\n\nProperly accounting for these specifics is crucial for accurate analysis and interpretation of time series data.\n\n1 Library\nTime series data structures are not standard in R, but the xts package is commonly used to work with time indices. However, it’s important to note that for processes that don’t rely on specific time indexing, the original data structure is sufficient. Time series structures are particularly useful when you need to perform time-based operations and analysis.\n\nlibrary(xts)\nlibrary(tidyverse)\n\n\n\n2 Example Files\nThe example files provided consist of three discharge time series for the Ruhr River in the Rhein basin, Germany. These data sets are sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github.\n\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Oeventrop &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Oeventrop_2761759000100.csv\"\nfn_Villigst &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Villigst_2765590000100.csv\"\n\n\n\n3 Create data\nBefore creating a time series structure, the data should be loaded into R. Time series in R can typically (only) support two-dimensional data structures, such as matrices and data frames.\nIf the date-time information is not correctly recognized during reading or if there is no time data present, you need to make sure that you have a valid time index.\nThere are two primary ways to create a time series in R:\n\nxts(): With this method, you explicitly specify the time index and create a time series object. This is useful when you have a matrix with an external time index.\nas.xts(): This method is more straightforward and is suitable when you have a data frame with a date column. The function will automatically recognize the date column and create a time series.\n\n\n# Read a CSV file as data.frame\ndf_Bachum &lt;- read_csv2(fn_Bachum, skip = 10, col_names = FALSE)\ndf_Villigst &lt;- read_csv2(fn_Villigst, skip = 10, col_names = FALSE)\n\n# Convert Date column to a Date type\ndf_Bachum$X1 &lt;- as_date(df_Bachum$X1, format = \"%d.%m.%Y\")\ndf_Villigst$X1 &lt;- as_date(df_Villigst$X1, format = \"%d.%m.%Y\")\n\n# Create an xts object\nxts_Bachum &lt;- xts(df_Bachum$X2, order.by = df_Bachum$X1)\nxts_Villigst &lt;- as.xts(df_Villigst)\n\n\n\n4 Merging Several Time Series\nIn R, the time index is consistent and follows a standardized format. This consistency in time indexing makes it easy to combine multiple time series into a single dataset based on their time index.\n\nmerge()\n\n\nxts_Rhur &lt;- merge(xts_Bachum, xts_Villigst)\nnames(xts_Rhur) &lt;- c(\"Bachum\", \"Villigst\")\n\nIt’s worth noting that when working with time series data in R, the length of the time series doesn’t necessarily have to be the same for all time series. This flexibility allows you to work with data that may have missing or varying data points over time, which is common in many real-world scenarios.\n\nlength(xts_Bachum)\n\n[1] 12053\n\nlength(xts_Villigst)\n\n[1] 11499\n\n\n\n\n5 Subsetting (Index with time)\nYou can work with time series data in R using both integer indexing, and time-based indexing using time intervals.\n\n# Create a time sequence\nts_Inteval &lt;- seq(as_date(\"1996-01-01\"), as_date(\"1996-12-31\"), \"days\")\n\n# Subset\nxts_Inteval &lt;- xts_Rhur[ts_Inteval, ]\nhead(xts_Inteval, 10)\n\n           Bachum Villigst\n1996-01-01 13.459    11.03\n1996-01-02 12.331    10.03\n1996-01-03 11.112     9.12\n1996-01-04 11.272     8.11\n1996-01-05 11.412     8.71\n1996-01-06 11.526     8.29\n1996-01-07 12.589     9.45\n1996-01-08 12.508    10.09\n1996-01-09 12.336     9.42\n1996-01-10 12.510     8.47\n\n\n\n\n6 Rolling Windows\nMoving averages are a valuable tool for smoothing time series data and uncovering underlying trends or patterns. With rolling windows, you can calculate not only the mean value but also other statistics like the median and sum. To expand the range of functions available, you can utilize the rollapply(). This enables you to apply a wide variety of functions to your time series data within specified rolling windows.\n\nrollmean()\nrollmedian()\nrollsum()\nrollmax()\n\n\nxts_RollMean &lt;- rollmean(xts_Inteval, 7)\nhead(xts_RollMean, 10)\n\n             Bachum Villigst\n1996-01-04 11.95729 9.248571\n1996-01-05 11.82143 9.114286\n1996-01-06 11.82214 9.027143\n1996-01-07 12.02186 8.934286\n1996-01-08 12.23314 9.242857\n1996-01-09 12.37214 9.238571\n1996-01-10 12.61357 9.541429\n1996-01-11 12.62643 9.535714\n1996-01-12 12.56257 9.357143\n1996-01-13 12.50186 9.242857\n\n\n\n\n7 Summary in Calendar Period\nDealing with irregularly spaced time series data can be challenging. One fundamental operation in time series analysis is applying a function by calendar period. This process helps in summarizing and analyzing time series data more effectively, even when the data points are irregularly spaced in time.\n\napply.daily()\napply.weekly()\napply.monthly()\napply.quarterly()\napply.yearly()\n\n\nxts_Month &lt;- apply.monthly(xts_Inteval, mean)\nxts_Month\n\n              Bachum  Villigst\n1996-01-31 12.478387  9.348065\n1996-02-29 15.794241 17.403448\n1996-03-31 14.244613 13.252903\n1996-04-30 10.217533  7.310667\n1996-05-31  9.331129  7.094839\n1996-06-30 10.589067  6.700667\n1996-07-31 11.607968  8.248710\n1996-08-31 12.897806  9.410968\n1996-09-30 14.516733 12.750000\n1996-10-31 18.214161 17.702903\n1996-11-30 30.673967 35.472667\n1996-12-31 35.720290 39.940645",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Basic Processing"
    ]
  },
  {
    "objectID": "dataprocess/statistic_graphic.html",
    "href": "dataprocess/statistic_graphic.html",
    "title": "Graphical Statistic",
    "section": "",
    "text": "Graphical statistic is a branch of statistics that involves using visual representations to analyze and communicate data. It provides a powerful way to convey complex information in a more understandable and intuitive form.\n\n1 Example Data\nThe example files provided consist of three discharge time series for the Ruhr River in the Rhein basin, Germany. These data sets are sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github.\n\n# Library\nlibrary(xts)\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\n\n# File name\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Oeventrop &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Oeventrop_2761759000100.csv\"\nfn_Villigst &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Villigst_2765590000100.csv\"\n\n# Load Data\ndf_Bachum &lt;- read_csv2(fn_Bachum, skip = 10, col_names = FALSE)\ndf_Oeventrop &lt;- read_csv2(fn_Oeventrop, skip = 10, col_names = FALSE)\ndf_Villigst &lt;- read_csv2(fn_Villigst, skip = 10, col_names = FALSE)\n\n# Convert Date column to a Date type\ndf_Bachum$X1 &lt;- as_date(df_Bachum$X1, format = \"%d.%m.%Y\")\ndf_Oeventrop$X1 &lt;- as_date(df_Oeventrop$X1, format = \"%d.%m.%Y\")\ndf_Villigst$X1 &lt;- as_date(df_Villigst$X1, format = \"%d.%m.%Y\")\n\n# Create an xts object\nxts_Bachum &lt;- as.xts(df_Bachum)\nxts_Oeventrop &lt;- as.xts(df_Oeventrop)\nxts_Villigst &lt;- as.xts(df_Villigst)\n\n# Merge into one data frame\nxts_Rhur &lt;- merge(xts_Bachum, xts_Oeventrop, xts_Villigst)\nnames(xts_Rhur) &lt;- c(\"Bachum\", \"Oeventrop\", \"Villigst\")\nxts_Rhur &lt;- xts_Rhur[seq(as_date(\"1991-01-01\"), as_date(\"2020-12-31\"), \"days\"), ]\n\n# Deal with negative\ndf_Ruhr &lt;- coredata(xts_Rhur)\ndf_Ruhr[df_Ruhr &lt; 0] &lt;- NA\n\n# Summary in month\nxts_Ruhr_Clean &lt;- xts(df_Ruhr, index(xts_Rhur))\ndf_Ruhr_Month &lt;- apply.monthly(xts_Ruhr_Clean, mean)\n\nIn this article, we will leverage the power of the ggplot2 library to create plots and visualizations. To achieve this, the first step is to reformat the dataframe to a structure suitable for plotting.\n\ngdf_Ruhr &lt;- reshape2::melt(data.frame(date=index(df_Ruhr_Month), df_Ruhr_Month), \"date\")\n\n\n\n2 Timeserise line\nThe time series lines will provide us with discharge from 1991-01-01 to 2020-12-31 of the three gauges.\n\ngeom_line()\n\n\ngg_TS_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_line(aes(date, value, color = variable)) +\n  labs(x = \"Date\", y = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_TS_Ruhr)\n\n\n\n\n\n\n\n3 Frequency Plots/Histogram\nHistograms and frequency plots are graphical representations of data distribution.\nHistograms display the counts (or frequency) with bars; frequency plots display the counts (or frequency) with lines.\nThe frequency plot represents the relative density of the data points by the relative height of the bars, while in a histogram, the area within the bar represents the relative density of the data points.\n\ngeom_histogram()\n\n\ngg_Hist_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_histogram(aes(value, group = variable, fill = variable, color = variable), position = \"dodge\", alpha = .5) +\n  labs(y = \"Count\", x = \"Discharge [m^3/s]\", color = \"Gauge\", fill = \"Gauge\")\n\nggplotly(gg_Hist_Ruhr)\n\n\n\n\n\n\ngeom_freqpoly()\n\n\ngg_Freq_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_freqpoly(aes(value, y = after_stat(count / sum(count)), group = variable, fill = variable, color = variable)) +\n  labs(y = \"Frequency\", x = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_Freq_Ruhr)\n\n\n\n\n\n\n\n4 Box and Whisker Plot\nA Box and Whisker Plot, also known as a box plot, is a graphical representation of the distribution of a dataset. It provides a concise summary of the dataset’s key statistical measures and helps you visualize the spread and skewness of the data (Machiwal and Jha 2012). Here’s how a typical box and whisker plot is structured:\n\nBox: The box in the middle of the plot represents the interquartile range (IQR), which contains the middle 50% of the data. The bottom edge of the box represents the 25th percentile (Q1), and the top edge represents the 75th percentile (Q3).\nWhiskers: The whiskers extend from the box and represent the range of the data, excluding outliers. They typically extend to a certain multiple of the IQR beyond the quartiles. Outliers beyond the whiskers are often plotted as individual points.\nMedian (line inside the box): A horizontal line inside the box represents the median (Q2), which is the middle value of the dataset when it’s sorted.\n\n\n\n\nFigure from Internet\n\n\n\ngg_Box_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_boxplot(aes(variable, value, fill = variable, color = variable), alpha = .5) +\n  labs(x = \"Gauge\", y = \"Discharge [m^3/s]\", color = \"Gauge\") +\n  theme(legend.position = \"none\")\n\nggplotly(gg_Box_Ruhr)\n\n\n\n\n\n\n\n5 Quantile Plot\nA ‘quantile plot’ can be used to evaluate the quantile information such as the median, quartiles, and interquartile range of the data points (Machiwal and Jha 2012).\n\ngeom_qq()\n\n\ngg_QQ_Ruhr &lt;- ggplot(gdf_Ruhr, aes(sample = value, color = variable)) +\n  geom_qq(alpha = .5, distribution = stats::qunif) +\n  geom_qq_line(distribution = stats::qunif) +\n  labs(x = \"Fraction\", y = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_QQ_Ruhr)\n\n\n\n\n\n\n\n\n\n\nReferences\n\nMachiwal, Deepesh, and Madan Kumar Jha. 2012. Hydrologic Time Series Analysis: Theory and Practice. Neu Dehli: Captial Publishing Company.",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Graphical Statistic"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html",
    "href": "dataprocess/spatial_interpolate.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Spatial interpolation is the process of estimating values at unmeasured locations based on observations from nearby points. In this section, we introduce several commonly used approaches.\n\n\nMost spatial interpolation methods, from the simplest global mean to nearest-neighbor, IDW, and Kriging, can be understood within a weighted-mean framework. The estimated value at an unmeasured location \\(x_0\\) is a weighted sum of observed values:\n\\[\n\\hat{z}(x_0) = \\sum_{i=1}^{n} \\lambda_i z_i\n\\]\nwhere \\(\\lambda_i\\) is the weight assigned to observation \\(i\\).\n\nIn the global mean, all observations are equally weighted: \\(\\lambda_i = 1/n\\).\n\nIn nearest-neighbor methods, only the closest points have non-zero weight: \\(\\lambda_i = 1/k\\).\n\nIn inverse distance weighting (IDW), weights decrease with distance: \\(\\lambda_i \\propto 1/d_i^p\\).\n\nIn Kriging, weights are chosen to minimize estimation variance while accounting for spatial correlation and ensuring unbiasedness.\n\nThus, every interpolation method can be interpreted as defining a specific set of weights \\(\\lambda_i\\) to combine observed values into an estimate. In the following sections, we will focus primarily on calculating the weights \\(\\lambda_i\\), as the weighted mean step itself is straightforward.\nSymbols used:\n\n\\(x_0\\) : location of the unmeasured point\n\n\\(x_i\\) : location of the \\(i\\)-th observed point\n\n\\(\\lambda_i\\) : weight for observation \\(i\\)\n\n\\(z_i\\) : observed value at location \\(x_i\\)\n\n\\(\\hat{z}(x_0)\\) : estimated value at location \\(x_0\\)\n\n\\(n\\) : total number of observed points\n\n\\(k\\) : number of nearest neighbors considered\n\n\\(d_i\\) : distance between \\(x_0\\) and \\(x_i\\)\n\n\\(p\\) : distance weighting power (for IDW)\n\n\\(\\gamma(h)\\) : semivariance for lag distance \\(h\\) in the variogram\n\n\\(\\mu\\) : Lagrange multiplier in ordinary Kriging\n\nLibraries and Data used:\n\n# LIBRARY\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(gstat)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(patchwork)\n# DATEN\nvct_Boundry_NRW &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Boundry_NRW.geojson\")\nvct_Station_NRW_S10 &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Station_NRW_S10.geojson\")\nvct_Station_NRW_S50 &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Station_NRW_S50.geojson\")\nvct_Segment_S10    &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Segment_S10.geojson\")\nvct_Point_Unknow   &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Point_Unknow.geojson\")\n\n\n\nCode\ncolor_RUB_blue &lt;- \"#17365c\"\ncolor_RUB_green &lt;- \"#8dae10\"\ncolor_TUD_middleblue &lt;- \"#006ab2\"\ncolor_TUD_lightblue &lt;- \"#009de0\"\ncolor_TUD_green &lt;- \"#007d3f\"\ncolor_TUD_lightgreen &lt;- \"#69af22\"\ncolor_TUD_orange &lt;- \"#ee7f00\"\ncolor_TUD_pink &lt;- \"#EC008D\"\ncolor_TUD_purple &lt;- \"#54368a\"\ncolor_TUD_redpurple &lt;- \"#93107d\"\ncolor_SafetyOrange &lt;- \"#ff5e00\"\ncolor_DRESDEN &lt;- c(\"#03305D\", \"#28618C\", \"#539DC5\", \"#84D1EE\", \"#009BA4\", \"#13A983\", \"#93C356\", \"#BCCF02\")\n\n\nTo illustrate the concepts of different spatial interpolation approaches, we use the mean annual precipitation (1981–2010) recorded at 10 gauges in North Rhine-Westphalia (NRW).\n\nThe orange point represents an unmeasured location, whose precipitation value is to be estimated using various interpolation methods.\n\nThe orange lines connect this unmeasured point to the observation stations utilized by each method.\n\nThe labels on the lines indicate the corresponding weights \\(\\lambda_i\\) assigned to each station in the interpolation process.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe simplest model assumes that the value at any unmeasured location is equal to the mean of all observed values. Within the weighted mean framework, all weights are equal:\n\\[\n\\lambda_i = \\frac{1}{n} \\quad \\text{for all } i\n\\]\n\n# Calculate the mean of the annual precipitation ('NiederschlagJahr') \n# from the data frame or vector 'vct_Station_NRW_S10'\nnum_GlobalMean &lt;- vct_Station_NRW_S10$NiederschlagJahr |&gt; mean()\n\n# Display the calculated mean\nnum_GlobalMean\n\n[1] 928.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis method assigns the value of the closest observation to the unmeasured location. Within the weighted mean framework, only the nearest point has weight 1, while all others have weight 0:\n\\[\n\\lambda_i =\n\\begin{cases}\n1 & \\text{if } x_i \\text{ is the nearest point} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n# Find the index of the shortest segment in 'vct_Segment_S10' based on 'length_km'\n# 'order()' returns the indices of segments sorted in ascending order\nidx_Near1 &lt;- order(vct_Segment_S10$length_km)[1]\n\n# Use this index to get the corresponding annual precipitation value\n# from 'vct_Station_NRW_S10'\nnum_Near1 &lt;- vct_Station_NRW_S10$NiederschlagJahr[idx_Near1]\n\n# Display the precipitation value of the station nearest to the shortest segment\nnum_Near1\n\n[1] 778.9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis method estimates the value at an unmeasured location using the average of the \\(k\\) nearest observations. Within the weighted mean framework, each of the \\(k\\) nearest neighbors is assigned equal weight:\n\\[\n\\lambda_i =\n\\begin{cases}\n\\frac{1}{k} & \\text{if } x_i \\text{ is among the $k$ nearest} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n# Find the indices of the 4 shortest segments in 'vct_Segment_S10' based on 'length_km'\n# 'order()' sorts the lengths in ascending order and returns their indices\nidx_Near5 &lt;- order(vct_Segment_S10$length_km)[1:4]\n\n# Use these indices to select the corresponding annual precipitation values\n# from 'vct_Station_NRW_S10' and calculate their mean\nnum_Near5 &lt;- vct_Station_NRW_S10$NiederschlagJahr[idx_Near5] |&gt; mean()\n\n# Display the precipitation value of the station nearest 5 to the shortest segment\nnum_Near5\n\n[1] 1005.175\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this method, the weight of each observation is inversely proportional to its distance from the unmeasured location:\n\\[\n\\lambda_i = \\frac{d_i^{-p}}{\\sum_{j=1}^{n} d_j^{-p}}\n\\]\nwhere \\(d_i\\) is the distance between the unmeasured location and observation \\(i\\), and \\(p\\) is the distance weighting power, which controls how quickly weights decrease with distance.\nNote: A larger value of \\(p\\) assigns more weight to closer points. Typically, \\(p = 1\\) is used.\n\n# Calculate inverse distance weights for the 4 nearest segments\n# Closer segments (shorter 'length_km') get higher weights\nnum_IDW_ID &lt;- 1 / (vct_Segment_S10$length_km[idx_Near5])\n\n# Normalize the weights so they sum to 1 and round to 3 decimal places\nnum_IDW_Weight &lt;- round(num_IDW_ID / sum(num_IDW_ID), 3)\n\n# Compute the weighted mean precipitation using the inverse distance weights\nnum_IDW &lt;- sum(vct_Station_NRW_S10$NiederschlagJahr[idx_Near5] * num_IDW_Weight)\n\n# Display the inverse distance weighted precipitation value\nnum_IDW\n\n[1] 971.208\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinary Kriging is a geostatistical interpolation method that estimates values at unmeasured locations by accounting for the spatial correlation between observations. It computes location-specific weights \\(\\lambda_i\\) using the semivariogram \\(\\gamma(h)\\), ensuring that the estimator is unbiased and has minimum estimation variance.\nUnlike previous methods (mean, nearest neighbor, IDW), where the weights depend only on distances between the unmeasured point and observed stations, Kriging weights are derived from the spatial structure of the variable itself. This means that for different variables (e.g., precipitation, temperature, soil moisture), even with the same observation locations, the Kriging weights will differ because the spatial correlation patterns are different.\n\n\nThe variogram \\(\\gamma(h)\\) describes how data similarity decreases with increasing distance \\(h\\). It is estimated from the observed data using pairs of points separated by approximately the same distance:\n\\[\n\\gamma(h) = \\frac{1}{2|N(h)|} \\sum_{(i,j) \\in N(h)} (z_i - z_j)^2\n\\]\nwhere \\(N(h)\\) is the set of all pairs of points separated by lag distance \\(h\\).\n\n\n\n\n\n\n\n\n\nAfter computing the empirical variogram, a theoretical model is fitted to describe the spatial dependence. Common variogram models include:\n\nExponential\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left(1 - \\exp\\left(-\\frac{h}{a}\\right)\\right), & h &gt; 0 \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nSpherical\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left(\\frac{3h}{2a} - \\frac{1}{2} \\left(\\frac{h}{a}\\right)^3 \\right), & 0 &lt; h \\le a \\\\\nc_0 + c, & h &gt; a \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nCircular\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left( \\frac{2}{\\pi} \\left[ \\arccos\\left(\\frac{h}{a}\\right) - \\frac{h}{a} \\sqrt{1 - \\left(\\frac{h}{a}\\right)^2} \\right] \\right), & 0 &lt; h \\le a \\\\\nc_0 + c, & h &gt; a \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nGaussian\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left(1 - \\exp\\left(-\\frac{h^2}{a^2}\\right)\\right), & h &gt; 0 \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nLinear\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + \\text{slope} \\cdot h, & h &gt; 0 \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nMatern\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left( 1 - \\frac{1}{2^{\\nu-1} \\Gamma(\\nu)} \\left( \\frac{2 \\sqrt{\\nu} h}{a} \\right)^\\nu K_\\nu\\left( \\frac{2 \\sqrt{\\nu} h}{a} \\right) \\right), & h &gt; 0 \\\\\n0, & h = 0\n\\end{cases}\n\\]\nWhere:\n\n\\(c_0\\) = nugget\n\n\\(c\\) = partial sill\n\n\\(a\\) = range\n\n\\(\\nu\\) = Matern smoothness parameter\n\n\\(K_\\nu\\) = modified Bessel function of the second kind\n\n\n\n\nfrom wikipedia/Variogram\n\n\n\npsill (partial sill): Represents the portion of the total variance that is spatially structured. It corresponds to the difference between the sill and the nugget and determines the overall height of the variogram curve (excluding measurement error or microscale variability).\nrange: Defines the distance at which spatial correlation becomes negligible. Beyond this distance, observations no longer influence each other, and the variogram reaches its sill. The range therefore indicates the spatial extent of dependence in the data.\nnugget: Captures the variance at infinitesimally small distances. It reflects measurement error, microscale variability, or unresolved spatial heterogeneity. A non-zero nugget results in a jump of the variogram at the origin.\n\n\n\n\n\n\n\n\n\n\nIn R, this step is typically performed automatically to minimize the difference between the model function and the empirical scatter points.\n\n\n\nUsing the fitted variogram and assuming second-order stationarity (constant mean and spatial dependence only on distance), the Kriging weights are obtained by solving the following system.\nThe following presents the mathematical formulation for determining the weights assigned to all observations when estimating the value at a target location.\n\nEstimation Variance\n\nThe estimation error is defined as:\n\\[\n\\epsilon = \\hat{Z}(x_0) - Z(x_0) = \\sum_{i=1}^{n} \\lambda_i Z(x_i) - Z(x_0)\n\\]\nThe Kriging variance, i.e., the variance of the estimation error, is:\n\\[\n\\sigma_K^2 = Var[\\epsilon] = Var\\left[\\sum_{i=1}^{n} \\lambda_i Z(x_i) - Z(x_0)\\right]\n\\]\nUsing the semivariogram \\(\\gamma(h)\\) fitted from all observations:\n\\[\nVar(Z(x_i) - Z(x_j)) = 2\\gamma(x_i - x_j)\n\\]\nwe can write the Kriging variance in terms of the weights:\n\\[\n\\sigma_K^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\lambda_i \\lambda_j \\gamma(x_i - x_j) - 2 \\sum_{i=1}^{n} \\lambda_i \\gamma(x_i - x_0) + \\gamma(0)\n\\]\nThis is a quadratic function of the weights \\(\\lambda_i\\).\n\nUnbiasedness Constraint\n\nOrdinary Kriging assumes an unknown but constant mean \\(m\\), so the estimator must be unbiased:\n\\[\nE[\\hat{Z}(x_0) - Z(x_0)] = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{n} \\lambda_i = 1\n\\]\nThis is a linear equality constraint.\n\nLagrange Multiplier Method\n\nTo minimize \\(\\sigma_K^2\\) under the constraint \\(\\sum \\lambda_i = 1\\), define the Lagrangian function:\n\\[\nL(\\lambda_1, \\dots, \\lambda_n, \\mu) = \\sigma_K^2 + \\mu \\left( \\sum_{i=1}^{n} \\lambda_i - 1 \\right)\n\\]\n\n\\(\\mu\\) is the Lagrange multiplier, enforcing the sum-of-weights constraint.\n\n\nDerivatives of the Lagrangian\n\nTake partial derivatives of \\(L\\) with respect to each \\(\\lambda_i\\) and \\(\\mu\\):\n\nWith respect to \\(\\lambda_i\\):\n\n\\[\n\\frac{\\partial L}{\\partial \\lambda_i} = \\frac{\\partial \\sigma_K^2}{\\partial \\lambda_i} + \\mu = 0\n\\]\nCompute \\(\\partial \\sigma_K^2 / \\partial \\lambda_i\\):\n\\[\n\\frac{\\partial \\sigma_K^2}{\\partial \\lambda_i} = 2 \\sum_{j=1}^{n} \\lambda_j \\gamma(x_i - x_j) - 2 \\gamma(x_i - x_0)\n\\]\nDivide by 2 and redefine \\(\\mu' = \\mu/2\\):\n\\[\n\\sum_{j=1}^{n} \\lambda_j \\gamma(x_i - x_j) + \\mu' = \\gamma(x_i - x_0)\n\\]\n\nWith respect to \\(\\mu\\):\n\n\\[\n\\frac{\\partial L}{\\partial \\mu} = \\sum_{i=1}^{n} \\lambda_i - 1 = 0\n\\]\nThis ensures unbiasedness.\n\nMatrix Form\n\nCombine all \\(n\\) equations for \\(\\lambda_i\\) with the constraint for \\(\\mu\\):\n\\[\n\\begin{bmatrix}\n\\gamma(x_1-x_1) & \\cdots & \\gamma(x_1-x_n) & 1 \\\\\n\\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\gamma(x_n-x_1) & \\cdots & \\gamma(x_n-x_n) & 1 \\\\\n1 & \\cdots & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 \\\\ \\vdots \\\\ \\lambda_n \\\\ \\mu\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\gamma(x_1-x_0) \\\\ \\vdots \\\\ \\gamma(x_n-x_0) \\\\ 1\n\\end{bmatrix}\n\\]\n\nThe top-left \\(n \\times n\\) block contains semivariances between observed points.\n\nThe last row and column enforce \\(\\sum \\lambda_i = 1\\).\n\nThe right-hand side contains semivariances between each observation and the target point.\n\nOnce the weights for all observations are determined, the estimation process proceeds similarly to IDW interpolation. However, in R, a more efficient workflow is available using the gstat and terra packages, which will be demonstrated in the next chapter.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#weighted-mean-concept-in-spatial-interpolation",
    "href": "dataprocess/spatial_interpolate.html#weighted-mean-concept-in-spatial-interpolation",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Most spatial interpolation methods, from the simplest global mean to nearest-neighbor, IDW, and Kriging, can be understood within a weighted-mean framework. The estimated value at an unmeasured location \\(x_0\\) is a weighted sum of observed values:\n\\[\n\\hat{z}(x_0) = \\sum_{i=1}^{n} \\lambda_i z_i\n\\]\nwhere \\(\\lambda_i\\) is the weight assigned to observation \\(i\\).\n\nIn the global mean, all observations are equally weighted: \\(\\lambda_i = 1/n\\).\n\nIn nearest-neighbor methods, only the closest points have non-zero weight: \\(\\lambda_i = 1/k\\).\n\nIn inverse distance weighting (IDW), weights decrease with distance: \\(\\lambda_i \\propto 1/d_i^p\\).\n\nIn Kriging, weights are chosen to minimize estimation variance while accounting for spatial correlation and ensuring unbiasedness.\n\nThus, every interpolation method can be interpreted as defining a specific set of weights \\(\\lambda_i\\) to combine observed values into an estimate. In the following sections, we will focus primarily on calculating the weights \\(\\lambda_i\\), as the weighted mean step itself is straightforward.\nSymbols used:\n\n\\(x_0\\) : location of the unmeasured point\n\n\\(x_i\\) : location of the \\(i\\)-th observed point\n\n\\(\\lambda_i\\) : weight for observation \\(i\\)\n\n\\(z_i\\) : observed value at location \\(x_i\\)\n\n\\(\\hat{z}(x_0)\\) : estimated value at location \\(x_0\\)\n\n\\(n\\) : total number of observed points\n\n\\(k\\) : number of nearest neighbors considered\n\n\\(d_i\\) : distance between \\(x_0\\) and \\(x_i\\)\n\n\\(p\\) : distance weighting power (for IDW)\n\n\\(\\gamma(h)\\) : semivariance for lag distance \\(h\\) in the variogram\n\n\\(\\mu\\) : Lagrange multiplier in ordinary Kriging\n\nLibraries and Data used:\n\n# LIBRARY\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(gstat)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(patchwork)\n# DATEN\nvct_Boundry_NRW &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Boundry_NRW.geojson\")\nvct_Station_NRW_S10 &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Station_NRW_S10.geojson\")\nvct_Station_NRW_S50 &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Station_NRW_S50.geojson\")\nvct_Segment_S10    &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Segment_S10.geojson\")\nvct_Point_Unknow   &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Point_Unknow.geojson\")\n\n\n\nCode\ncolor_RUB_blue &lt;- \"#17365c\"\ncolor_RUB_green &lt;- \"#8dae10\"\ncolor_TUD_middleblue &lt;- \"#006ab2\"\ncolor_TUD_lightblue &lt;- \"#009de0\"\ncolor_TUD_green &lt;- \"#007d3f\"\ncolor_TUD_lightgreen &lt;- \"#69af22\"\ncolor_TUD_orange &lt;- \"#ee7f00\"\ncolor_TUD_pink &lt;- \"#EC008D\"\ncolor_TUD_purple &lt;- \"#54368a\"\ncolor_TUD_redpurple &lt;- \"#93107d\"\ncolor_SafetyOrange &lt;- \"#ff5e00\"\ncolor_DRESDEN &lt;- c(\"#03305D\", \"#28618C\", \"#539DC5\", \"#84D1EE\", \"#009BA4\", \"#13A983\", \"#93C356\", \"#BCCF02\")\n\n\nTo illustrate the concepts of different spatial interpolation approaches, we use the mean annual precipitation (1981–2010) recorded at 10 gauges in North Rhine-Westphalia (NRW).\n\nThe orange point represents an unmeasured location, whose precipitation value is to be estimated using various interpolation methods.\n\nThe orange lines connect this unmeasured point to the observation stations utilized by each method.\n\nThe labels on the lines indicate the corresponding weights \\(\\lambda_i\\) assigned to each station in the interpolation process.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#null-model-global-mean",
    "href": "dataprocess/spatial_interpolate.html#null-model-global-mean",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "The simplest model assumes that the value at any unmeasured location is equal to the mean of all observed values. Within the weighted mean framework, all weights are equal:\n\\[\n\\lambda_i = \\frac{1}{n} \\quad \\text{for all } i\n\\]\n\n# Calculate the mean of the annual precipitation ('NiederschlagJahr') \n# from the data frame or vector 'vct_Station_NRW_S10'\nnum_GlobalMean &lt;- vct_Station_NRW_S10$NiederschlagJahr |&gt; mean()\n\n# Display the calculated mean\nnum_GlobalMean\n\n[1] 928.1",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#nearest-neighbor-1-nearest",
    "href": "dataprocess/spatial_interpolate.html#nearest-neighbor-1-nearest",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "This method assigns the value of the closest observation to the unmeasured location. Within the weighted mean framework, only the nearest point has weight 1, while all others have weight 0:\n\\[\n\\lambda_i =\n\\begin{cases}\n1 & \\text{if } x_i \\text{ is the nearest point} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n# Find the index of the shortest segment in 'vct_Segment_S10' based on 'length_km'\n# 'order()' returns the indices of segments sorted in ascending order\nidx_Near1 &lt;- order(vct_Segment_S10$length_km)[1]\n\n# Use this index to get the corresponding annual precipitation value\n# from 'vct_Station_NRW_S10'\nnum_Near1 &lt;- vct_Station_NRW_S10$NiederschlagJahr[idx_Near1]\n\n# Display the precipitation value of the station nearest to the shortest segment\nnum_Near1\n\n[1] 778.9",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#k-nearest-neighbors",
    "href": "dataprocess/spatial_interpolate.html#k-nearest-neighbors",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "This method estimates the value at an unmeasured location using the average of the \\(k\\) nearest observations. Within the weighted mean framework, each of the \\(k\\) nearest neighbors is assigned equal weight:\n\\[\n\\lambda_i =\n\\begin{cases}\n\\frac{1}{k} & \\text{if } x_i \\text{ is among the $k$ nearest} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n# Find the indices of the 4 shortest segments in 'vct_Segment_S10' based on 'length_km'\n# 'order()' sorts the lengths in ascending order and returns their indices\nidx_Near5 &lt;- order(vct_Segment_S10$length_km)[1:4]\n\n# Use these indices to select the corresponding annual precipitation values\n# from 'vct_Station_NRW_S10' and calculate their mean\nnum_Near5 &lt;- vct_Station_NRW_S10$NiederschlagJahr[idx_Near5] |&gt; mean()\n\n# Display the precipitation value of the station nearest 5 to the shortest segment\nnum_Near5\n\n[1] 1005.175",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#inverse-distance-weighting-idw",
    "href": "dataprocess/spatial_interpolate.html#inverse-distance-weighting-idw",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "In this method, the weight of each observation is inversely proportional to its distance from the unmeasured location:\n\\[\n\\lambda_i = \\frac{d_i^{-p}}{\\sum_{j=1}^{n} d_j^{-p}}\n\\]\nwhere \\(d_i\\) is the distance between the unmeasured location and observation \\(i\\), and \\(p\\) is the distance weighting power, which controls how quickly weights decrease with distance.\nNote: A larger value of \\(p\\) assigns more weight to closer points. Typically, \\(p = 1\\) is used.\n\n# Calculate inverse distance weights for the 4 nearest segments\n# Closer segments (shorter 'length_km') get higher weights\nnum_IDW_ID &lt;- 1 / (vct_Segment_S10$length_km[idx_Near5])\n\n# Normalize the weights so they sum to 1 and round to 3 decimal places\nnum_IDW_Weight &lt;- round(num_IDW_ID / sum(num_IDW_ID), 3)\n\n# Compute the weighted mean precipitation using the inverse distance weights\nnum_IDW &lt;- sum(vct_Station_NRW_S10$NiederschlagJahr[idx_Near5] * num_IDW_Weight)\n\n# Display the inverse distance weighted precipitation value\nnum_IDW\n\n[1] 971.208",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#ordinary-kriging",
    "href": "dataprocess/spatial_interpolate.html#ordinary-kriging",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Ordinary Kriging is a geostatistical interpolation method that estimates values at unmeasured locations by accounting for the spatial correlation between observations. It computes location-specific weights \\(\\lambda_i\\) using the semivariogram \\(\\gamma(h)\\), ensuring that the estimator is unbiased and has minimum estimation variance.\nUnlike previous methods (mean, nearest neighbor, IDW), where the weights depend only on distances between the unmeasured point and observed stations, Kriging weights are derived from the spatial structure of the variable itself. This means that for different variables (e.g., precipitation, temperature, soil moisture), even with the same observation locations, the Kriging weights will differ because the spatial correlation patterns are different.\n\n\nThe variogram \\(\\gamma(h)\\) describes how data similarity decreases with increasing distance \\(h\\). It is estimated from the observed data using pairs of points separated by approximately the same distance:\n\\[\n\\gamma(h) = \\frac{1}{2|N(h)|} \\sum_{(i,j) \\in N(h)} (z_i - z_j)^2\n\\]\nwhere \\(N(h)\\) is the set of all pairs of points separated by lag distance \\(h\\).\n\n\n\n\n\n\n\n\n\nAfter computing the empirical variogram, a theoretical model is fitted to describe the spatial dependence. Common variogram models include:\n\nExponential\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left(1 - \\exp\\left(-\\frac{h}{a}\\right)\\right), & h &gt; 0 \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nSpherical\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left(\\frac{3h}{2a} - \\frac{1}{2} \\left(\\frac{h}{a}\\right)^3 \\right), & 0 &lt; h \\le a \\\\\nc_0 + c, & h &gt; a \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nCircular\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left( \\frac{2}{\\pi} \\left[ \\arccos\\left(\\frac{h}{a}\\right) - \\frac{h}{a} \\sqrt{1 - \\left(\\frac{h}{a}\\right)^2} \\right] \\right), & 0 &lt; h \\le a \\\\\nc_0 + c, & h &gt; a \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nGaussian\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left(1 - \\exp\\left(-\\frac{h^2}{a^2}\\right)\\right), & h &gt; 0 \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nLinear\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + \\text{slope} \\cdot h, & h &gt; 0 \\\\\n0, & h = 0\n\\end{cases}\n\\]\n\nMatern\n\n\\[\n\\gamma(h) =\n\\begin{cases}\nc_0 + c \\left( 1 - \\frac{1}{2^{\\nu-1} \\Gamma(\\nu)} \\left( \\frac{2 \\sqrt{\\nu} h}{a} \\right)^\\nu K_\\nu\\left( \\frac{2 \\sqrt{\\nu} h}{a} \\right) \\right), & h &gt; 0 \\\\\n0, & h = 0\n\\end{cases}\n\\]\nWhere:\n\n\\(c_0\\) = nugget\n\n\\(c\\) = partial sill\n\n\\(a\\) = range\n\n\\(\\nu\\) = Matern smoothness parameter\n\n\\(K_\\nu\\) = modified Bessel function of the second kind\n\n\n\n\nfrom wikipedia/Variogram\n\n\n\npsill (partial sill): Represents the portion of the total variance that is spatially structured. It corresponds to the difference between the sill and the nugget and determines the overall height of the variogram curve (excluding measurement error or microscale variability).\nrange: Defines the distance at which spatial correlation becomes negligible. Beyond this distance, observations no longer influence each other, and the variogram reaches its sill. The range therefore indicates the spatial extent of dependence in the data.\nnugget: Captures the variance at infinitesimally small distances. It reflects measurement error, microscale variability, or unresolved spatial heterogeneity. A non-zero nugget results in a jump of the variogram at the origin.\n\n\n\n\n\n\n\n\n\n\nIn R, this step is typically performed automatically to minimize the difference between the model function and the empirical scatter points.\n\n\n\nUsing the fitted variogram and assuming second-order stationarity (constant mean and spatial dependence only on distance), the Kriging weights are obtained by solving the following system.\nThe following presents the mathematical formulation for determining the weights assigned to all observations when estimating the value at a target location.\n\nEstimation Variance\n\nThe estimation error is defined as:\n\\[\n\\epsilon = \\hat{Z}(x_0) - Z(x_0) = \\sum_{i=1}^{n} \\lambda_i Z(x_i) - Z(x_0)\n\\]\nThe Kriging variance, i.e., the variance of the estimation error, is:\n\\[\n\\sigma_K^2 = Var[\\epsilon] = Var\\left[\\sum_{i=1}^{n} \\lambda_i Z(x_i) - Z(x_0)\\right]\n\\]\nUsing the semivariogram \\(\\gamma(h)\\) fitted from all observations:\n\\[\nVar(Z(x_i) - Z(x_j)) = 2\\gamma(x_i - x_j)\n\\]\nwe can write the Kriging variance in terms of the weights:\n\\[\n\\sigma_K^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\lambda_i \\lambda_j \\gamma(x_i - x_j) - 2 \\sum_{i=1}^{n} \\lambda_i \\gamma(x_i - x_0) + \\gamma(0)\n\\]\nThis is a quadratic function of the weights \\(\\lambda_i\\).\n\nUnbiasedness Constraint\n\nOrdinary Kriging assumes an unknown but constant mean \\(m\\), so the estimator must be unbiased:\n\\[\nE[\\hat{Z}(x_0) - Z(x_0)] = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{n} \\lambda_i = 1\n\\]\nThis is a linear equality constraint.\n\nLagrange Multiplier Method\n\nTo minimize \\(\\sigma_K^2\\) under the constraint \\(\\sum \\lambda_i = 1\\), define the Lagrangian function:\n\\[\nL(\\lambda_1, \\dots, \\lambda_n, \\mu) = \\sigma_K^2 + \\mu \\left( \\sum_{i=1}^{n} \\lambda_i - 1 \\right)\n\\]\n\n\\(\\mu\\) is the Lagrange multiplier, enforcing the sum-of-weights constraint.\n\n\nDerivatives of the Lagrangian\n\nTake partial derivatives of \\(L\\) with respect to each \\(\\lambda_i\\) and \\(\\mu\\):\n\nWith respect to \\(\\lambda_i\\):\n\n\\[\n\\frac{\\partial L}{\\partial \\lambda_i} = \\frac{\\partial \\sigma_K^2}{\\partial \\lambda_i} + \\mu = 0\n\\]\nCompute \\(\\partial \\sigma_K^2 / \\partial \\lambda_i\\):\n\\[\n\\frac{\\partial \\sigma_K^2}{\\partial \\lambda_i} = 2 \\sum_{j=1}^{n} \\lambda_j \\gamma(x_i - x_j) - 2 \\gamma(x_i - x_0)\n\\]\nDivide by 2 and redefine \\(\\mu' = \\mu/2\\):\n\\[\n\\sum_{j=1}^{n} \\lambda_j \\gamma(x_i - x_j) + \\mu' = \\gamma(x_i - x_0)\n\\]\n\nWith respect to \\(\\mu\\):\n\n\\[\n\\frac{\\partial L}{\\partial \\mu} = \\sum_{i=1}^{n} \\lambda_i - 1 = 0\n\\]\nThis ensures unbiasedness.\n\nMatrix Form\n\nCombine all \\(n\\) equations for \\(\\lambda_i\\) with the constraint for \\(\\mu\\):\n\\[\n\\begin{bmatrix}\n\\gamma(x_1-x_1) & \\cdots & \\gamma(x_1-x_n) & 1 \\\\\n\\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\gamma(x_n-x_1) & \\cdots & \\gamma(x_n-x_n) & 1 \\\\\n1 & \\cdots & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 \\\\ \\vdots \\\\ \\lambda_n \\\\ \\mu\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\gamma(x_1-x_0) \\\\ \\vdots \\\\ \\gamma(x_n-x_0) \\\\ 1\n\\end{bmatrix}\n\\]\n\nThe top-left \\(n \\times n\\) block contains semivariances between observed points.\n\nThe last row and column enforce \\(\\sum \\lambda_i = 1\\).\n\nThe right-hand side contains semivariances between each observation and the target point.\n\nOnce the weights for all observations are determined, the estimation process proceeds similarly to IDW interpolation. However, in R, a more efficient workflow is available using the gstat and terra packages, which will be demonstrated in the next chapter.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#basic-workflow",
    "href": "dataprocess/spatial_interpolate.html#basic-workflow",
    "title": "Spatial Interpolation",
    "section": "2.1 Basic Workflow",
    "text": "2.1 Basic Workflow\n\n2.1.1 Defining the Interpolation Model with gstat()\nThe gstat() function provides a unified framework for specifying interpolation models. It allows the user to define the variable to be interpolated, the spatial coordinates, and method-specific parameters controlling interpolation behavior.\nGeneral Syntax\n#| eval: false\nmodel_Interpolate &lt;- gstat(\n  formula   = variable ~ 1,\n  locations = ~x + y,\n  data      = df_Data,\n  nmax      = 10,\n  set       = list(idp = 2)\n)\nKey Arguments\n\nformula = variable ~ 1 Specifies the target variable for interpolation. The right-hand side 1 indicates that only a constant mean is assumed, i.e., no trend or covariates are considered.\n\nExample: precip ~ 1 for ordinary kriging or IDW.\nIf predictors are included, e.g., precip ~ elevation, the model becomes universal kriging.\n\nlocations = ~x + y Defines the coordinate columns in the dataset.\ndata = df_Data The input dataset, which may be a dataframe or a spatial object such as SpatVector, sf, or SpatialPointsDataFrame.\nNeighbor selection parameters (nmax, nmin, maxdist) Control the spatial neighborhood used for interpolation.\n\nnmax: maximum number of neighbors considered.\nnmin: minimum number of neighbors required for prediction.\nmaxdist: maximum search distance; points beyond this threshold are ignored. These parameters influence both the smoothness of the interpolated surface and computational efficiency.\n\n\n-set = list() Method-specific parameters.\n\nFor IDW, set = list(idp = p) defines the power parameter \\(p\\).\n\nidp = 0: equal weighting for all neighbors.\nidp &gt; 1: higher weight for closer points; larger \\(p\\) → more localized influence.\n\nFor nearest neighbor, set nmax = 1.\nFor kriging, variogram parameters are specified using the model argument.\nmodel Required for kriging methods; specifies the fitted variogram model, typically generated with variogram() and fit.variogram().\n\n\n\n\n\n\n\n\n\nMethod\nKey Parameters\nDescription\n\n\n\n\nNearest Neighbor\nnmax = 1\nAssigns the value of the closest observed point\n\n\nk-Nearest Neighbor\nnmax = k\nUses the mean of the k nearest observations\n\n\nIDW\nset = list(idp = p)\nPower parameter \\(p\\) controls weight decay with distance\n\n\nOrdinary Kriging\nmodel = v_model (variogram)\nAccounts for spatial correlation and provides estimation variance\n\n\n\nIn summary, gstat() acts as the central function for constructing spatial interpolation models. The choice of formula, neighbor parameters (nmax, nmin), and method-specific arguments (set or model) determines the behavior of the interpolation method.\n\n\n2.1.2 Applying the Interpolation Model with interpolate()\nAfter defining the model, the interpolate() function from the terra package is used to estimate values at unsampled locations, typically on a raster grid.\n\nrst_Interpolate &lt;- interpolate(\n  rst_Template,\n  model_Vario,\n  debug.level = 0\n)\n\nMain Parameters\n-raster_template: A SpatRaster object defining the spatial extent, resolution, and coordinate reference system for the interpolated output. -model: The interpolation model object created via gstat(). -debug.level: Controls the verbosity of output messages; commonly set to 0 (silent) or 1 (minimal information).\nThe function returns a raster object containing predicted values at each grid cell, allowing for subsequent visualization and analysis.\nBy combining gstat() and interpolate(), R provides a flexible and robust framework for spatial prediction, enabling the creation of continuous surfaces from point measurements. This approach is widely applicable in hydrology, meteorology, ecology, and other spatially explicit fields.\n\n\n2.1.3 Example Dataset\nFor demonstration purposes, we consider the average precipitation (1981–2010) recorded at 50 meteorological stations in North Rhine-Westphalia (NRW). The dataset comprises the coordinates of each station and the corresponding mean precipitation values. This dataset will be used to illustrate spatial interpolation methods.\n\n\nCode\nggplot() +\n  geom_spatvector(data = vct_Station_NRW_S50, aes(fill = NiederschlagJahr),\n                  shape = 24, size = 4) +\n  geom_spatvector(data = vct_Boundry_NRW, color = color_TUD_pink, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(550, 1500)) +\n  ggtitle('Mean Annual Precipitation (1981–2010)') +\n  coord_sf(expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\n\n\n\n\n\n\n\n\nIn order to perform the interpolation, it is necessary to define a template raster, which specifies the locations where predictions should be made. The template raster allows the user to control the spatial resolution and the extent of the interpolation, ensuring that the resulting surface matches the desired research area and level of detail.\n\n# Create a template raster over the NRW boundary\n# 'rast(vct_Boundry_NRW, res = 0.01)' defines a raster with 0.01 degree resolution\nrst_Template_NRW &lt;- rast(vct_Boundry_NRW, res = 0.01) \n\n# Assign a default value of 1 to all raster cells\nvalues(rst_Template_NRW) &lt;- 1\n\n# Mask the raster using the NRW boundary\n# This sets all cells outside the boundary to NA\nrst_Template_NRW &lt;- mask(rst_Template_NRW, vct_Boundry_NRW)\n\n# Convert station SpatVector to a data frame including coordinates\n# 'geom(...)[, c(\"x\", \"y\")]' extracts x and y coordinates\n# 'as.data.frame(...)' extracts attribute data\ndf_Station_NRW_S50 &lt;- data.frame(\n  geom(vct_Station_NRW_S50)[, c(\"x\", \"y\")],\n  as.data.frame(vct_Station_NRW_S50)\n)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#nearest-neighbor-1-nearest-and-thiessen-polygons",
    "href": "dataprocess/spatial_interpolate.html#nearest-neighbor-1-nearest-and-thiessen-polygons",
    "title": "Spatial Interpolation",
    "section": "2.2 Nearest Neighbor (1-Nearest) and Thiessen Polygons",
    "text": "2.2 Nearest Neighbor (1-Nearest) and Thiessen Polygons\nThe nearest neighbor method is closely related to Thiessen polygons (also called Voronoi diagrams). In this approach, the study area is partitioned into polygons, each surrounding a single observation point. All locations within a polygon are assumed to have the same value as the observation at its center.\n\n2.2.1 Nearest Neighbor (1-Nearest)\n\n# Create a gstat model for nearest neighbor interpolation\n# 'NiederschlagJahr ~ 1' models only the mean (no trend)\n# 'locations = ~x + y' specifies station coordinates\n# 'nmax = 1' uses only the nearest station for each prediction\n# 'set = list(idp = 0)' sets inverse distance power to 0, making it pure nearest neighbor\nmodel_Near1 &lt;- gstat(\n  formula = NiederschlagJahr ~ 1,\n  locations = ~x + y,\n  data = df_Station_NRW_S50,\n  nmax = 1,\n  set = list(idp = 0)\n)\n\n# Interpolate the raster using the nearest neighbor model\n# 'rst_Template_NRW' provides the grid where predictions are made\n# 'mask(vct_Boundry_NRW)' ensures values outside NRW are set to NA\nrst_Near1 &lt;- interpolate(rst_Template_NRW, model_Near1, debug.level = 0) |&gt; \n  mask(vct_Boundry_NRW)\n\n\n\n2.2.2 Thiessen Polygons\nThiessen polygons provide a geometric framework for nearest neighbor interpolation, particularly useful when measurement points are sparse, and smooth spatial gradients cannot be assumed.\nThe construction procedure is as follows:\n\nTriangulation: Connect all observation points to form a Triangulated Irregular Network (TIN) that satisfies the Delaunay criterion.\nPerpendicular Bisectors: Draw perpendicular bisectors for each triangle edge. The intersections of these bisectors define the vertices of the Thiessen polygons.\nAnchor Points: Each polygon contains exactly one observation point (the anchor), and all locations within the polygon are closer to this anchor than to any other observation.\n\nThe resulting Thiessen polygons effectively assign each unsampled location to the nearest observation, creating a spatial partitioning consistent with the 1-nearest neighbor principle.\n\n\n\nIllustration of Thiessen polygon\n\n\nIn R, Thiessen polygons are implemented as Voronoi polygons using the voronoi() function:\n\n# Create Voronoi polygons from the station locations\n# Each Voronoi cell contains all points closer to that station than to any other\nvct_Voronoi_NRW_S50 &lt;- voronoi(vct_Station_NRW_S50) |&gt; \n\n  # Crop the Voronoi polygons to the NRW boundary\n  # Ensures polygons do not extend beyond the study area\n  crop(vct_Boundry_NRW)\n\nThis function generates the polygonal tessellation corresponding to the nearest neighbor regions of the input points.\n\n\n2.2.3 Comparison of Results\nThe following plots illustrate the outcomes of 1-nearest neighbor interpolation and Thiessen polygons applied to the same dataset.\nAs expected, both methods produce almost same results because Thiessen polygons are a geometric representation of the 1-nearest neighbor principle. Each unsampled location is assigned the value of the closest observation point in both approaches.\nVisually, there is no obvious difference between the two methods; both generate a piecewise-constant surface with abrupt changes at the boundaries between regions influenced by different observation points.\nKey observations:\n\nBoth surfaces preserve the exact values of the original observations.\n\nDiscontinuities occur along polygon edges or nearest-neighbor boundaries.\n\n\n\nCode\ngp_Near1 &lt;- ggplot() +\n  geom_spatraster(data = rst_Near1, aes(fill = var1.pred)) +\n  geom_spatvector(data = vct_Boundry_NRW, color = color_TUD_pink, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(550, 1500)) +\n  ggtitle('Nearest Neighbor (1-nearest)') +\n  coord_sf(expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.99, 0.01),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\"\n  ) +\n  guides(\n    fill = guide_colorbar(\n      barwidth = 8,\n      barheight = 0.6\n    )\n  )\ngp_Voronoi_NRW_S50 &lt;- ggplot() +\n  geom_spatvector(data = vct_Voronoi_NRW_S50, aes(fill = NiederschlagJahr)) +\n  geom_spatvector(data = vct_Boundry_NRW, color = color_TUD_pink, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(550, 1500)) +\n  ggtitle('Thiessen Polygons') +\n  coord_sf(expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.99, 0.01),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\"\n  ) +\n  guides(\n    fill = guide_colorbar(\n      barwidth = 8,\n      barheight = 0.6\n    )\n  )\n\n(gp_Near1 | gp_Voronoi_NRW_S50)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#k-nearest-neighbors-k-nn",
    "href": "dataprocess/spatial_interpolate.html#k-nearest-neighbors-k-nn",
    "title": "Spatial Interpolation",
    "section": "2.3 k-Nearest Neighbors (k-NN)",
    "text": "2.3 k-Nearest Neighbors (k-NN)\n\n# Create a gstat model for nearest neighbor interpolation using the 5 closest stations\n# 'NiederschlagJahr ~ 1' models only the mean (no trend)\n# 'locations = ~x + y' specifies station coordinates\n# 'nmax = 5' uses the 5 nearest stations for each prediction\n# 'set = list(idp = 0)' sets inverse distance power to 0, so all selected stations are equally weighted\nmodel_Near5 &lt;- gstat(\n  formula = NiederschlagJahr ~ 1,\n  locations = ~x + y,\n  data = df_Station_NRW_S50,\n  nmax = 5,\n  set = list(idp = 0)\n)\n\n# Interpolate the raster using the nearest 5 neighbor model\n# 'rst_Template_NRW' provides the grid for predictions\n# 'mask(vct_Boundry_NRW)' ensures values outside NRW are set to NA\nrst_Near5 &lt;- interpolate(rst_Template_NRW, model_Near5, debug.level = 0) |&gt; \n  mask(vct_Boundry_NRW)\n\nCompared to the 1-nearest method, the k-nearest approach still exhibits noticeable block-like boundaries. However, the resulting surface is already smoother, as each block incorporates information from multiple observation points rather than relying on a single nearest station. A remaining drawback is that, at some observation locations, the interpolated value may differ from the original measured value, leading to local inconsistencies.\n\n\nCode\ngp_Near5 &lt;- ggplot() +\n  geom_spatraster(data = rst_Near5, aes(fill = var1.pred)) +\n  geom_spatvector(data = vct_Boundry_NRW, color = color_TUD_pink, fill = NA) +\n  geom_spatvector(data = vct_Station_NRW_S50, aes(fill = NiederschlagJahr),\n                  shape = 24, size = 3, color = \"grey96\", linewidth = 1) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(550, 1500)) +\n  ggtitle('k-Nearest Neighbors') +\n  coord_sf(expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.99, 0.01),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\"\n  ) +\n  guides(\n    fill = guide_colorbar(\n      barwidth = 8,\n      barheight = 0.6\n    )\n  )\n\n(gp_Near1 | gp_Near5)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#inverse-distance-weighting-idw-1",
    "href": "dataprocess/spatial_interpolate.html#inverse-distance-weighting-idw-1",
    "title": "Spatial Interpolation",
    "section": "2.4 Inverse Distance Weighting (IDW)",
    "text": "2.4 Inverse Distance Weighting (IDW)\n\n# Create a gstat model for inverse distance weighted (IDW) interpolation\n# 'NiederschlagJahr ~ 1' models only the mean (no trend)\n# 'locations = ~x + y' specifies station coordinates\n# 'nmax = 5' uses the 5 nearest stations for each prediction\n# Since 'idp' is not set, the default inverse distance weighting is applied\nmodel_IDW &lt;- gstat(\n  formula = NiederschlagJahr ~ 1,\n  locations = ~x + y,\n  data = df_Station_NRW_S50,\n  nmax = 5\n)\n\n# Interpolate the raster using the IDW model\n# 'rst_Template_NRW' provides the grid for predictions\n# 'mask(vct_Boundry_NRW)' ensures values outside NRW are set to NA\nrst_IDW &lt;- interpolate(rst_Template_NRW, model_IDW, debug.level = 0) |&gt; \n  mask(vct_Boundry_NRW)\n\nCompared to the k-nearest-neighbor method, the IDW approach exhibits only very subtle block-like boundaries. Within each block, the values already vary smoothly due to the distance-based weighting. Overall, the interpolated surface produced by IDW appears much smoother, as the influence of each observation point gradually decreases with distance, resulting in a more continuous spatial pattern. The phenomenon in which values appear mismatched or inconsistent within local regions has largely disappeared.\n\n\nCode\ngp_IDW &lt;- ggplot() +\n  geom_spatraster(data = rst_IDW, aes(fill = var1.pred)) +\n  geom_spatvector(data = vct_Boundry_NRW, color = color_TUD_pink, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(550, 1500)) +\n  ggtitle('Inverse Distance Weighting (IDW)') +\n  coord_sf(expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.99, 0.01),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\"\n  ) +\n  guides(\n    fill = guide_colorbar(\n      barwidth = 8,\n      barheight = 0.6\n    )\n  )\n\n(gp_Near5 | gp_IDW)\n\n\n\n\n\n\n\n\n\n\n2.4.1 Comparison of \\(k\\) and \\(p\\) in Interpolation\nThe following figure illustrates how the number of neighbors \\(k\\) and the IDW power parameter \\(p\\) influence the interpolation results.\n\nEffect of \\(k\\) (number of nearest neighbors)\n\nIncreasing \\(k\\) includes more observation points in the interpolation.\nThe resulting surface becomes smoother and less influenced by single extreme values.\nPolygon-like discontinuities seen in 1-nearest neighbor or small \\(k\\) cases are reduced.\n\nEffect of \\(p\\) (IDW power parameter)\n\nControls the weighting of neighbors based on distance.\nLarger \\(p\\) values assign more weight to nearby or extreme values, producing areas dominated by high or low observations.\nSmall \\(p\\) values distribute influence more evenly, creating a smoother surface similar to simple averaging.\n\n\nIn summary:\n\n\n\n\n\n\n\nParameter\nEffect on Interpolation\n\n\n\n\n\\(k\\)\nLarger \\(k\\) → more neighbors considered → smoother surface, less polygonal artifacts\n\n\n\\(p\\)\nLarger \\(p\\) → stronger influence of nearby/extreme points → more pronounced peaks and valleys\n\n\n\nThis demonstrates the trade-off between smoothing and sensitivity to local extremes when choosing \\(k\\) and \\(p\\) for k-NN or IDW interpolation.\n\n# Define combinations of neighbors (nmax) and inverse distance powers (idp) for IDW\n# Each element is a list with a specific nmax and idp value\nlst_IDW_Params &lt;- list(\n  k5p1  = list(nmax = 5, idp = 1),   # 5 nearest neighbors, idp = 1\n  k10p1 = list(nmax = 10, idp = 1),  # 10 nearest neighbors, idp = 1\n  k5p3  = list(nmax = 5, idp = 3),   # 5 nearest neighbors, idp = 3 (stronger weighting for closer stations)\n  k10p3 = list(nmax = 10, idp = 3)   # 10 nearest neighbors, idp = 3\n)\n\n# Function to create an IDW raster given nmax and idp\ncreate_idw &lt;- function(nmax, idp) {\n  # Set up gstat model for IDW\n  model &lt;- gstat(\n    formula = NiederschlagJahr ~ 1,       # model only the mean\n    locations = ~x + y,                    # use station coordinates\n    data = df_Station_NRW_S50,            # station data\n    nmax = nmax,                           # number of nearest neighbors\n    set = list(idp = idp)                  # inverse distance power\n  )\n  \n  # Interpolate on the template raster and mask to NRW boundary\n  interpolate(rst_Template_NRW, model, debug.level = 0) |&gt; \n    mask(vct_Boundry_NRW)\n}\n\n# Generate IDW rasters for all parameter combinations\n# Result is a list of raster layers, one for each combination\nlst_rst_IDW &lt;- lapply(lst_IDW_Params, \\(p) create_idw(p$nmax, p$idp))\n\n\n\nCode\nrst_IDW_combined &lt;- rast(lst_rst_IDW)\nrst_IDW_combined &lt;- rst_IDW_combined[[c(1, 3, 5, 7)]]\n# Set layer names to match parameter combinations\nnames(rst_IDW_combined) &lt;- c(\"k=5, p=1\", \"k=10, p=1\", \"k=5, p=3\", \"k=10, p=3\")\n\n# Create facet plot\nggplot() +\n  geom_spatraster(data = rst_IDW_combined) +\n  geom_spatvector(data = vct_Boundry_NRW, color = color_TUD_pink, fill = NA) +\n  facet_wrap(~lyr, ncol = 2) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(550, 1500)) +\n  coord_sf(expand = FALSE) +\n  labs(title = \"Inverse Distance Weighting (IDW) - Parameter Comparison\") +\n  theme(\n    axis.title = element_blank(),\n    axis.text.y = element_text(angle = 90, hjust = 0.5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.99, 0.01),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\",\n    strip.text = element_text(size = 10, face = \"bold\")\n  ) +\n  guides(\n    fill = guide_colorbar(\n      barwidth = 8,\n      barheight = 0.6\n    )\n  )",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_interpolate.html#ordinary-kriging-1",
    "href": "dataprocess/spatial_interpolate.html#ordinary-kriging-1",
    "title": "Spatial Interpolation",
    "section": "2.5 Ordinary Kriging",
    "text": "2.5 Ordinary Kriging\n\n2.5.1 Fitting a Variogram and Selecting a Model\nThe variogram is a key tool in kriging, representing the relationship between semivariance and distance. In R, this process involves three main steps:\n\nDefine the kriging model with gstat()\n\nSet the variable to interpolate (formula = variable ~ 1), indicating an assumed constant mean.\nSpecify the spatial coordinates (locations = ~x + y).\nProvide the observed data (data = df_Data).\n\nCompute the experimental variogram with variogram()\n\nGroup semivariance values by distance to create a scatter plot of spatial dependence.\nUse the argument width to set the distance bin size (e.g., width = 0.05), which controls how semivariance is averaged across distance intervals.\n\nFit a theoretical variogram with fit.variogram()\n\nFit standard variogram functions (spherical, exponential, Gaussian, etc.) to the experimental variogram.\nThe chosen model provides the parameters required for ordinary kriging, such as the sill, range, and nugget.\n\n\nThe variogram fitting process requires plausible initial estimates for the following parameters, as they guide the optimization toward a meaningful solution. These initial values can typically be inferred from the empirical variogram scatter plot.\nProviding reasonable initial values for these parameters is crucial, as they ensure stable and accurate variogram fitting during model optimization.\n\n# Set up a gstat model for variogram calculation\n# 'NiederschlagJahr ~ 1' models only the mean (no trend)\n# 'locations = ~x + y' specifies the station coordinates\nmodel_Vario &lt;- gstat(\n  formula = NiederschlagJahr ~ 1,\n  locations = ~x + y,\n  data = df_Station_NRW_S50\n)\n\n# Compute the experimental variogram with a lag width of 0.05\nvario_S50 &lt;- variogram(model_Vario, width = 0.05)\n\n# Define names of theoretical variogram models to try\nname_VarioModel &lt;- c(\"Exp\", \"Sph\", \"Cir\", \"Gau\", \"Lin\", \"Mat\")\nnames(name_VarioModel) &lt;- name_VarioModel  # set names for easier reference\n\n# Set initial parameters for variogram fitting\nnum_psill  &lt;- 6e4    # partial sill (variance explained by spatial structure)\nnum_range  &lt;- 0.5    # range (distance beyond which points are uncorrelated)\nnum_nugget &lt;- 3e3    # nugget effect (small-scale variability or measurement error)\n\n# Fit variogram models (excluding 'Mat') to the experimental variogram\nlst_FitVario &lt;- map(name_VarioModel[-6], \\(m_Vario) \n  fit.variogram(vario_S50, vgm(num_psill, m_Vario, num_range, num_nugget))\n)\n\n# Fit the 'Matérn' variogram separately with kappa = 0.9\nlst_FitVario$Mat &lt;- fit.variogram(\n  vario_S50, \n  vgm(num_psill, \"Mat\", num_range, num_nugget, kappa = 0.9)\n)\n\n\n\n2.5.2 Comparison of Variogram Models\nIn the figure below, six theoretical variogram models are fitted to the same dataset:\n\nAt short distances, all models show a similar increasing trend in semivariance, reflecting strong local correlation.\nAt larger distances, the models diverge:\n\nSome flatten earlier, indicating limited long-range correlation.\nOthers increase more gradually, suggesting extended spatial dependence.\n\n\nThis illustrates that the selection of variogram model and the associated parameters (sill, range, nugget) is critical in ordinary kriging, as it directly affects how observations are weighted and the resulting spatial interpolation pattern.\n\n\nCode\nlst_df_VarioLine &lt;- map(lst_FitVario, variogramLine, maxdist = max(vario_S50$dist))\ndf_VarioLine_All &lt;- do.call(rbind, lst_df_VarioLine)\ndf_VarioLine_All$model &lt;- rep(c(\"Exponential\", \"Spherical\", \"Circular\", \"Gaussian\", \"Linear\", \"Matern\"), each = 200)\n\n# Create color palette for models\nmodel_colors &lt;- c(\n  \"Exponential\" = color_TUD_orange,\n  \"Spherical\" = color_RUB_blue,\n  \"Circular\" = color_TUD_lightblue,\n  \"Gaussian\" = color_RUB_green, \n  \"Linear\" = color_TUD_pink,\n  \"Matern\" = color_TUD_redpurple\n)\n\n# Determine the y-axis range from your data\ny_range &lt;- range(vario_S50$gamma, df_VarioLine_All$gamma, na.rm = TRUE)\n\ngp_Fit &lt;- ggplot() +\n  geom_point(data = vario_S50, aes(x = dist, y = gamma), size = 3, shape = 1, color = color_RUB_blue) +\n  geom_line(data = df_VarioLine_All, aes(x = dist, y = gamma, color = model), linewidth = 1) +\n  scale_color_manual(\"Model\", values = model_colors) +\n  labs(x = \"Distance (°)\", y = NULL) +  # Remove y-axis title\n  lims(y = y_range) +  # Same y-axis range\n  theme(\n    axis.text.y = element_blank(),  # Remove y-axis text\n    axis.ticks.y = element_blank(), # Remove y-axis ticks\n    legend.position = \"inside\",\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.position.inside = c(0.99, 0.01)\n  )\n\n\n\ngp_Scatter &lt;- ggplot() +\n  geom_point(data = vario_S50, aes(x = dist, y = gamma, color = dist), size = 3) +\n  scale_color_gradientn(\"Distance h (°)\", colors = color_DRESDEN) +\n  labs(x = \"Distance (°)\", y = \"Semivariance\") +\n  lims(y = y_range) +\n  theme(\n    axis.text.y = element_text(angle = 90, hjust = .5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.99, 0.01),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\"\n  ) +\n  guides(\n    color = guide_colorbar(barwidth = 8, barheight = 0.6)\n  )\n\n\n# Combine the plots\n(gp_Scatter | gp_Fit) +\n  plot_annotation(title = \"Variogram Models Comparison\")\n\n\n\n\n\n\n\n\n\n\n\n2.5.3 Cross-Validation and Method Selection\nTo select the most appropriate interpolation method, cross-validation can be performed using the observed data. This approach evaluates how well each method predicts known values by temporarily withholding each observation and comparing the predicted value to the actual measurement.\nThe following table summarizes the root mean square error (RMSE) for six variogram-based kriging models:\n\n# Compute RMSE for each fitted variogram model using cross-validation\nlst_RMSE_VarioFit &lt;- map(lst_FitVario, \\(m_Fit) {\n  \n  # Perform leave-one-out cross-validation using kriging\n  # 'NiederschlagJahr ~ 1' models only the mean\n  # 'locations = ~x + y' specifies station coordinates\n  # 'model = m_Fit' uses the fitted variogram model\n  cv_model &lt;- krige.cv(\n    NiederschlagJahr ~ 1, \n    locations = ~x + y, \n    data = df_Station, \n    model = m_Fit\n  )\n  \n  # Extract observed and predicted values\n  obs  &lt;- cv_model$observed\n  pred &lt;- cv_model$var1.pred\n  \n  # Compute Root Mean Squared Error (RMSE)\n  # 'na.rm = TRUE' ignores missing values\n  sqrt(mean((obs - pred)^2, na.rm = TRUE))\n})\n\n# Print the RMSE values for all variogram models\nprint(lst_RMSE_VarioFit |&gt; unlist())\n\n     Exp      Sph      Cir      Gau      Lin      Mat \n93.15135 87.70613 90.56338 96.44990 94.64919 92.29901 \n\n\nFrom this comparison, the spherical (Sph) model exhibits the lowest RMSE, indicating the best predictive performance for the dataset. Consequently, the spherical variogram will be used in subsequent analyses.\n\n\n2.5.4 Results of Ordinary Kriging Compared to IDW\nThe figure below presents the interpolation results obtained with ordinary kriging (OK) and inverse distance weighting (IDW) for the same dataset.\nThere is no significant difference between the two methods in overall pattern. Ordinary kriging produces a slightly smoother surface, reducing abrupt changes observed in IDW or nearest neighbor methods. OK is less influenced by local extreme values, thanks to its use of spatial correlation weights derived from the variogram.\n\n# Create an ordinary kriging (OK) model using the spherical variogram\n# 'NiederschlagJahr ~ 1' models only the mean (no trend)\n# 'locations = ~x + y' specifies the station coordinates\n# 'model = lst_FitVario$Sph' uses the fitted spherical variogram\n# 'nmax = 5' limits the number of nearest stations used for each prediction\nmodel_OK &lt;- gstat(\n  formula = NiederschlagJahr ~ 1,\n  locations = ~x + y,\n  data = df_Station,\n  model = lst_FitVario$Sph,\n  nmax = 5\n)\n\n# Interpolate precipitation values on the raster grid using ordinary kriging\n# 'rst_Template_NRW' provides the grid for prediction\n# 'mask(vct_Boundry_NRW)' ensures results are limited to NRW boundary\nrst_OK &lt;- interpolate(rst_Template_NRW, model_OK, debug.level = 0) |&gt; \n  mask(vct_Boundry_NRW)\n\n\n\nCode\ngp_OK &lt;- ggplot() +\n  geom_spatraster(data = rst_OK, aes(fill = var1.pred)) +\n  geom_spatvector(data = vct_Boundry_NRW, color = color_TUD_pink, fill = NA) +\n  geom_spatvector(data = vct_Station_NRW_S50, aes(fill = NiederschlagJahr),\n                  shape = 24, size = 3, color = \"grey96\", linewidth = 1) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(550, 1500)) +\n  ggtitle('Ordinary Kriging with Spherical Variogramm') +\n  coord_sf(expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.99, 0.01),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\"\n  ) +\n  guides(\n    fill = guide_colorbar(\n      barwidth = 8,\n      barheight = 0.6\n    )\n  )\n\n(gp_IDW | gp_OK)\n\n\n\n\n\n\n\n\n\n\n\n2.5.5 Effect of Variogram Model Choice\nThe subsequent figure shows the results for four different variogram models applied in OK:\n\nAt short distances, all models behave similarly, consistent with the fitted experimental variogram.\nDifferences at larger distances have a minimal effect on the interpolated surface.\nOverall, the choice of variogram model slightly influences long-range patterns but does not substantially change the final interpolation results.\n\nThis demonstrates that, for this dataset, ordinary kriging is robust to the selection of standard variogram models, and the key influence on the interpolation comes from the short-range spatial correlation captured in the variogram.\n\n# Generate ordinary kriging rasters for all fitted variogram models\nlst_rst_OK_VarioModel &lt;- map(lst_FitVario, \\(m_OK) {\n\n  # Create ordinary kriging model using the fitted variogram\n  model_OK &lt;- gstat(\n    formula = NiederschlagJahr ~ 1,   # model only the mean\n    locations = ~x + y,                # use station coordinates\n    data = df_Station,                 # station data\n    model = m_OK                        # use the current variogram model from the list\n  )\n\n  # Interpolate the raster using the kriging model and mask to NRW boundary\n  interpolate(rst_Template_NRW, model_OK, debug.level = 0) |&gt; \n    mask(vct_Boundry_NRW)\n})\n\n\n\nCode\nrst_OK_combined &lt;- rast(lst_rst_OK_VarioModel)\nrst_OK_combined &lt;- rst_OK_combined[[c(1, 3, 5, 7, 9, 11)]]\n# Set layer names to match parameter combinations\nnames(rst_OK_combined) &lt;- names(lst_rst_OK_VarioModel)\n\n# Create facet plot\nggplot() +\n  geom_spatraster(data = rst_OK_combined[[1:4]]) +\n  geom_spatvector(data = vct_Boundry_NRW, color = color_TUD_pink, fill = NA) +\n  facet_wrap(~lyr, ncol = 2) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(550, 1500)) +\n  coord_sf(expand = FALSE) +\n  labs(title = \"Ordinary Kriging (OK) - Variogram Comparison\") +\n  theme(\n    axis.title = element_blank(),\n    axis.text.y = element_text(angle = 90, hjust = 0.5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.99, 0.01),\n    legend.justification = c(\"right\", \"bottom\"),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\",\n    strip.text = element_text(size = 10, face = \"bold\")\n  ) +\n  guides(\n    fill = guide_colorbar(\n      barwidth = 8,\n      barheight = 0.6\n    )\n  )",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html",
    "href": "dataprocess/spatial_data.html",
    "title": "Basic Manipulation",
    "section": "",
    "text": "RPython\n\n\nThe terra package in R is a powerful and versatile package for working with geospatial data, including vector and raster data. It provides a wide range of functionality for reading, processing, analyzing, and visualizing spatial data.\nFor more in-depth information and resources on the terra package and spatial data science in R, you can explore the original website Spatial Data Science.\nFirstly load the library to the R space:\n\n# load the library\nlibrary(terra)\nlibrary(tidyverse)\n\n\n\nThe libraries for spatial data in Python are divided into several libraries, unlike the comprehensive terra library in R. For vector data, you can use the geopandas library, and for raster data, rasterio is a good choice, among others.\nFor more in-depth information and resources on the spatial data science in Python, you can explore the website Python Open Source Spatial Programming & Remote Sensing.\n\nimport os\nimport pandas as pd\nimport numpy as np\n# Vector\nimport geopandas as gpd\nfrom shapely.geometry import Point, LineString, Polygon, shape\nimport fiona\n\n# Rster\nimport rasterio\nfrom rasterio.plot import show as rast_plot\nfrom rasterio.crs import CRS\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nimport rasterio.features\nfrom rasterio.enums import Resampling\n\n# Plot\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#vector",
    "href": "dataprocess/spatial_data.html#vector",
    "title": "Basic Manipulation",
    "section": "2.1 Vector",
    "text": "2.1 Vector\nAs introduced in the section, spatial vector data typically consists of three main components:\n\nGeometry: Describes the spatial location and shape of features.\nAttributes: Non-spatial properties associated with features.\nCRS (Coordinate Reference System): Defines the spatial reference framework.\n\n\nRPython\n\n\n\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 &lt;- \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_1 &lt;- c(4484566, 4483922, 4483002, 4481929, 4481222, 4482500, 4483000, 4484666, 4484233)\ny_polygon_1 &lt;- c(5554566, 5554001, 5553233, 5554933, 5550666, 5551555, 5550100, 5551711, 5552767)\ngeometry_polygon_1 &lt;- cbind(id=1, part=1, x_polygon_1, y_polygon_1)\n# Define coordinates for the second polygon\nx_polygon_2 &lt;- c(4481929, 4481222, 4480500)\ny_polygon_2 &lt;- c(5554933, 5550666, 5552555)\ngeometry_polygon_2 &lt;- cbind(id=2, part=1, x_polygon_2, y_polygon_2)\n# Combine the two polygons into one data frame\ngeometry_polygon &lt;- rbind(geometry_polygon_1, geometry_polygon_2)\n\n# Create a vector layer for the polygons, specifying their type, attributes, CRS, and additional attributes\nvect_Test &lt;- vect(geometry_polygon, type=\"polygons\", \n                  atts = data.frame(ID_region = 1:2, Name = c(\"a\", \"b\")), \n                  crs = crs_31468)\nvect_Test$region_area &lt;- expanse(vect_Test)\n\n# Visualize the created polygons\nplot(vect_Test)\n\n\n\n\n\n\n\n\n\n\n\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 = \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_1 = [4484566, 4483922, 4483002, 4481929, 4481222, 4482500, 4483000, 4484666, 4484233]\ny_polygon_1 = [5554566, 5554001, 5553233, 5554933, 5550666, 5551555, 5550100, 5551711, 5552767]\n# Create a list of coordinate pairs for the first polygon\ngeometry_polygon_1 = Polygon([(x, y) for x, y in zip(x_polygon_1, y_polygon_1)])\n\n# Define coordinates for the second polygon\nx_polygon_2 = [4481929, 4481222, 4480500]\ny_polygon_2 = [5554933, 5550666, 5552555]\n# Create a list of coordinate pairs for the second polygon\ngeometry_polygon_2 = Polygon([(x, y) for x, y in zip(x_polygon_2, y_polygon_2)])\n\n# Construct Shapely polygons using the lists of coordinates\ngeometry_polygon = [geometry_polygon_1, geometry_polygon_2]\n\n\n# Create a GeoDataFrame with the polygons, specifying their attributes, CRS, and additional attributes\nvect_Test = gpd.GeoDataFrame({\n    'ID_region': [1, 2],\n    'Name': ['a', 'b'],\n    'geometry': geometry_polygon,\n}, crs=crs_31468)\n\n# Calculate the region area and add it as a new column\nvect_Test['region_area'] = vect_Test.area\n\n# Visualize the created polygons\nvect_Test.plot()\nplt.show()\n\n\n\n\n\n\n\nplt.close()",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#raster",
    "href": "dataprocess/spatial_data.html#raster",
    "title": "Basic Manipulation",
    "section": "2.2 Raster",
    "text": "2.2 Raster\nFor raster data, the geometry is relatively simple and can be defined by the following components:\n\nCoordinate of Original Point (X0, Y0) plus Resolutions (X and Y)\nBoundaries (Xmin, Xmax, Ymin, Ymax) plus Number of Rows and Columns\n\nOne of the most critical aspects of raster data is the values stored within its cells. You can set or modify these values using the values()&lt;- function in R.\n\nRPython\n\n\n\nrast_Test &lt;- rast(ncol=10, nrow=10, xmin=-150, xmax=-80, ymin=20, ymax=60)\nvalues(rast_Test) &lt;- runif(ncell(rast_Test))\n\nplot(rast_Test)\n\n\n\n\n\n\n\n\n\n\n\nfn_Rast_Test = \"C:\\\\Lei\\\\HS_Web\\\\data_share/raster_Py.tif\"\n\n# Create a new raster with the specified dimensions and extent\nncol, nrow = 10, 10\nxmin, xmax, ymin, ymax = -150, -80, 20, 60\n\n# Create the empty raster with random values\nwith rasterio.open(\n    fn_Rast_Test,\n    \"w\",\n    driver=\"GTiff\",\n    dtype=np.float32,\n    count=1,\n    width=ncol,\n    height=nrow,\n    transform=rasterio.transform.from_origin(xmin, ymax, (xmax - xmin) / ncol, (ymax - ymin) / nrow),\n    crs=\"EPSG:4326\"\n) as dst:\n    # Generate random values and assign them to the raster\n    random_values = np.random.rand(nrow, ncol).astype(np.float32)\n    dst.write(random_values, 1)  # Write the values to band 1\n\n# Now you have an empty raster with random values, and you can read and manipulate it as needed\nwith rasterio.open(fn_Rast_Test) as src:\n    rast_Test = src.read(1)\n\n\nrast_plot(rast_Test)\n\n\n\n\n\n\n\n\n\n\n\nCertainly, you can directly create a data file like an ASC (ASCII) file for raster data.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#assigning-a-crs",
    "href": "dataprocess/spatial_data.html#assigning-a-crs",
    "title": "Basic Manipulation",
    "section": "4.1 Assigning a CRS",
    "text": "4.1 Assigning a CRS\nIn cases where the Coordinate Reference System (CRS) information is not included in the data file’s content, you can assign it manually using the crs() function. This situation often occurs when working with raster data in formats like ASC (Arc/Info ASCII Grid) or other file formats that may not store CRS information.\n\nRPython\n\n\n\ncrs(rast_Test) &lt;- \"EPSG:31468\"\nrast_Test\n\nclass       : SpatRaster \nsize        : 5, 5, 1  (nrow, ncol, nlyr)\nresolution  : 1000, 1000  (x, y)\nextent      : 4480000, 4485000, 5550000, 5555000  (xmin, xmax, ymin, ymax)\ncoord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \nsource      : minibeispiel_raster.asc \nname        : minibeispiel_raster \n\n\nAs the results showed, the CRS information has been filled with the necessary details in line coord. ref..\n\n\n\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\", 'r+')\nrast_Test.crs = CRS.from_epsg(31468)\nprint(rast_Test.crs)\n\nEPSG:31468\n\n\n\n\n\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. You can obtain information about EPSG codes from the EPSG website.\n\n\n\n\n\n\nNOTE\n\n\n\nYou should not use this approach to change the CRS of a data set from what it is to what you want it to be. Assigning a CRS is like labeling something.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#transforming-vector-data",
    "href": "dataprocess/spatial_data.html#transforming-vector-data",
    "title": "Basic Manipulation",
    "section": "4.2 Transforming vector data",
    "text": "4.2 Transforming vector data\nThe transformation of vector data is relatively simple, as it involves applying a mathematical formula to the coordinates of each point to obtain their new coordinates. This transformation can be considered as without loss of precision.\n\nRPython\n\n\nThe project() function can be utilized to reproject both vector and raster data.\n\n# New CRS\ncrs_New &lt;- \"EPSG:4326\"\n# Reproject\nvect_Test_New &lt;- project(vect_Test, crs_New)\n\n# Info of vector layer\nvect_Test_New\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 3  (geometries, attributes)\n extent      : 11.72592, 11.78419, 50.08692, 50.13034  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_region  Name region_area\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;\n values      :         1     a   8.853e+06\n                       2     b   2.208e+06\n\n\n\n\n\ngeopands.to_crs()\n\n\n# New CRS\ncrs_New = \"EPSG:4326\"\n\n# Reproject the vector layer to the new CRS\nvect_Test_New = vect_Test.to_crs(crs=crs_New)\n\n# Info of vector layer\nprint(vect_Test_New)\n\n   ID_region  ...                                           geometry\n0          1  ...  MULTIPOLYGON (((11.78268 50.12711, 11.7737 50....\n1          2  ...  MULTIPOLYGON (((11.74578 50.13033, 11.73611 50...\n\n[2 rows x 4 columns]",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#transforming-raster-data",
    "href": "dataprocess/spatial_data.html#transforming-raster-data",
    "title": "Basic Manipulation",
    "section": "4.3 Transforming raster data",
    "text": "4.3 Transforming raster data\nVector data can be transformed from lon/lat coordinates to planar and back without loss of precision. This is not the case with raster data. A raster consists of rectangular cells of the same size (in terms of the units of the CRS; their actual size may vary). It is not possible to transform cell by cell. For each new cell, values need to be estimated based on the values in the overlapping old cells. If the values are categorical data, the “nearest neighbor” method is commonly used. Otherwise some sort of interpolation is employed (e.g. “bilinear”). (From Spatial Data Science)\n\n\n\n\n\n\nNote\n\n\n\nBecause projection of rasters affects the cell values, in most cases you will want to avoid projecting raster data and rather project vector data.\n\n\n\n4.3.1 With CRS\nThe simplest approach is to provide a new CRS:\n\nRPython\n\n\n\nproject()\n\n\n# New CRS\ncrs_New &lt;- \"EPSG:4326\"\n# Reproject\nrast_Test_New &lt;- project(rast_Test, crs_New, method = 'near')\n\n# Info and Plot of vector layer\nrast_Test_New\n\nclass       : SpatRaster \nsize        : 4, 6, 1  (nrow, ncol, nlyr)\nresolution  : 0.01176853, 0.01176853  (x, y)\nextent      : 11.7188, 11.78941, 50.08395, 50.13102  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\n\nplot(rast_Test)\nplot(rast_Test_New)\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nNew\n\n\n\n\n\n\n\n\nfn_Rast_New = 'C:\\\\Lei\\\\HS_Web\\\\data_share/minibeispiel_raster.tif'\n# Define the new CRS\nnew_crs = {'init': 'EPSG:4326'}\ntransform, width, height = calculate_default_transform(\n        rast_Test.crs, new_crs, rast_Test.width, rast_Test.height, *rast_Test.bounds)\nkwargs = rast_Test.meta.copy()\nkwargs.update({\n        'crs': new_crs,\n        'transform': transform,\n        'width': width,\n        'height': height\n    })        \nrast_Test_New = rasterio.open(fn_Rast_New, 'w', **kwargs)        \nreproject(\n    source=rasterio.band(rast_Test, 1),\n    destination=rasterio.band(rast_Test_New, 1),\n    #src_transform=rast_Test.transform,\n    src_crs=rast_Test.crs,\n    #dst_transform=transform,\n    dst_crs=new_crs,\n    resampling=Resampling.nearest)\n\n(None, None)\n\nrast_Test_New.close()        \n\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\")\nrast_plot(rast_Test)\n\n\n\n\n\n\n\nrast_Test_New = rasterio.open(fn_Rast_New)\nrast_plot(rast_Test_New)\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 With Mask Raster\nA second way is provide an existing SpatRaster with the geometry you desire, with special boundary and resolution, this is a better way.\n\n# New CRS\nrast_Mask &lt;- rast(ncol=10, nrow=10, xmin=265000, xmax=270000, ymin=5553000, ymax=5558000)\ncrs(rast_Mask) &lt;- \"EPSG:25833\"\nvalues(rast_Mask) &lt;- 1\n# Reproject\nrast_Test_New &lt;- project(rast_Test, rast_Mask)\n\n# Info and Plot of vector layer\nrast_Test_New\n\nclass       : SpatRaster \nsize        : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 500, 500  (x, y)\nextent      : 265000, 270000, 5553000, 5558000  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRS89 / UTM zone 33N (EPSG:25833) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\n\nplot(rast_Test)\nplot(rast_Test_New)\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nNew",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#attributes-manipulation",
    "href": "dataprocess/spatial_data.html#attributes-manipulation",
    "title": "Basic Manipulation",
    "section": "5.1 Attributes manipulation",
    "text": "5.1 Attributes manipulation\n\n5.1.1 Extract all Attributes\n\nas.data.frame()\n\n\ndf_Attr &lt;- as.data.frame(vect_Test)\ndf_Attr\n\n  ID_region Name region_area\n1         1    a     8853404\n2         2    b     2208109\n\n\n\n\n5.1.2 Extract one with attribute name\n\n$name\n[, \"name\"]\n\n\nvect_Test$ID_region\n\n[1] 1 2\n\nvect_Test[,\"ID_region\"]\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 1  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region\n type        :     &lt;int&gt;\n values      :         1\n                       2\n\n\n\n\n5.1.3 Add a new attribute\n\n$name &lt;-\n[, \"name\"] &lt;-\n\n\nvect_Test$New_Attr &lt;- c(\"n1\", \"n2\")\nvect_Test[,\"New_Attr\"] &lt;- c(\"n1\", \"n2\")\n\n\n\n5.1.4 Merge several attributes\n\nsame order\n\ncbind()\n\ncommon (key-)attributes\n\nmerge()\n\n\n\ndf_New_Attr &lt;- data.frame(Name = c(\"a\", \"b\"), new_Attr2 = c(9, 6))\n\ncbind(vect_Test, df_New_Attr)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 6  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region  Name region_area New_Attr  Name new_Attr2\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;num&gt;\n values      :         1     a   8.853e+06       n1     a         9\n                       2     b   2.208e+06       n2     b         6\n\nmerge(vect_Test, df_New_Attr, by = \"Name\")\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 5  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       :  Name ID_region region_area New_Attr new_Attr2\n type        : &lt;chr&gt;     &lt;int&gt;       &lt;num&gt;    &lt;chr&gt;     &lt;num&gt;\n values      :     a         1   8.853e+06       n1         9\n                   b         2   2.208e+06       n2         6\n\n\n\n\n5.1.5 Delete a attribute\n\n$name &lt;- NULL\n\n\nvect_Test$New_Attr &lt;- c(\"n1\", \"n2\")\nvect_Test[,\"New_Attr\"] &lt;- c(\"n1\", \"n2\")",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#object-append-and-aggregate",
    "href": "dataprocess/spatial_data.html#object-append-and-aggregate",
    "title": "Basic Manipulation",
    "section": "5.2 Object Append and aggregate",
    "text": "5.2 Object Append and aggregate\n\n5.2.1 Append new Objects\n\nRPython\n\n\n\nrbind()\n\n\n# New Vect\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 &lt;- \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_3 &lt;- c(4480400, 4481222, 4480500)\ny_polygon_3 &lt;- c(5551000, 5550666, 5552555)\ngeometry_polygon_3 &lt;- cbind(id=3, part=1, x_polygon_3, y_polygon_3)\n\n# Create a vector layer for the polygons, specifying their type, attributes, CRS, and additional attributes\nvect_New &lt;- vect(geometry_polygon_3, type=\"polygons\", atts = data.frame(ID_region = 3, Name = c(\"b\")), crs = crs_31468)\nvect_New$region_area &lt;- expanse(vect_New)\n\n# Append the objects\nvect_Append &lt;- rbind(vect_Test, vect_New)\nvect_Append\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 3, 4  (geometries, attributes)\n extent      : 4480400, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region  Name region_area New_Attr\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;    &lt;chr&gt;\n values      :         1     a   8.853e+06       n1\n                       2     b   2.208e+06       n2\n                       3     b   6.558e+05       NA\n\n\n\n\n\npandas.concat()\n\n\n# Define the coordinate reference system (CRS) with EPSG code\ncrs_31468 = \"EPSG:31468\"\n\n# Define coordinates for the new polygon\nx_polygon_3 = [4480400, 4481222, 4480500]\ny_polygon_3 = [5551000, 5550666, 5552555]\n\n# Create a Polygon geometry\ngeometry_polygon_3 = Polygon(zip(x_polygon_3, y_polygon_3))\n\n# Create a GeoDataFrame for the new polygon\nvect_New = gpd.GeoDataFrame({'ID_region': [3], 'Name': ['b'], 'geometry': [geometry_polygon_3]}, crs=crs_31468)\n\n# Calculate the region area\nvect_New['region_area'] = vect_New['geometry'].area\n\n# Append the new GeoDataFrame to the existing one\nvect_Append = gpd.GeoDataFrame(pd.concat([vect_Test, vect_New], ignore_index=True), crs=crs_31468)\n\n# Now, vect_Append contains the combined data\nprint(vect_Append)\n\n   ID_region  ...                                           geometry\n0          1  ...  MULTIPOLYGON (((4484566 5554566, 4483922 55540...\n1          2  ...  MULTIPOLYGON (((4481929 5554933, 4481222 55506...\n2          3  ...  POLYGON ((4480400 5551000, 4481222 5550666, 44...\n\n[3 rows x 4 columns]\n\n\n\n\n\n\n\n5.2.2 Aggregate / Dissolve\nIt is common to aggregate (“dissolve”) polygons that have the same value for an attribute of interest.\n\nRPython\n\n\n\naggregate()\n\n\n# Aggregate by the \"Name\"\nvect_Aggregated &lt;- terra::aggregate(vect_Append, by = \"Name\")\nvect_Aggregated\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 5  (geometries, attributes)\n extent      : 4480400, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       :  Name mean_ID_region mean_region_area New_Attr agg_n\n type        : &lt;chr&gt;          &lt;num&gt;            &lt;num&gt;    &lt;chr&gt; &lt;int&gt;\n values      :     a              1        8.853e+06       n1     1\n                   b            2.5        1.432e+06       NA     2\n\n\nplot(vect_Append, \"ID_region\")\nplot(vect_Aggregated, \"Name\")\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nAggregated\n\n\n\n\n\n\n\n\ngeopandas.dissolve()\n\n\n# Aggregate by the \"Name\"\nvect_Aggregated = vect_Append.dissolve(by=\"Name\", aggfunc=\"first\")\n\nprint(vect_Aggregated)\n\n                                               geometry  ...   region_area\nName                                                     ...              \na     POLYGON ((4484566 5554566, 4483922 5554001, 44...  ...  8.853404e+06\nb     POLYGON ((4480400 5551000, 4480500 5552555, 44...  ...  2.208109e+06\n\n[2 rows x 3 columns]\n\n\n\nvect_Test.plot()\nplt.show()\n\n\n\n\n\n\n\nplt.close()\nvect_Aggregated.plot()\nplt.show()\n\n\n\n\n\n\n\nplt.close()",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#overlap",
    "href": "dataprocess/spatial_data.html#overlap",
    "title": "Basic Manipulation",
    "section": "5.3 Overlap",
    "text": "5.3 Overlap\nTo perform operations that involve overlap between two vector datasets, we will create a new vector dataset:\n\nRPython\n\n\n\nvect_Overlap &lt;- as.polygons(rast_Test)[1,]\nnames(vect_Overlap) &lt;- \"ID_Rast\"\n\nplot(vect_Overlap, \"ID_Rast\")\n\n\n\n\n\n\n\n\n\n\n\n# Read the raster and get the shapes\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\", 'r+')\nrast_Test.crs = CRS.from_epsg(31468)\ntransform = rast_Test.transform\nshapes = rasterio.features.shapes(rast_Test.read(1), transform=transform)\n\n# Convert the shapes to a GeoDataFrame\ngeometries = [shape(s) for s, v in shapes if v == 1]\nvect_Overlap = gpd.GeoDataFrame({'geometry': geometries})\n\n# Add an \"ID_Rast\" column to the GeoDataFrame\nvect_Overlap['ID_Rast'] = range(1, len(geometries) + 1)\nvect_Overlap.crs =\"EPSG:31468\"\n\n# Plot the polygons with \"ID_Rast\" as the attribute\nvect_Overlap.plot(column='ID_Rast')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n5.3.1 Erase\n\nRPython\n\n\n\nerase()\n\n\nvect_Erase &lt;- erase(vect_Test, vect_Overlap)\nplot(vect_Erase, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='difference')\n\n\nvect_Erase = gpd.overlay(vect_Test, vect_Overlap, how='difference')\nvect_Erase.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.2 Intersect\n\nRPython\n\n\n\nintersect()\n\n\nvect_Intersect &lt;- terra::intersect(vect_Test, vect_Overlap)\nplot(vect_Intersect, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='intersection')\n\n\nvect_Intersect = gpd.overlay(vect_Test, vect_Overlap, how='intersection')\nvect_Intersect.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.3 Union\nAppends the geometries and attributes of the input.\n\nRPython\n\n\n\nunion()\n\n\nvect_Union &lt;- terra::union(vect_Test, vect_Overlap)\nplot(vect_Union, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='union')\n\n\nvect_Union = gpd.overlay(vect_Test, vect_Overlap, how='union')\nvect_Union.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.4 Cover\ncover() is a combination of intersect() and union(). intersect returns new (intersected) geometries with the attributes of both input datasets. union appends the geometries and attributes of the input. cover returns the intersection and appends the other geometries and attributes of both datasets.\n\nRPython\n\n\n\ncover()\n\n\nvect_Cover &lt;- terra::cover(vect_Test, vect_Overlap)\nplot(vect_Cover, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='identity')\n\n\nvect_Cover = gpd.overlay(vect_Test, vect_Overlap, how='identity')\nvect_Cover.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.5 Difference\n\nRPython\n\n\n\nsymdif()\n\n\nvect_Difference &lt;- terra::symdif(vect_Test, vect_Overlap)\nplot(vect_Difference, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='symmetric_difference')\n\n\nvect_Difference = gpd.overlay(vect_Test, vect_Overlap, how='symmetric_difference')\nvect_Difference.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#raster-algebra",
    "href": "dataprocess/spatial_data.html#raster-algebra",
    "title": "Basic Manipulation",
    "section": "6.1 Raster algebra",
    "text": "6.1 Raster algebra\nMany generic functions that allow for simple and elegant raster algebra have been implemented for Raster objects, including the normal algebraic operators such as +, -, *, /, logical operators such as &gt;, &gt;=, &lt;, ==, !, and functions like abs, round, ceiling, floor, trunc, sqrt, log, log10, exp, cos, sin, atan, tan, max, min, range, prod, sum, any, all. In these functions, you can mix raster objects with numbers, as long as the first argument is a raster object. (Spatial Data Science)\n\nRPython\n\n\n\nrast_Add &lt;- rast_Test + 10\nplot(rast_Add)\n\n\n\n\n\n\n\n\n\n\n\nrast_Add = rast_Test_data + 10\nrast_plot(rast_Add)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#replace-with-condition",
    "href": "dataprocess/spatial_data.html#replace-with-condition",
    "title": "Basic Manipulation",
    "section": "6.2 Replace with Condition",
    "text": "6.2 Replace with Condition\n\nRPython\n\n\n\nrast[condition] &lt;-\n\n\n# Copy to a new raster\nrast_Replace &lt;- rast_Test\n\n# Replace\nrast_Replace[rast_Replace &gt; 1] &lt;- 10\nplot(rast_Replace)\n\n\n\n\n\n\n\n\n\n\n\nrast[condition] =\n\n\nrast_Replace = rast_Test_data\n\n# Replace values greater than 1 with 10\nrast_Replace[rast_Replace &gt; 1] = 10\nrast_plot(rast_Replace)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#summary-of-multi-layers",
    "href": "dataprocess/spatial_data.html#summary-of-multi-layers",
    "title": "Basic Manipulation",
    "section": "6.3 Summary of multi-layers",
    "text": "6.3 Summary of multi-layers\n\nRPython\n\n\n\nrast_Mean &lt;- mean(rast_Test, rast_Replace)\nplot(rast_Mean)\n\n\n\n\n\n\n\n\n\n\n\nrast_Mean = (rast_Test_data + rast_Replace) / 2\nrast_plot(rast_Mean)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#aggregate-and-disaggregate",
    "href": "dataprocess/spatial_data.html#aggregate-and-disaggregate",
    "title": "Basic Manipulation",
    "section": "6.4 Aggregate and disaggregate",
    "text": "6.4 Aggregate and disaggregate\n\nRPython\n\n\n\naggregate()\ndisagg()\n\n\n# Aggregate by factor 2\nrast_Aggregate &lt;- aggregate(rast_Test, 2)\nplot(rast_Aggregate)\n\n\n\n\n\n\n\n# Disaggregate by factor 2\nrast_Disagg &lt;- disagg(rast_Test, 2)\nrast_Disagg\n\nclass       : SpatRaster \nsize        : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 500, 500  (x, y)\nextent      : 4480000, 4485000, 5550000, 5555000  (xmin, xmax, ymin, ymax)\ncoord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \nsource(s)   : memory\nvarname     : minibeispiel_raster \nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\nplot(rast_Disagg)\n\n\n\n\n\n\n\n\n\n\n\n# Aggregate by factor 2\nrast_Aggregate = rast_Test_data\nrast_Aggregate = rast_Aggregate[::2, ::2]\nrast_plot(rast_Aggregate)\n\n\n\n\n\n\n\n# Disaggregate by factor 2\nrast_Disagg = rast_Test_data\nrast_Disagg = np.repeat(np.repeat(rast_Disagg, 2, axis=0), 2, axis=1)\nrast_plot(rast_Disagg)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#crop",
    "href": "dataprocess/spatial_data.html#crop",
    "title": "Basic Manipulation",
    "section": "6.5 Crop",
    "text": "6.5 Crop\nThe crop function lets you take a geographic subset of a larger raster object with an extent. But you can also use other spatial object, in them an extent can be extracted.\n\ncrop()\n\nwith extention\nwith rster\nwith vector\n\n\n\nrast_Crop &lt;- crop(rast_Test, vect_Test[1,])\nplot(rast_Crop)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#trim",
    "href": "dataprocess/spatial_data.html#trim",
    "title": "Basic Manipulation",
    "section": "6.6 Trim",
    "text": "6.6 Trim\n\ntrim()\n\nTrim (shrink) a SpatRaster by removing outer rows and columns that are NA or another value.\n\nrast_Trim0 &lt;- rast_Test\nrast_Trim0[21:25] &lt;- NA\nrast_Trim &lt;- trim(rast_Trim0)\n\nplot(rast_Trim0)\nplot(rast_Trim)\n\n\n\n\n\n\nwith NA\n\n\n\n\n\n\n\nTrimed",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#mask",
    "href": "dataprocess/spatial_data.html#mask",
    "title": "Basic Manipulation",
    "section": "6.7 Mask",
    "text": "6.7 Mask\n\nRPython\n\n\n\nmask()\ncrop(mask = TRUE) = mask() + trim()\n\nWhen you use mask manipulation in spatial data analysis, it involves setting the cells that are not covered by a mask to NA (Not Available) values. If you apply the crop(mask = TRUE) operation, it means that not only will the cells outside of the mask be set to NA, but the resulting raster will also be cropped to match the extent of the mask.\n\nrast_Mask &lt;- mask(rast_Disagg, vect_Test[1,])\nrast_CropMask &lt;- crop(rast_Disagg, vect_Test[1,], mask = TRUE)\n\nplot(rast_Mask)\nplot(rast_CropMask)\n\n\n\n\n\n\nMask\n\n\n\n\n\n\n\nMask + Crop (Trim)\n\n\n\n\n\n\n\n\nvect_Mask = vect_Test.iloc[0:1].geometry.values[0]\n\n# Create a mask for the vect_Mask on the raster\nrast_Mask = rasterio.features.geometry_mask([vect_Mask], out_shape=rast_Test.shape, transform=rast_Test.transform, invert=True)\n# Apply the mask to the raster\nrast_Crop = rast_Test_data.copy()\nrast_Crop[~rast_Mask] = rast_Test.nodata  # Set values outside the geometry to nodata\n\nrast_plot(rast_Crop)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/index.html",
    "href": "dataprocess/index.html",
    "title": "Data Processing",
    "section": "",
    "text": "Data processing is a fundamental step in transforming raw data into valuable insights. In the domain of hydrology, data often involves spatial and temporal aspects, making it complex and voluminous. Effective processing tools and methods are essential to successfully analyze and utilize this data. This page provides valuable technological skills for processing raw data.",
    "crumbs": [
      "Dataprocess"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html",
    "href": "dataprocess/data_load.html",
    "title": "Data Loading",
    "section": "",
    "text": "This Aritcl will show the process to load data from other files. I t will divide into four paties: plain text (read able ASCII), Excel, NetCDF and spatial data.\nOverview:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#example-file",
    "href": "dataprocess/data_load.html#example-file",
    "title": "Data Loading",
    "section": "1.1 Example File",
    "text": "1.1 Example File\nLet’s start with an example CSV file named Bachum_2763190000100.csv. This file contains pegel discharge data and is sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github, just like you would access a local file.\nTake a look:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#library-and-functions",
    "href": "dataprocess/data_load.html#library-and-functions",
    "title": "Data Loading",
    "section": "1.2 Library and functions",
    "text": "1.2 Library and functions\n\nRPython\n\n\nFirst, we need to load the necessary library tidyverse. This library collection includes readr for reading files and dplyr for data manipulation, among others.\nAnd, we set the URL address as the file path (including the file name).\n\n# load the library\nlibrary(tidyverse)\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n\nThe documentation for the readr library is available online and can be accessed at https://readr.tidyverse.org.\nOf particular interest are the following functions:\n\nreadr::read_csv()\nreadr::read_table()\n\nWe can observe that the CSV file is divided by semicolons. Therefore, it’s more appropriate to use read_csv2() rather than read_csv().\nThe difference between read_*() functions in the readr package is determined by the delimiter character used in the files:\n\n\n\nCHEAT SHEET from Rstudio\n\n\n\n\n\n# load the library\nimport pandas as pd\nfn_Bachum = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n\nThe documentation for the pandas library is available online and can be accessed at https://pandas.pydata.org/docs/index.html.\nOf particular interest are the following functions:\n\npandas.read_csv()\npandas.read_table()",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#metadata-handel",
    "href": "dataprocess/data_load.html#metadata-handel",
    "title": "Data Loading",
    "section": "1.3 Metadata Handel",
    "text": "1.3 Metadata Handel\nMetadata can vary widely between datasets, so it’s handled separately from the data body.\nThere are three ways to deal with metadata:\n\nDirectly Ignore: This approach involves ignoring metadata when it’s redundant or readily available from other data sources, such as file names or external references.\nExtract from Text: When metadata is crucial but not in table form, you can extract information from text strings. For more information, refer to the section on string manipulation Section 3.\nRead as a Second Table: If metadata is well-organized in a tabular format, it can be read as a separate table to facilitate its use.\n\nIn the Bachum_2763190000100.csv file, you will find that there are 10 lines of metadata, which are well-organized in a tabular format. However, it’s important to note that the consistency in values column varies.\n\n1.3.1 Directly Ignore use grguments skip\n\n# skip = 10\nread_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\n\n# A tibble: 10 × 2\n   X1            X2\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n\n\n\n\n1.3.2 Read metadata as table\nWhen directly reading all metadata into one table, you may encounter mixed data types. In the metadata, there are three data types:\n\nNumeric: Examples include Pegelnullpunkt and Einzugsgebiet.\nString: This category covers fields like Name, Pegelnummer, and others.\nDate: Date values are present in columns like Datum von and Datum bis.\n\nIn a data frame (tibble), columns must have the same data type. Consequently, R will automatically convert them to a single data type, which is typically string.\nTo address this situation, you should specify the data type you want to read. For example, to read the date values in lines 4 and 5, you can use the following settings: 1. skip = 3 to skip the first three lines of metadata. 2. n_max = 2 to read the next two lines (lines 4 and 5) as date values.\n\nRPython\n\n\n\n# skip = 3\nread_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\n\n# A tibble: 2 × 2\n  X1        X2        \n  &lt;chr&gt;     &lt;chr&gt;     \n1 Datum von 01.01.1990\n2 Datum bis 31.12.2022\n\n\n\n\n\ndf_bach = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\nprint(df_bach)\n\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n\n\n\n\n\nUnfortunately, R may not always recognize date values correctly, so you may need to perform additional steps for conversion:\n\nAfter Reading: This involves transforming the data from its initial format to the desired date format within your R environment.\nSet the Data Type by Reading: Another approach is to set the data type while reading the data.\n\nMore details in the next section:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#load-tabular-data",
    "href": "dataprocess/data_load.html#load-tabular-data",
    "title": "Data Loading",
    "section": "1.4 Load tabular data",
    "text": "1.4 Load tabular data\n\nRPython\n\n\nTo read the first 10 lines of metadata, you can use the n_max setting with a value of n_max = 10 in the read_csv2() function.\n\nread_csv2(fn_Bachum, n_max = 10, col_names = FALSE)\n\n# A tibble: 10 × 2\n   X1                          X2             \n   &lt;chr&gt;                       &lt;chr&gt;          \n 1 \"Name\"                      \"Bachum\"       \n 2 \"Pegelnummer\"               \"2763190000100\"\n 3 \"Gew\\xe4sser\"               \"Ruhr\"         \n 4 \"Datum von\"                 \"01.01.1990\"   \n 5 \"Datum bis\"                 \"31.12.2022\"   \n 6 \"Parameter\"                 \"Abfluss\"      \n 7 \"Q Einheit\"                 \"m\\xb3/s\"      \n 8 \"Tagesmittelwerte\"           &lt;NA&gt;          \n 9 \"Pegelnullpunkt [m\\xfcNHN]\" \"146,83\"       \n10 \"Einzugsgebiet [km\\xb2]\"    \"1.532,02\"     \n\n\nAfter dealing with the metadata, we can proceed to load the data body using the readr::read_*() function cluster. Plain text files typically store data in a tabular or matrix format, both of which have at most two dimensions. When using the readr::read_() function, it automatically returns a tibble. If your data in the text file is in matrix format, you can use conversion functions like as.matrix() to transform it into other data structures.\n\n# 1. load\ntb_Read &lt;- read_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\ntb_Read\n\n# A tibble: 10 × 2\n   X1            X2\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n\n# 2. convert\ndf_Read &lt;- as.data.frame(tb_Read)\nmat_Read &lt;- as.matrix(tb_Read)\n\ndf_Read\n\n           X1     X2\n1  01.01.1990 20.640\n2  02.01.1990 18.994\n3  03.01.1990 17.949\n4  04.01.1990 16.779\n5  05.01.1990 16.019\n6  06.01.1990 14.817\n7  07.01.1990 14.296\n8  08.01.1990 13.952\n9  09.01.1990 14.403\n10 10.01.1990 14.500\n\nmat_Read\n\n      X1           X2      \n [1,] \"01.01.1990\" \"20.640\"\n [2,] \"02.01.1990\" \"18.994\"\n [3,] \"03.01.1990\" \"17.949\"\n [4,] \"04.01.1990\" \"16.779\"\n [5,] \"05.01.1990\" \"16.019\"\n [6,] \"06.01.1990\" \"14.817\"\n [7,] \"07.01.1990\" \"14.296\"\n [8,] \"08.01.1990\" \"13.952\"\n [9,] \"09.01.1990\" \"14.403\"\n[10,] \"10.01.1990\" \"14.500\"\n\n\n\n\n\ntb_Read = pd.read_csv(fn_Bachum, skiprows=10, nrows=10, header=None, delimiter=';', decimal=',', encoding='latin-1')\nprint(tb_Read)\n\n            0       1\n0  01.01.1990  20.640\n1  02.01.1990  18.994\n2  03.01.1990  17.949\n3  04.01.1990  16.779\n4  05.01.1990  16.019\n5  06.01.1990  14.817\n6  07.01.1990  14.296\n7  08.01.1990  13.952\n8  09.01.1990  14.403\n9  10.01.1990  14.500",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#sec-datatype",
    "href": "dataprocess/data_load.html#sec-datatype",
    "title": "Data Loading",
    "section": "1.5 Data type",
    "text": "1.5 Data type\nIn this section, we will work with a custom-made text file that contains various data types and formats. The file consists of three rows, with one of them serving as the header containing column names, and six columns in total.\nLet’s take a look:\n\nActually the function will always guse the dattype for each column, when the data really normally format the function will return the right datatype for the data:\n\nRPython\n\n\n\nread_table(fn_Datatype)\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de    str  \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;      &lt;chr&gt;\n1     1      0.1 0,1      2023-09-15 15.09.2023 en   \n2     9      9.6 9,6      2023-09-16 16.09.2023 de   \n\n\n\n\n\ndf = pd.read_table(fn_Datatype)\nprint(df)\n\n   int  float_en float_de     date_en     date_de str\n0    1       0.1      0,1  2023-09-15  15.09.2023  en\n1    9       9.6      9,6  2023-09-16  16.09.2023  de\n\nprint(df.dtypes)\n\nint           int64\nfloat_en    float64\nfloat_de     object\ndate_en      object\ndate_de      object\nstr          object\ndtype: object\n\n\n\n\n\nBy default, functions like readr::read_table() in R and pandas.read_table() in Python will attempt to guess data types automatically when reading data. Here’s how this guessing typically works:\n\nIf a column contains only numbers and decimal dots (periods), it will be recognized as numeric (double in R and int or float in Python).\nIf a date is formatted in “Y-M-D” (e.g., “2023-08-27”) or “h:m:s” (e.g., “15:30:00”) formats, it may be recognized as a date or time type. Nur in R\nIf the data type cannot be confidently determined, it is often treated as a string (str in R and object in Python).\n\nThis automatic guessing is convenient, but it’s essential to verify the inferred data types, especially when working with diverse datasets.\n\n1.5.1 Set the Data Type by Reading\nExplicitly setting data types using the col_types (in R) or dtype (in Python) argument can help ensure correct data handling.\n\nRPython\n\n\nTo address the issue of date recognition, you can set the col_types argument, you can use a compact string representation where each character represents one column:\n\nc: Character\ni: Integer\nn: Number\nd: Double\nl: Logical\nf: Factor\nD: Date\nT: Date Time\nt: Time\n?: Guess\n_ or -: Skip\n\nto \"cD\" when reading the data. This informs the function that the first column contains characters (c) and the second column contains Dates (D).\n\nread_table(fn_Datatype, col_types = \"iddDDc\")\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;  &lt;chr&gt;\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n\n\n\nread_table(fn_Datatype, col_types = \"idd?Dc\")\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;  &lt;chr&gt;\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n\n\n\n\nTo set data types when reading data using functions pandas.read_*, you have three main choices by using the dtype parameter:\n\nstr: Specify the data type as a string.\nint: Specify the data type as an integer.\nfloat: Specify the data type as a floating-point number.\n\nHowever, you can also use the dtype parameter with a callable function to perform more advanced type conversions. Some commonly used functions include:\n\npd.to_datetime: Converts a column to datetime format.\npd.to_numeric: Converts a column to numeric (integer or float) format.\npd.to_timedelta: Converts a column to timedelta format.\n\n\n# Define column names and types as a dictionary\ncol_types = {\"X1\": str, \"X2\": pd.to_datetime}\n# Read the CSV file, skip 3 rows, read 2 rows, and specify column names and types\ndf = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', names=[\"X1\", \"X2\"], dtype=col_types, encoding='latin-1')\n\n# Display the loaded data\nprint(df)\n\n\nDON’T RUN Error, because data doesn’t match the default format of ‘Y-m-d’.\n\n\n\n\n\nUnfortunately, the default date format in R and Python may not work for German-style dates like “d.m.Y” as R and Python primarily recognizes the “Y-m-d” format.\n\n\n\n1.5.2 After Reading\nTo address this issue, you can perform date conversions after reading the data:\n\nRPython\n\n\nUsing function as.Date() and specify the date format using the format argument, such as format = \"%d.%m.%Y\".\n\ndf_Date &lt;- read_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\ndf_Date$X2 &lt;- df_Date$X2 |&gt; as.Date(format = \"%d.%m.%Y\")\ndf_Date\n\n# A tibble: 2 × 2\n  X1        X2        \n  &lt;chr&gt;     &lt;date&gt;    \n1 Datum von 1990-01-01\n2 Datum bis 2022-12-31\n\n\n\n\n\ndf_Date = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\n\n# Display the loaded data\nprint(df_Date)\n\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n\n# 2. Convert the second column (X2) to a date format\ndf_Date[1] = pd.to_datetime(df_Date[1], format='%d.%m.%Y')\n\n# Display the DataFrame with the second column converted to date format\nprint(df_Date)\n\n           0          1\n0  Datum von 1990-01-01\n1  Datum bis 2022-12-31\n\nprint(df_Date.dtypes)\n\n0            object\n1    datetime64[ns]\ndtype: object",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#example-file-1",
    "href": "dataprocess/data_load.html#example-file-1",
    "title": "Data Loading",
    "section": "2.1 Example File",
    "text": "2.1 Example File\nLet’s begin with an example Excel file named Pegeln_NRW.xlsx. This file contains information about measurement stations in NRW (Nordrhein-Westfalen, Germany) and is sourced from open data available at ELWAS-WEB NRW. You can also access it directly from Github.\nTake a look:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#library-and-functions-1",
    "href": "dataprocess/data_load.html#library-and-functions-1",
    "title": "Data Loading",
    "section": "2.2 Library and functions",
    "text": "2.2 Library and functions\n\nRPython\n\n\nTo load the necessary library, readxl, and access its help documentation, you can visit this link. The readxl::read_excel() function is versatile, as it can read both .xls and .xlsx files and automatically detects the format based on the file extension. Additionally, you have the options of using read_xls() for .xls files and read_xlsx() for .xlsx files. More details in the Page.\n\n# load the library\nlibrary(readxl)\n# The Excel file cannot be read directly from GitHub. You will need to download it to your local machine first\nfn_Pegeln &lt;- \"C:\\\\Lei\\\\HS_Web\\\\data_share/Pegeln_NRW.xlsx\"\n\n\n\nThe pandas.read_excel() function is versatile, as it can read both .xls and .xlsx files and automatically detects the format based on the file extension. More details in the Page.\n\nimport pandas as pd\n\n# Specify the path to the Excel file\nfn_Pegeln = \"C:\\\\Lei\\\\HS_Web\\\\data_share/Pegeln_NRW.xlsx\"",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#load-tabular-data-1",
    "href": "dataprocess/data_load.html#load-tabular-data-1",
    "title": "Data Loading",
    "section": "2.3 Load tabular data",
    "text": "2.3 Load tabular data\nSimilar to plain text files, metadata is often provided before the data body in Excel files. In Excel, each cell can be assigned a specific data type, while in R tables (data.frame or tibble), every column must have the same data type. This necessitates separate handling of metadata and data body to ensure that the correct data types are maintained.\nUnlike plain text files where we can only select lines to load, Excel allows us to define coordinates to access a specific celles-box wherever they are located.\n\n2.3.1 First try without any setting\n\nRPython\n\n\n\n# try without setting\ntb_Pegeln &lt;- read_excel(fn_Pegeln)\ntb_Pegeln\n\n# A tibble: 277 × 16\n   Suchergebnisse Pegel.…¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 \"Suchkriterien:\\n -- \\… &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 2  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 3  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 4 \"Name\"                  Pege… Gewä… Betr… Pege… Einz… Q von Q bis NQ    MNQ  \n 5 \"Ahlen\"                 3211… Werse LANU… 73,47 46,62 1975  2013  0     0,07 \n 6 \"Ahmsen\"                4639… Werre LANU… 64,28 593   1963  2022  1,21  2,22 \n 7 \"Ahrhütte-Neuhof\"       2718… Ahr   LANU… 340,… 124   1986  2011  0,22  0,36 \n 8 \"Albersloh\"             3259… Werse LANU… 48,68 321,… 1973  2020  0,12  0,24 \n 9 \"Altena\"                2766… Lenne LANU… 154,… 1.190 1950  2021  1,36  6,48 \n10 \"Altena_Rahmedestraße\"  2766… Rahm… LANU… 157,… 29,6  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n# ℹ 267 more rows\n# ℹ abbreviated name: ¹​`Suchergebnisse Pegel.xlsx 14.09.2023 10:01`\n# ℹ 6 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;\n\n\n\n\n\n# Read the Excel file into a pandas DataFrame\ntb_Pegeln = pd.read_excel(fn_Pegeln)\n\n# Display the DataFrame\nprint(tb_Pegeln)\n\n            Suchergebnisse Pegel.xlsx 14.09.2023 10:01  ...      Unnamed: 15\n0    Suchkriterien:\\n -- \\n\\n273 von 273 Datensätze...  ...              NaN\n1                                                  NaN  ...              NaN\n2                                                  NaN  ...              NaN\n3                                                 Name  ...  Nordwert in UTM\n4                                                Ahlen  ...       5733198,52\n..                                                 ...  ...              ...\n272                                            Wolbeck  ...      5750912,504\n273                                    Wt-Kluserbrücke  ...          5679856\n274                                         Zeppenfeld  ...      5626270,319\n275                                          Zerkall 1  ...      5618750,896\n276                                          Zerkall 2  ...       5617784,98\n\n[277 rows x 16 columns]\n\n\n\n\n\nWhen we provide only the file name to the function, we will always retrieve all the content from the first sheet. However, due to the limitations in R (and Python) tables, every column will be recognized as the same data type, typically character.\n\n\n2.3.2 Give a range\n\nRPython\n\n\n\n# using the range argument\ntb_Pegeln_Range &lt;- read_excel(fn_Pegeln, range = \"Suchergebnisse Pegel!A5:P10\")\ntb_Pegeln_Range\n\n# A tibble: 5 × 16\n  Name            Pegelnummer   Gewässername Betreiber  `Pegelnullpunkt [müNHN]`\n  &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                   \n1 Ahlen           3211000000300 Werse        LANUV, NRW 73,47                   \n2 Ahmsen          4639000000100 Werre        LANUV, NRW 64,28                   \n3 Ahrhütte-Neuhof 2718193000100 Ahr          LANUV, NRW 340,58                  \n4 Albersloh       3259000000100 Werse        LANUV, NRW 48,68                   \n5 Altena          2766930000100 Lenne        LANUV, NRW 154,22                  \n# ℹ 11 more variables: `Einzugsgebiet [km²]` &lt;chr&gt;, `Q von` &lt;chr&gt;,\n#   `Q bis` &lt;chr&gt;, NQ &lt;chr&gt;, MNQ &lt;chr&gt;, MQ &lt;chr&gt;, MHQ &lt;chr&gt;, HQ &lt;chr&gt;,\n#   `Q Einheit` &lt;chr&gt;, `Ostwert in UTM` &lt;chr&gt;, `Nordwert in UTM` &lt;chr&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe data type of “Pegelnullpunkt [müNHN]” appears to be incorrect due to improper settings in Excel.\n\n\n\n\nCompared to R, Python doesn’t have a direct equivalent to the “Range”. Instead, you can achieve a similar method like plain text with skiprows. Additionally, you can use usecols to specify the columns you want to include.\n\n# Read the specified range from the Excel file into a pandas DataFrame\ntb_Pegeln_Range = pd.read_excel(fn_Pegeln, sheet_name=\"Suchergebnisse Pegel\", skiprows = 4, usecols=\"A:P\")\n\n# Display the DataFrame\nprint(tb_Pegeln_Range)\n\n                Name    Pegelnummer  ... Ostwert in UTM Nordwert in UTM\n0              Ahlen  3211000000300  ...      425366,05      5733198,52\n1             Ahmsen  4639000000100  ...     479549,678     5771201,838\n2    Ahrhütte-Neuhof  2718193000100  ...     339937,139     5583651,051\n3          Albersloh  3259000000100  ...     412463,351     5748891,345\n4             Altena  2766930000100  ...     407683,712     5682846,836\n..               ...            ...  ...            ...             ...\n268          Wolbeck  3289100000100  ...     416214,865     5750912,504\n269  Wt-Kluserbrücke  2736510000100  ...         371494         5679856\n270       Zeppenfeld  2722590000100  ...     430354,152     5626270,319\n271        Zerkall 1  2823500000100  ...     320063,421     5618750,896\n272        Zerkall 2  2823490000100  ...     319449,788      5617784,98\n\n[273 rows x 16 columns]",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#data-type",
    "href": "dataprocess/data_load.html#data-type",
    "title": "Data Loading",
    "section": "2.4 Data type",
    "text": "2.4 Data type\nCompared to plain text files, Excel data already contains data type information for each cell. Therefore, the data type will be directly determined by the data type specified in Excel.\nHowever, there are instances where the data type in Excel is not correctly set, so manual data type conversion may be necessary. For more details, refer to Section 1.5.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html",
    "href": "dataprocess/basic_format.html",
    "title": "Basic Data & File Format",
    "section": "",
    "text": "In the realm of hydrological modeling, various data structures and file formats are employed to effectively manage, analyze, and simulate hydrological processes. These structures and formats facilitate the representation of hydrological data, making it accessible for researchers, modelers, and decision-makers. Below are some of the common data structures and file formats used in hydrological modeling, along with their key features.\nOverview:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#array",
    "href": "dataprocess/basic_format.html#array",
    "title": "Basic Data & File Format",
    "section": "Array",
    "text": "Array\nArrays are collections of elements, typically of the SAME data type, organized in a linear or multi-dimensional fashion. They provide efficient data storage and manipulation, making them essential for numerical computations.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#table-dataframe",
    "href": "dataprocess/basic_format.html#table-dataframe",
    "title": "Basic Data & File Format",
    "section": "Table (Dataframe)",
    "text": "Table (Dataframe)\nTabular data structures, often referred to as tables, are a fundamental way of organizing and representing data in a structured format. They consist of rows and columns, where each row typically represents a single observation or record, and each column represents a specific attribute or variable associated with those observations. Tabular structures are highly versatile and are widely used for storing and analyzing various types of data, ranging from simple lists to complex datasets. This characteristic of tables enables them to represent and manage a wide range of information efficiently.\nCompare to array, in a table, columns are allowed to have different data types, but all values within a specific column must share the same data type.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#sec-spatialData",
    "href": "dataprocess/basic_format.html#sec-spatialData",
    "title": "Basic Data & File Format",
    "section": "Spatial Data",
    "text": "Spatial Data\nMore Details in Spatial Data Science\nSpatial data refers to data that has a geographic or spatial component, representing the locations and shapes of physical objects on the Earth’s surface. This type of data is essential in various fields, including geography, environmental science, urban planning, and more. One of the key elements in spatial data is its association with coordinate systems, which allow precise location referencing.\n\nSpatial Vector\nSpatial vector data structures represent geometric shapes like points, lines, and polygons in space. They are widely used in geographic information systems (GIS) for mapping and analyzing spatial data, such as landuse boundaries or river networks.\n\nThe term “Vector” is used because spatial vector data is essentially stored as a vector of points, lines, or polygons (which are composed of lines). The data structure for geographic shapes is divided into two key components:\n\nGeometry: Geometry represents the spatial shape or location of the geographic feature. It defines the boundaries, points, lines, or polygons that make up the feature. These geometric elements are used to precisely describe the geometric feature.\nAttributes: Attributes are associated with the geographic feature and provide additional information about it. These attributes can include data such as the feature’s name, population, temperature, or any other relevant details. Attributes are typically organized and stored in a tabular format, making it easy to perform data analysis and visualization.\n\nThe data structure of points in geospatial data is relatively simple. The geometry of one point is described by its coordinates, typically represented as X (or longitude) and Y (or latitude) values.\n\nOn the other hand, lines and polygons are more complex geometric shapes. The geometry of a line or polygon is defined by a sequence of multiple points. These points are connected in a specific order to form the shape of the line or polygon. In other words, the geometry of every line (or polygon) is composed of a series of coordinates of points.\n\n\n\nSpatial Raster\nSpatial raster data structures are grid-based representations of spatial data, where each cell holds a value. They are commonly used for storing continuous data, like satellite imagery or elevation models.\nThe datastructure of raster data is quite simple. In a raster, each row shares the same X value, and each column shares the same Y value. Additionally, in most situations, the resolution in each dimension remains constant. This means that specifying the starting point and the resolutions is usually sufficient to describe the coordinates of every grid cell. A single raster layer indeed resembles a 2D matrix.\n\n\n\nCoordinate Reference System (CRS)\nIn addition to Geometry (of Vector) and Koordinate (of aster), another essential component of spatial data is the Coordinate Reference System (CRS). The CRS plays a crucial role in geospatial data by providing a framework for translating the Earth’s 3D surface into a 2D coordinate system.\nKey points about the Coordinate Reference System (CRS) include:\n\nAngular coordinates: The earth has an irregular spheroid-like shape. The natural coordinate reference system for geographic data is longitude/latitude.\nProjection: The CRS defines how the Earth’s curved surface is projected onto a 2D plane, enabling the representation of geographic features on maps and in geographic information systems (GIS). Different projection methods exist, each with its own strengths and weaknesses depending on the region and purpose of the map.\nUnits: CRS specifies the units of measurement for coordinates. Common units include degrees (for latitude and longitude), meters, and feet, among others.\nReference Point: It establishes a reference point (usually the origin) and orientation for the coordinate system.\nEPSG Code: Many CRS are identified by an EPSG (European Petroleum Survey Group) code, which is a unique numeric identifier that facilitates data sharing and standardization across GIS systems.\n\nThe CRS is fundamental for correctly interpreting and analyzing spatial data, as it ensures that geographic features are accurately represented in maps and GIS applications. Different CRSs are used for different regions and applications to minimize distortion and provide precise geospatial information.\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. These codes consist of a string of numbers that uniquely identify a specific CRS. By using EPSG codes, you can easily access comprehensive definitions of different CRSs, which include details about their coordinate systems, datums, projections, and other parameters. Many software applications and libraries support EPSG codes, making it a standardized and convenient way to specify CRS information in spatial data.\nYou can obtain information about EPSG codes from the EPSG website. This website serves as a valuable resource for accessing detailed information associated with EPSG codes, including coordinate reference system (CRS) definitions and specifications.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#time-series",
    "href": "dataprocess/basic_format.html#time-series",
    "title": "Basic Data & File Format",
    "section": "Time Series",
    "text": "Time Series\nTime series data structures are specifically designed to capture and represent information recorded over a period of time. They play a crucial role in analyzing trends, patterns, and dependencies within sequences of data. Time series data, by definition, have a temporal dimension, making time an essential component of these structures.\nIn comparison to spatial information, time information is relatively straightforward. When the time dimension progresses in uniform steps, it can be efficiently described using the start time and step intervals. However, when the time intervals are irregular or non-uniform, additional time-related details are necessary. This can include specifying the year, month, and day for date-based time data or the hour, minute, and second for time-based information.\nIt’s worth noting that while most time series data adheres to the standard calendar system, some datasets may use alternative calendar systems such as the Julian calendar. Additionally, time zone information is crucial when working with time data, as it ensures accurate temporal references across different geographical regions.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#plain-text-ascii",
    "href": "dataprocess/basic_format.html#plain-text-ascii",
    "title": "Basic Data & File Format",
    "section": "Plain text (ASCII)",
    "text": "Plain text (ASCII)\nASCII (American Standard Code for Information Interchange) is a plain text format, making it human-readable.\n\nAdvantages\n\nHuman-Readable: Users can easily view, understand, and edit the data directly in a text editor.\nWidespread Support, Ease of Import/Export: ASCII is universally supported. Most programming languages, data analysis tools, and software applications can read and write ASCII files, ensuring high compatibility.\nLightweight: ASCII files are typically lightweight and do not consume excessive storage space, making them suitable for large datasets.\nSimple Structure: ASCII files have a straightforward structure, often using lines of text with fields separated by delimiters. This simplicity aids in data extraction and manipulation.\n\n\n\nDisadvantages\n\nLimited Data Types: ASCII primarily handles text-based data and is not suitable for complex data types such as images, multimedia, or hierarchical data.\nNo Inherent Data Validation: ASCII files lack built-in mechanisms for data validation or integrity checks, requiring users to ensure data conformity.\nLack of Compression: ASCII files do not inherently support data compression, potentially resulting in larger file sizes compared to binary formats.\nSlower Reading/Writing: Reading and writing data in ASCII format may be slower, especially for large datasets, due to additional parsing required to interpret text-based data.\n\n\n\nFile format for ASCII data\nWhen it comes to plain text formats, there is no universal standard, and it’s highly adaptable to specific needs. The initial step in loading a plain text table is to analyze the structure of the file.\nTypically, a text table can store 2D data, comprising columns and rows or a matrix. However, above the data body, there’s often metadata that describes the data. Metadata can vary widely between data body.\nDividing rows is usually straightforward and can be achieved by identifying row-end characters. However, dividing columns within each row presents multiple possibilities, such as spaces, tabs, commas, or semicolons.\n\n.txt: This is the most generic and widely used file extension for plain text files. It doesn’t imply any specific format or structure; it’s just a simple text file.\n.csv (Comma-Separated Values): While CSV files contain data separated by commas, they are still considered ASCII files because they use plain text characters to represent data values. Each line in a CSV file typically represents a record, with values separated by commas.\n\nIn .txt files, any of these separators can be used, but in .csv files, commas or semicolons are commonly employed as separator characters.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#excel-files",
    "href": "dataprocess/basic_format.html#excel-files",
    "title": "Basic Data & File Format",
    "section": "Excel Files",
    "text": "Excel Files\nExcel files, often denoted with the extensions .xls or .xlsx, are a common file format used for storing structured data in tabular form. These files are not to be confused with the Microsoft Excel software itself but are the data containers created and manipulated using spreadsheet software like Excel.\nExcel files are widely used in various applications, including data storage, analysis, reporting, and sharing. They consist of rows and columns, where each cell can contain text, numbers, formulas, or dates. These files are versatile and can hold different types of data, making them a popular choice for managing information.\n\nAdvantages:\n\nUser-Friendly Interface: Excel’s user-friendly interface makes it accessible to users with varying levels of expertise. Its familiar grid layout simplifies data input and manipulation.\nVersatility: Excel can handle various types of data, from simple lists to complex calculations.\nFormulas and Functions: Excel provides an extensive library of built-in formulas and functions, allowing users to automate calculations and streamline data processing.\nData Visualization: Creating charts and graphs in Excel is straightforward. It helps in visualizing data trends and patterns, making complex information more accessible.\nData Validation: Excel allows you to set rules and validation criteria for data entry, reducing errors and ensuring data accuracy.\n\n\n\nDisadvantages:\n\nLimited Data Handling: Excel has limitations in handling very large datasets. Performance may degrade, and it’s not suitable for big data analytics.\nLack of Version Control: Excel lacks robust version control features, making it challenging to track changes and manage document versions in collaborative environments.\n\nIn conclusion, Excel is a valuable tool for various data-related tasks but comes with limitations in terms of scalability, data integrity, and security. Careful consideration of its strengths and weaknesses is essential when deciding whether it’s the right choice for your data management needs.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#binary",
    "href": "dataprocess/basic_format.html#binary",
    "title": "Basic Data & File Format",
    "section": "Binary",
    "text": "Binary\nUnlike text-based files, binary files store data in a way that is optimized for computer processing and can represent a wide range of data types, from simple numbers to complex structures. These files are used in various applications, including programming, scientific research, and data storage, due to their efficiency in handling data.\n\nAdvantages of Binary Formats\n\nEfficiency: Binary formats are highly efficient for data storage and transmission because they represent data in a compact binary form. This can significantly reduce storage space and data transfer times, making them ideal for large datasets.\nData Integrity: Binary formats often include built-in mechanisms for data integrity and error checking. This helps ensure that data remains intact and accurate during storage and transmission.\nComplex Data: Binary formats can represent complex data structures, which makes them suitable for a wide range of data types.\nFaster I/O: Reading and writing data in binary format is generally faster than text-based formats like ASCII. This efficiency is particularly important for applications that require high-speed data processing.\nSecurity: Binary formats can provide a level of data security because they are not easily human-readable. This can be advantageous when dealing with sensitive information.\n\n\n\nDisadvantages of Binary Formats\n\nLack of Human-Readability: Binary formats are not human-readable, making it difficult to view or edit the data directly. This can be a disadvantage when data inspection or manual editing is required.\nCompatibility: Binary formats may not be universally compatible across different software platforms and programming languages. This can lead to issues when sharing or accessing data in various environments.\nLimited Metadata: Binary formats may not include comprehensive metadata structures, making it challenging to document and describe the data effectively.\nVersion Compatibility: Changes in the binary format’s structure or encoding can lead to compatibility issues when working with data created using different versions of software or hardware.\nPlatform Dependence: Binary formats can be platform-dependent, meaning they may not be easily transferable between different operating systems or hardware architectures.\n\nBinary formats are a valuable choice for certain applications, particularly when efficiency, data integrity, and complex data types are crucial. However, they may not be suitable for all scenarios, especially when human readability, compatibility, or ease of data inspection is essential.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#nectcdf",
    "href": "dataprocess/basic_format.html#nectcdf",
    "title": "Basic Data & File Format",
    "section": "NectCDF",
    "text": "NectCDF\n\nNetCDF (Network Common Data Form) is a versatile data format widely used in scientific and environmental applications. It is primarily a binary data format, but it includes structured elements for efficient data storage and management. Here are some key characteristics of NetCDF:\n\nBinary Representation: NetCDF data files are primarily stored in binary format, which enables efficient storage and handling of numerical data, particularly floating-point numbers.\nSelf-Describing: NetCDF files are self-describing, meaning they include metadata alongside the data. This metadata provides essential information about the data’s structure, dimensions, units, and other attributes.\nHierarchical Structure: NetCDF supports a hierarchical structure capable of representing complex data types, including multi-dimensional arrays and groups of data variables.\nData Compression: NetCDF allows for data compression, which can reduce the storage space required for large datasets while maintaining data integrity.\nLanguage Support: NetCDF libraries and tools are available for multiple programming languages, making it accessible to a wide range of scientific and data analysis applications.\n\nNetCDF’s combination of binary efficiency and structured metadata makes it an invaluable choice for storing and sharing scientific data, particularly in fields such as meteorology, oceanography, and environmental science.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#database-systems",
    "href": "dataprocess/basic_format.html#database-systems",
    "title": "Basic Data & File Format",
    "section": "Database Systems",
    "text": "Database Systems\nDatabase Systems, such as SQL and NoSQL databases, are crucial for efficiently managing and querying large, structured datasets. They provide structured data storage, ensuring data integrity and consistency. SQL databases like MySQL and PostgreSQL are well-suited for relational data, while NoSQL databases like MongoDB excel in handling semi-structured or unstructured data. These systems are commonly used for storing long-term observational data, model outputs, and sensor data in scientific research and various enterprise applications.\n\nAdvantages\n\nEfficient Data Retrieval: Databases are optimized for querying and retrieving data, making it quick and efficient to access information.\nData Integrity: Databases enforce data integrity rules, ensuring that data remains consistent and reliable over time.\nStructured Storage: They provide a structured way to store data, making it easier to organize and manage large datasets.\nConcurrent Access: Multiple users or applications can access the database simultaneously, enabling collaboration and scalability.\nSecurity: Database systems offer security features like user authentication and authorization to protect sensitive data.\nBackup and Recovery: They often include mechanisms for automated data backup and recovery, reducing the risk of data loss.\n\n\n\nDisadvantages\n\nComplexity: Setting up and maintaining a database can be complex and requires specialized knowledge.\nCost: Licensing, hardware, and maintenance costs can be significant, especially for enterprise-grade database systems.\nScalability Challenges: Some database systems may face scalability limitations as data volume grows.\nLearning Curve: Users and administrators need to learn query languages (e.g., SQL) and database management tools.\nOverhead: Databases can introduce overhead due to indexing, data normalization, and transaction management.\nVendor Lock-In: Depending on the chosen database system, there may be vendor lock-in, making it challenging to switch to another system.\nResource Intensive: Databases consume computing resources, such as CPU and RAM, which can affect system performance.\n\nThe choice of using a database system depends on specific requirements, such as data volume, complexity, security, and scalability needs. It’s essential to carefully evaluate the advantages and disadvantages in the context of your project.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#spatial-data-file-formats",
    "href": "dataprocess/basic_format.html#spatial-data-file-formats",
    "title": "Basic Data & File Format",
    "section": "Spatial Data File Formats",
    "text": "Spatial Data File Formats\nSpatial data files are a specialized type of data format designed for storing geographic or location-based information. Unlike standard data files that store text, numbers, or other types of data, spatial data files are tailored for representing the geographical features of our world.\nSpatial data comes in various file formats, each tailored for specific types of geographic data and applications. Here are some commonly used formats and their key differences:\n\nRaster Data Formats\nMore Raster file-formats in GDAL\n\nTIFF (Tagged Image File Format):\n\nA widely used raster format for storing high-quality images and raster datasets.\nSupports georeferencing and metadata, making it suitable for spatial applications.\n\nASC (Arc/Info ASCII Grid):\n\nA plain text format used to represent raster data in a grid format.\nContains elevation or other continuous data with rows and columns of values.\n\nJPEG (Joint Photographic Experts Group), PNG (Portable Network Graphics):\n\nCommonly used for photographs and images, but not ideal for spatial analysis due to lossy compression.\n\n\n\n\nVector Data Formats\nMore Vector file-formats in GDAL\n\nShapefile (SHP):\n\nOne of the most common vector formats used in GIS applications.\nConsists of multiple files (.shp, .shx, .dbf, etc.) to store point, line, or polygon geometries and associated attributes.\n\n\n\n\n\nFile extension\nContent\n\n\n\n\n.dbf\nAttribute information\n\n\n.shp\nFeature geometry\n\n\n.shx\nFeature geometry index\n\n\n.aih\nAttribute index\n\n\n.ain\nAttribute index\n\n\n.prj\nCoordinate system information\n\n\n.sbn\nSpatial index file\n\n\n.sbx\nSpatial index file\n\n\n\n\nGeoPackage (GPKG):\n\nAn open, standards-based platform-independent format for spatial data.\nCan store multiple layers, attributes, and geometries in a single file.\n\nKML (Keyhole Markup Language):\n\nXML-based format used for geographic visualization in Earth browsers like Google Earth.\nSuitable for storing points, lines, polygons, and related attributes.\n\nGeoJSON:\n\nA lightweight format for encoding geographic data structures using JSON (JavaScript Object Notation).\nIdeal for web applications due to its simplicity and ease of use.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html",
    "href": "dataprocess/basic_r_python.html",
    "title": "R & Python Basic",
    "section": "",
    "text": "More Details of R in R for Data Science (2e) and Advanced R\nMore Details of Python in Automate the Boring Stuff with Python and W3 School Python\nThis article serves as a brief introduction to the fundamental coding aspects of both R and Python. It provides a first impression of these scripting languages. For a more comprehensive understanding and in-depth techniques related to both languages, you are encouraged to explore the website mentioned above. The content here is primarily a condensed compilation of information from the provided links, aimed at facilitating a comparison between R and Python.\nData and Functions are the two essential components of every programming language, especially in the context of data science and data processing. They can be likened to nouns and verbs in natural languages. Data describes information, while Functions define actions for manipulating that data.\nThis article is divided into two main sections: Data (Section 1) and Coding (Section 2).\nIn the Data section, we will explore:\nIn the Coding section, we will delve into three key aspects:\nThe above five elements can be considered as the most fundamental elements of every scripting language. Additionally, we will explore object creation and naming in a section called ‘New Objects’ (Section 3). Objects can encompass functions and variables, further enriching our understanding of scripting.\nThis article will provide a solid introduction to the core concepts in programming, laying the groundwork for further exploration in both R and Python.\nOverview:",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#datatypes-structure",
    "href": "dataprocess/basic_r_python.html#datatypes-structure",
    "title": "R & Python Basic",
    "section": "1.1 Datatypes & Structure",
    "text": "1.1 Datatypes & Structure\nIn programming, the concept of datatypes is fundamental. It forms the basis for how we handle and manipulate information in software. The most basic data types, such as integers, numerics, booleans, characters, and bytes, are supported by almost all programming languages. Additionally, there are more complex data types built upon these basics, like strings, which are sequences of characters, and dates, which can be represented as variables of integers and more.\nData structures are equally important, as they determine the organization of data, whether it involves the same data types in multiple dimensions or combinations of different types. Data types and structures are intertwined, serving as the cornerstone for our programming endeavors.\nVariables play a pivotal role in storing data of different types. The choice of data type and structure is critical, as different types and structures enable various operations and functionalities. Therefore, understanding data types and structures is paramount before embarking on data manipulation tasks.\n\n1.1.1 Datatypes\nA data type of a variable specifies the type of data that is stored inside that variable. In this context, we will just discuss Atomic Variables, which represent fundamental data types. There are six basic atomic data types:\n\nLogical (boolean data type)\n\ncan only have two values: TRUE and FALSE\n\nNumeric (double, float, lang)\n\nrepresents all real numbers with or without decimal values.\n\nInteger\n\nspecifies real values without decimal points.\n\nComplex\n\nis used to specify purely imaginary values\n\nCharacter (string)\n\ndata type is used to specify character or string values in a variable\n\nRaw (bytes)\n\nspecifies values as raw bytes\n\n\n\nRPython\n\n\nIn R, variables do not require explicit declaration with a particular data type. Instead, R is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in R:\n\nChecking Data Types: To determine the data type of a variable, you can use the class() function.\nType Conversion: When needed, you can change the data type of a variable using R’s conversion functions, typically prefixed with as..\n\nR’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\nx &lt;- 1000L\nclass(x)\n\n[1] \"integer\"\n\n# Complex\nx &lt;- 9i + 3\nclass(x)\n\n[1] \"complex\"\n\n# Character/String\nx &lt;- \"R is exciting\"\nclass(x)\n\n[1] \"character\"\n\n# Logical/Boolean\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\n# Convert\ny &lt;- as.numeric(x)\nclass(y)\n\n[1] \"numeric\"\n\n# Raw (bytes)\nx &lt;- charToRaw(\"A\")\nx\n\n[1] 41\n\nclass(x)\n\n[1] \"raw\"\n\n\n\n\nIn Python, variables also do not require explicit declaration with a particular data type. Python is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in Python:\n\nChecking Data Types: To determine the data type of a variable, you can use the type() function. It allows you to inspect the current data type of a variable.\nType Conversion: When needed, you can change the data type of a variable in Python using various conversion functions, like float().\n\nPython’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx = 10.5\nprint(type(x))\n\n&lt;class 'float'&gt;\n\n# Integer\nx = 1000\nprint(type(x))\n\n&lt;class 'int'&gt;\n\n# Complex\nx = 9j + 3\nprint(type(x))\n\n&lt;class 'complex'&gt;\n\n# Character/String\nx = \"Python is exciting\"\nprint(type(x))\n\n&lt;class 'str'&gt;\n\n# Logical/Boolean\nx = True\nprint(type(x))\n\n&lt;class 'bool'&gt;\n\n# Convert to Numeric\ny = float(x)\nprint(type(y))\n\n&lt;class 'float'&gt;\n\n# Raw (bytes)\nx = b'A'\nprint(x)\n\nb'A'\n\nprint(type(x))\n\n&lt;class 'bytes'&gt;\n\n\n\n\n\n\n\n1.1.2 Data Structure\nComparatively, data structures between R and Python tend to exhibit more differences than their data types. However, by incorporating additional libraries like NumPy and pandas, we can access shared data structures which play a vital role in the field of data science.\n\nVector: A set of multiple values (items)\n\nContains items of the same data type or structure\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nArray: A multi-dimensional extension of a vector\n\nMatrix: two dimensions\n\nList: A set of multiple values (items)\n\nContains items of different data types or structures\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nTable (Data Frame): Tabular data structure\n\nTwo-dimensional objects with rows and columns\nContains elements of several types\nEach column has the same data type\n\n\n\nRPython\n\n\nThe structure of R variable can be checked with str()ucture:\n\n# Create a vector\nvct_Test &lt;- c(1,5,7)\n# View the structure\nstr(vct_Test)\n\n num [1:3] 1 5 7\n\n# Create a array\nary_Test &lt;- array(1:24, c(2,3,4))\n# View the structure\nstr(ary_Test)\n\n int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a matrix\nmat_Test &lt;- matrix(1:24, 6, 4)\nmat_Test\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    2    8   14   20\n[3,]    3    9   15   21\n[4,]    4   10   16   22\n[5,]    5   11   17   23\n[6,]    6   12   18   24\n\n# View the structure\nstr(mat_Test)\n\n int [1:6, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a list\nlst_Test &lt;- list(c(1,3,5), \"abc\", FALSE)\n# View the structure\nstr(lst_Test)\n\nList of 3\n $ : num [1:3] 1 3 5\n $ : chr \"abc\"\n $ : logi FALSE\n\n# Create a table (data frame)\ndf_Test &lt;- data.frame(name = c(\"Bob\", \"Tom\"), age = c(12, 13))\ndf_Test\n\n  name age\n1  Bob  12\n2  Tom  13\n\n# View the structure\nstr(df_Test)\n\n'data.frame':   2 obs. of  2 variables:\n $ name: chr  \"Bob\" \"Tom\"\n $ age : num  12 13\n\n\n\n\nIn Python, the structure of a variable is treated as the data type, and you can confirm it using the type() function.\nIt’s important to note that some of the most commonly used data structures, such as arrays and data frames (tables), are not part of the core Python language itself. Instead, they are provided by two popular libraries: numpy and pandas.\n\nimport numpy as np\nimport pandas as pd\n\n# Create a vector (list in Python)\nvct_Test = [1, 5, 7]\n# View the structure\nprint(type(vct_Test))\n\n&lt;class 'list'&gt;\n\n# Create a 3D array (NumPy ndarray)\nary_Test = np.arange(1, 25).reshape((2, 3, 4))\n# View the structure\nprint(type(ary_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a matrix (NumPy ndarray)\nmat_Test = np.arange(1, 25).reshape((6, 4))\nprint(type(mat_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a list\nlst_Test = [[1, 3, 5], \"abc\", False]\n# View the structure\nprint(type(lst_Test))\n\n&lt;class 'list'&gt;\n\n# Create a table (pandas DataFrame)\ndf_Test = pd.DataFrame({\"name\": [\"Bob\", \"Tom\"], \"age\": [12, 13]})\nprint(type(df_Test))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nprint(df_Test)\n\n  name  age\n0  Bob   12\n1  Tom   13\n\n\nPython offers several original data structures, including:\n\nTuples: Tuples are ordered collections of elements, similar to lists, but unlike lists, they are immutable, meaning their elements cannot be changed after creation. Tuples are often used to represent fixed collections of items.\nSets: Sets are unordered collections of unique elements. They are valuable for operations that require uniqueness, such as finding unique values in a dataset or performing set-based operations like unions and intersections.\nDictionaries: Dictionaries, also known as dicts, are collections of key-value pairs. They are used to store data in a structured and efficient manner, allowing quick access to values using their associated keys.\n\nWhile these data structures may not be as commonly used in data manipulation and calculations as arrays and data frames, they have unique features and use cases that can be valuable in various programming scenarios.",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#index-subset",
    "href": "dataprocess/basic_r_python.html#index-subset",
    "title": "R & Python Basic",
    "section": "1.2 Index & subset",
    "text": "1.2 Index & subset\nAdditionally, subsetting plays a crucial role in data manipulation. Subsetting allows you to extract specific subsets of data based on conditions, criteria, or filters.\n\nRPython\n\n\nMore Details in Advanced R: 4 Subsetting.\nR’s subsetting operators are fast and powerful. Mastering them allows you to succinctly perform complex operations in a way that few other languages can match. Subsetting in R is easy to learn but hard to master because you need to internalise a number of interrelated concepts:\n\nThere are six ways to subset atomic vectors.\nThere are three subsetting operators, [[, [, and $.\nSubsetting operators interact differently with different vector types (e.g., atomic vectors, lists, factors, matrices, and data frames).\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn Python, indexing starts from 0, not 1.\n\n\n\n\n\n\n1.2.1 Vector\n\nRPython\n\n\n\nPositive integers return elements at the specified positions:\n\n\nx &lt;- c(2.1, 4.2, 3.3, 5.4)\n\n# One value\nx[1]\n\n[1] 2.1\n\n# More values\nx[c(1:2, 4)]\n\n[1] 2.1 4.2 5.4\n\n# Duplicate indices will duplicate values\nx[c(1, 1)]\n\n[1] 2.1 2.1\n\n# Real numbers are silently truncated to integers\nx[c(2.1, 2.9)]\n\n[1] 4.2 4.2\n\n\n\nNegative integers exclude elements at the specified positions:\n\n\n# Exclude elements\nx[-c(3, 1)]\n\n[1] 4.2 5.4\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nNote that you can’t mix positive and negative integers in a single subset:\n\n\n\nx[c(-1, 2)]\n\nError in x[c(-1, 2)]: nur Nullen dürfen mit negativen Indizes gemischt werden\n\n\n\n\n\nPositive integers return elements at the specified positions:\n\n\nimport numpy as np\nimport pandas as pd\n\n# Create a NumPy array\nx = np.array([2.1, 4.2, 3.3, 5.4])\n\n# One value\nprint(x[0])\n\n2.1\n\n# More values\nprint(x[np.array([0, 1, 3])])\n\n[2.1 4.2 5.4]\n\n# Duplicate indices will duplicate values\nprint(x[np.array([0, 0])])\n\n[2.1 2.1]\n\n\n\negative indexing to access an array from the end:\n\n\n# One value\nprint(x[-1])\n\n5.4\n\n# More values\nprint(x[-np.array([1, 3])])\n\n[5.4 4.2]\n\n\n\n\n\n\n\n1.2.2 Matrices and arrays\n\nRPython\n\n\nThe most common way of subsetting matrices (2D) and arrays (&gt;2D) is a simple generalisation of 1D subsetting: supply a 1D index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns.\n\n# Create a matrix\na2 &lt;- matrix(1:9, nrow = 3)\n# Rename the columns (equivalent to colnames in R)\ncolnames(a2) &lt;- c(\"A\", \"B\", \"C\")\n# Access a specific element using column name\na2[1, \"A\"]\n\nA \n1 \n\n# Select specific rows with all columns\na2[1:2, ]\n\n     A B C\n[1,] 1 4 7\n[2,] 2 5 8\n\n# columns which are excluded \na2[0, -2]\n\n     A C\n\n# Create a 3D array\na3 &lt;- array(1:24, c(2,3,4))\n# Access a specific element(s), in different dimensions\na3[1,2,2]\n\n[1] 9\n\na3[1,2,]\n\n[1]  3  9 15 21\n\na3[1,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    3    9   15   21\n[3,]    5   11   17   23\n\n\n\n\nIn Python, the : symbol is used to indicate all elements of a particular dimension or slice. It allows you to select or reference all items along that dimension in a sequence, array, or data structure.\n\nimport numpy as np\n\n# Create a NumPy matrix\na2 = np.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\n# Rename the columns (equivalent to colnames in R)\ncolnames = [\"A\", \"B\", \"C\"]\n\n# Access a specific element using column name\nprint(a2[0, colnames.index(\"A\")])\n\n1\n\n# Select the first two rows\nprint(a2[0:2, :])\n\n[[1 2 3]\n [4 5 6]]\n\n# Create a NumPy 3D array\na3 = np.arange(1, 25).reshape((2, 3, 4))\n\n# Access a specific element in the 3D array\nprint(a3[0, 1, 1])\n\n6\n\nprint(a3[0, 1, :])\n\n[5 6 7 8]\n\nprint(a3[0, :, :])\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\n\n\n\n\n1.2.3 Data frames\n\nRPython\n\n\nData frames have the characteristics of both lists and matrices:\n\nWhen subsetting with a single index, they behave like lists and index the columns, so df[1:2] selects the first two columns.\nWhen subsetting with two indices, they behave like matrices, so df[1:3, ] selects the first three rows (and all the columns)[^python-dims].\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Select rows\ndf[df$x == 2, ]\n\n  x y z\n2 2 2 b\n\ndf[c(1, 3), ]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# There are two ways to select columns from a data frame\n# Like a list\ndf[c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# Like a matrix\ndf[, c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# There's an important difference if you select a single \n# column: matrix subsetting simplifies by default, list \n# subsetting does not.\nstr(df[\"x\"])\n\n'data.frame':   3 obs. of  1 variable:\n $ x: int  1 2 3\n\nstr(df[, \"x\"])\n\n int [1:3] 1 2 3\n\n\n\n\nMore detail about Function pandas.Seies.iloc() and pandas.Seies.loc() in pandas document\n\nloc gets rows (and/or columns) with particular labels.\niloc gets rows (and/or columns) at integer locations.\n\n\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Select rows\nprint(df[df['x'] == 2])\n\n   x  y  z\n1  2  2  b\n\nprint(df.iloc[[0, 2]])\n\n   x  y  z\n0  1  3  a\n2  3  1  c\n\n# Select columns\nprint(df[['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select columns like a DataFrame\nprint(df.loc[:, ['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select a single column as a Series (simplifies by default)\nprint(df['x'])\n\n0    1\n1    2\n2    3\nName: x, dtype: int64\n\n# Select a single column as a DataFrame (does not simplify)\nprint(df[['x']])\n\n   x\n0  1\n1  2\n2  3\n\n\n\n\n\n\n\n1.2.4 List\n\nRPython\n\n\nThere are two other subsetting operators: [[ and $. [[ is used for extracting single items, while x$y is a useful shorthand for x[[\"y\"]].\n[[ is most important when working with lists because subsetting a list with [ always returns a smaller list. To help make this easier to understand we can use a metaphor:\n[[ can return only a single item, you must use it with either a single positive integer or a single string.\n\nx &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n\n# Get the subset \nx[1]\n\n$a\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\nx[1:2]\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n# Get the element\nx[[1]]\n\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\n# with Label\nx$a\n\n[1] 1 2 3\n\nx[[\"a\"]]\n\n[1] 1 2 3\n\n\n\n\nIn Python there are no effectiv ways to create a items named list. It can always get the element of the list but not a subset of the list.\nIn Python, there are no effective ways to create items with named elements in a list. While you can access individual elements by their positions, there isn’t a straightforward method to create a subset of the list with named elements.\n\n# Create a Python list with nested lists\nx = [list(range(1, 4)), \"a\", list(range(4, 7))]\n\n# Get the subset (Python list slice)\nprint([x[0]])\n\n[[1, 2, 3]]\n\n# Get the element using list indexing\nprint(x[0])\n\n[1, 2, 3]\n\nprint(type(x[0]))\n\n&lt;class 'list'&gt;\n\n\nHowever, dictionaries in Python excel in this regard, as they allow you to assign and access elements using user-defined keys, providing a more efficient way to work with named elements and subsets of data.\n\n# Create a dictionary with labels\nx = {\"a\": list(range(1, 4)), \"b\": \"a\", \"d\": list(range(4, 7))}\n\n\n# Get the element using dictionary indexing\nprint(x[\"a\"])\n\n[1, 2, 3]\n\n# Access an element with a label\nprint(x[\"a\"])\n\n[1, 2, 3]\n\nprint(x.get(\"a\"))\n\n[1, 2, 3]\n\nprint(type(x[\"a\"]))\n\n&lt;class 'list'&gt;",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#data-crud",
    "href": "dataprocess/basic_r_python.html#data-crud",
    "title": "R & Python Basic",
    "section": "1.3 Data CRUD",
    "text": "1.3 Data CRUD\nData manipulation is the art and science of transforming raw data into a more structured and useful format for analysis, interpretation, and decision-making. It’s a fundamental process in data science, analytics, and database management.\nOperations for creating and managing persistent data elements can be summarized as CRUD:\n\nCreate (Add): The creation of new data elements or records.\nRead: The retrieval and access of existing data elements for analysis or presentation.\nUpdate: The modification or editing of data elements to reflect changes or corrections.\nDelete: The removal or elimination of data elements that are no longer needed or relevant.\n\nCombining CRUD operations with subsetting provides a powerful toolkit for working with data, ensuring its accuracy, relevance, and utility in various applications, from database management to data analysis.\n\n1.3.1 Create & Add\nMost of the original data we work with is often loaded from external data sources or files. This process will be discussed in detail in the article titled Data Load.\nIn this section, we will focus on the fundamental aspects of creating and adding data, which may have already been mentioned several times in the preceding text.\n\nRPython\n\n\nCreating new objects in R is commonly done using the assignment operator &lt;-.\nWhen it comes to vectors or list, there are two primary methods to append new elements:\n\nc(): allows you to combine the original vector with a new vector or element, effectively extending the vector.\nappend(): enables you to append a new vector or element at a specific location within the original vector.\n\n\n# Automic value\na &lt;- 1 / 200 * 30\n\n# vector\nx_v &lt;- c(2.1, 4.2, 3.3, 5.4)\n# List\nx_l &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n# add new elements\nc(x_v, c(-1,-5.6))\n\n[1]  2.1  4.2  3.3  5.4 -1.0 -5.6\n\nc(x_l, list(e = c(TRUE, FALSE)))\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\n# append after 2. Element\nappend(x_v, c(-1,-5.6), 2)\n\n[1]  2.1  4.2 -1.0 -5.6  3.3  5.4\n\nappend(x_l, list(e = c(TRUE, FALSE)), 2)\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$e\n[1]  TRUE FALSE\n\n$d\n[1] 4 5 6\n\n\nWhen working with 2D matrices or data frames in R, you can use the following functions to add new elements in the row or column dimensions:\n\ncbind(): to combine data frames or matrices by adding new columns.\nrbind(): to combine data frames or matrices by adding new rows.\n\n\n# Create a matrix\nx_m &lt;- matrix(1:9, nrow = 3)\n# data frame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n# append in colum dimension\ncbind(x_m, -1:-3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   -1\n[2,]    2    5    8   -2\n[3,]    3    6    9   -3\n\ncbind(df, k = -1:-3)\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\n# append in row dimension\nrbind(x_m, -1:-3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n[4,]   -1   -2   -3\n\nrbind(df, list(-1, -2, \"z\")) # try with rbind(df, c(-1, -2, \"z\"))\n\n   x  y z\n1  1  3 a\n2  2  2 b\n3  3  1 c\n4 -1 -2 z\n\n\nAdditionally, for both lists and data frames in R, you can use the $ &lt;- operator to add new elements:\n\n# Data frame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ncbind(df, k = -1:-3)\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\ndf$k &lt;- -1:-3 # same to df[['k']] &lt;- -1:-3\ndf\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\n# List\nx_l &lt;- list(a = 1:3, b = \"a\", d = 4:6)\nc(x_l, list(e = c(TRUE, FALSE)))\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\nx_l$e &lt;- c(TRUE, FALSE) # same to x_l[['e']] &lt;- c(TRUE, FALSE)\nx_l\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\n\n\n\nCreating new objects in Python is often accomplished using the assignment operator =. When it comes to adding elements to list, there are three primary functions to consider:\n\nappend(): add a single element to the end of a list.\ninsert(): add an element at a specific position within a list.\nextend() same as +: append elements from an iterable (e.g., another list) to the end of an existing list, allowing for the expansion of the list with multiple elements.\n\n\n# Atomic element\na = 1 / 200 * 30\nb = a + 1\nprint(a)\n\n0.15\n\nprint(b)\n\n1.15\n\n# List\nx = [2.1, 4.2, 3.3, 5.4]\n\n# Append on element\nx.append(-1)\nprint(x)\n\n[2.1, 4.2, 3.3, 5.4, -1]\n\n# Insert on eelement\nx.insert(3, -5.6)\nprint(x)\n\n[2.1, 4.2, 3.3, -5.6, 5.4, -1]\n\n# Extend with new list\nx.extend([6.7, 7.9])\nprint(x)\n\n[2.1, 4.2, 3.3, -5.6, 5.4, -1, 6.7, 7.9]\n\n\nWhen working with numpy.array in Python, you can add elements in two primary ways:\n\nappend(): add element or a new numpy array to the end.\ninsert(): insert element or a new numpy array at specific locations within the original numpy array.\n\n\nimport numpy as np\n\n# Create a NumPy array\nx_a = np.array([2.1, 4.2, 3.3, 5.4])\n\nprint(np.append(x_a, -1))\n\n[ 2.1  4.2  3.3  5.4 -1. ]\n\nprint(np.append(x_a, np.array([6.7, 7.9])))\n\n[2.1 4.2 3.3 5.4 6.7 7.9]\n\nprint(np.insert(x_a, 3, -5.6))\n\n[ 2.1  4.2  3.3 -5.6  5.4]\n\nprint(np.insert(x_a, 3, np.array([6.7, 7.9])))\n\n[2.1 4.2 3.3 6.7 7.9 5.4]\n\n\n\n\n\n\n\n1.3.2 Read\nThe read process is essentially a form of subsetting, where you access specific elements or subsets of data using their indexes. The crucial aspect of this operation is how to obtain and utilize these indexes effectively.\n\nRPython\n\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Access using integer index \ndf[1,2]\n\n[1] 3\n\n# Access using names index\ndf[,\"z\"]\n\n[1] \"a\" \"b\" \"c\"\n\ndf$z\n\n[1] \"a\" \"b\" \"c\"\n\n# Access with a value condition\nidx &lt;- which(df$x &gt; 1)\ndf[idx,]\n\n  x y z\n2 2 2 b\n3 3 1 c\n\ndf[idx, \"z\"]\n\n[1] \"b\" \"c\"\n\nidx &lt;- which(df$z == \"a\")\ndf[idx,]\n\n  x y z\n1 1 3 a\n\ndf[idx, 1:2]\n\n  x y\n1 1 3\n\n\n\n\n\nimport pandas as pd\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Access using integer index (iloc)\nprint(df.iloc[0, 1])\n\n3\n\n# Access using column label\nprint(df['z'])\n\n0    a\n1    b\n2    c\nName: z, dtype: object\n\nprint(df.z)\n\n0    a\n1    b\n2    c\nName: z, dtype: object\n\n# Access with a value condition\nidx = df['x'] &gt; 1\nprint(df[idx])\n\n   x  y  z\n1  2  2  b\n2  3  1  c\n\nprint(df[df['z'] == 'a'])\n\n   x  y  z\n0  1  3  a\n\nprint(df[df['z'] == 'a'][['x', 'y']])\n\n   x  y\n0  1  3\n\n\n\n\n\n\n\n1.3.3 Update\nThe update operation builds upon the principles of reading. It involves replacing an existing value with a new one, but with certain constraints. The new value must have the same data type, size, and structure as the original value. This ensures data consistency and integrity when modifying data elements. About “data type” it is not so strength, somtimes it is chanable if you replace the whol e.g. colums in data frame.\nIt’s important to note that the concept of ‘data type’ isn’t always rigid. There are cases where data types can change, particularly when replacing entire columns in a data frame, for instance. While data types typically define the expected format and behavior of data, specific operations and transformations may lead to changes in data types to accommodate new values or structures.\n\nRPython\n\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf\n\n  x y z\n1 1 3 a\n2 2 2 b\n3 3 1 c\n\n# Update using integer index \ndf[1,2] &lt;- 0\ndf\n\n  x y z\n1 1 0 a\n2 2 2 b\n3 3 1 c\n\n# Update using names index\ndf[2,\"z\"] &lt;- \"lk\"\ndf\n\n  x y  z\n1 1 0  a\n2 2 2 lk\n3 3 1  c\n\n# Update with a value condition\nidx &lt;- which(df$x &gt; 1)\ndf[idx, \"z\"] &lt;- \"bg1\"\ndf\n\n  x y   z\n1 1 0   a\n2 2 2 bg1\n3 3 1 bg1\n\nidx &lt;- which(df$z == \"a\")\ndf[idx,] &lt;- c(-1, -5, \"new_a\")\ndf\n\n   x  y     z\n1 -1 -5 new_a\n2  2  2   bg1\n3  3  1   bg1\n\n\n\n\n\nimport pandas as pd\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\nprint(df)\n\n   x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c\n\n# Update using integer index\ndf.iat[0, 1] = 0\nprint(df)\n\n   x  y  z\n0  1  0  a\n1  2  2  b\n2  3  1  c\n\n# Update using column label and row index\ndf.at[1, 'z'] = \"lk\"\nprint(df)\n\n   x  y   z\n0  1  0   a\n1  2  2  lk\n2  3  1   c\n\n# Update with a value condition\nidx_x_gt_1 = df['x'] &gt; 1\ndf.loc[idx_x_gt_1, 'z'] = \"bg1\"\nprint(df)\n\n   x  y    z\n0  1  0    a\n1  2  2  bg1\n2  3  1  bg1\n\nidx_z_eq_a = df['z'] == 'a'\ndf.loc[idx_z_eq_a] = [-1, -5, \"new_a\"]\nprint(df)\n\n   x  y      z\n0 -1 -5  new_a\n1  2  2    bg1\n2  3  1    bg1\n\n\n\n\n\n\n\n1.3.4 Delete\n\nRPython\n\n\nDeletion in R can be accomplished relatively easily using methods like specifying negative integer indices or setting elements to NULL within a list. However, it’s essential to recognize that there are limitations to deletion operations. For instance, when dealing with multi-dimensional arrays, you cannot delete a single element in the same straightforward manner; instead, you can only delete entire sub-dimensions.\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf\n\n  x y z\n1 1 3 a\n2 2 2 b\n3 3 1 c\n\n# Delete using negative integer index \ndf[,-2]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\ndf[-2,]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# Setting elements to `NULL`\ndf$y &lt;- NULL\ndf\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n\n\n\nIn Python is to use the .drop() command to delete the elemnts in datatframe. More details in pandas document\n\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\nprint(df)\n\n   x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c\n\n# Drop columns\nprint(df.drop(['x', 'z'], axis=1))\n\n   y\n0  3\n1  2\n2  1\n\nprint(df.drop(columns=['x', 'y']))\n\n   z\n0  a\n1  b\n2  c\n\n# Drop a row by index\nprint(df.drop([0, 1]))\n\n   x  y  z\n2  3  1  c",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#math",
    "href": "dataprocess/basic_r_python.html#math",
    "title": "R & Python Basic",
    "section": "2.1 Math",
    "text": "2.1 Math\n\n‘+’ ‘-’ ’*’ ‘/’\nExponent, Logarithm\nTrigonometric functions\nLinear algebra, Matrix multiplication\n\n\nRPython\n\n\n\n1 / 200 * 30\n\n[1] 0.15\n\n(59 + 73 - 2) / 3\n\n[1] 43.33333\n\n3^2\n\n[1] 9\n\nsin(pi / 2) # pi as Const number in R\n\n[1] 1\n\n\n\n\n\nprint(1 / 200 * 30)\n\n0.15\n\nprint((59 + 73 - 2) / 3)\n\n43.333333333333336\n\nprint(3**2)\n\n9\n\nimport math\nprint(math.sin(math.pi/2))\n\n1.0",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#control-flow",
    "href": "dataprocess/basic_r_python.html#control-flow",
    "title": "R & Python Basic",
    "section": "2.2 Control flow",
    "text": "2.2 Control flow\nThere are two primary tools of control flow: choices and loops.\n\nChoices, like if statements calls, allow you to run different code depending on the input.\nLoops, like for and while, allow you to repeatedly run code, typically with changing options.\n\n\n2.2.1 choices\n\n2.2.1.1 Basic If-Else\n\nRPython\n\n\nThe basic form of an if statement in R is as follows:\n\nif (condition) {\n  true_action\n}\nif (condition) {\n  true_action\n} else {\n  false_action\n}\n\nIf condition is TRUE, true_action is evaluated; if condition is FALSE, the optional false_action is evaluated.\nTypically the actions are compound statements contained within {:\nif returns a value so that you can assign the results:\n\na &lt;- 6\nb &lt;- 8\n\nif (b &gt; a) {\n  cat(\"b is greater than a\\n\")\n} else if (a == b) {\n  cat(\"a and b are equal\\n\")\n} else {\n  cat(\"a is greater than b\\n\")\n}\n\nb is greater than a\n\n\n\n\n\n# if statements\nif condition: \n  true_action\n  \n# if-else\nif condition: \n  true_action \nelse: \n  false_action\n\n\n# if-ifel-else\nif condition1: \n  true_action1 \nelif condition2: \n  true_action2 \nelse: \n  false_action\n\n\na = 6\nb = 8\nif b &gt; a:\n  print(\"b is greater than a\")\nelif a == b:\n  print(\"a and b are equal\")\nelse:\n  print(\"a is greater than b\")\n\nb is greater than a\n\n\n\n\n\n\n\n2.2.1.2 switch\n\nRPython\n\n\nClosely related to if is the switch()-statement. It’s a compact, special purpose equivalent that lets you replace code like:\n\nx_option &lt;- function(x) {\n  if (x == \"a\") {\n    \"option 1\"\n  } else if (x == \"b\") {\n    \"option 2\" \n  } else if (x == \"c\") {\n    \"option 3\"\n  } else {\n    stop(\"Invalid `x` value\")\n  }\n}\n\nwith the more succinct:\n\nx_option &lt;- function(x) {\n  switch(x,\n    a = \"option 1\",\n    b = \"option 2\",\n    c = \"option 3\",\n    stop(\"Invalid `x` value\")\n  )\n}\nx_option(\"b\")\n\n[1] \"option 2\"\n\n\nThe last component of a switch() should always throw an error, otherwise unmatched inputs will invisibly return NULL:\n\n\n\nmatch subject:\n    case &lt;pattern_1&gt;:\n        &lt;action_1&gt;\n    case &lt;pattern_2&gt;:\n        &lt;action_2&gt;\n    case &lt;pattern_3&gt;:\n        &lt;action_3&gt;\n    case _:\n        &lt;action_wildcard&gt;\n\n\ndef x_option(x):\n    options = {\n        \"a\": \"option 1\",\n        \"b\": \"option 2\",\n        \"c\": \"option 3\"\n    }\n    return options.get(x, \"Invalid `x` value\")\n\nprint(x_option(\"b\"))\n\noption 2\n\n\n\n\n\n\n\n2.2.1.3 Vectorised if\n\nRPython\n\n\nGiven that if only works with a single TRUE or FALSE, you might wonder what to do if you have a vector of logical values. Handling vectors of values is the job of ifelse(): a vectorised function with test, yes, and no vectors (that will be recycled to the same length):\n\nx &lt;- 1:10\nifelse(x %% 5 == 0, \"XXX\", as.character(x))\n\n [1] \"1\"   \"2\"   \"3\"   \"4\"   \"XXX\" \"6\"   \"7\"   \"8\"   \"9\"   \"XXX\"\n\nifelse(x %% 2 == 0, \"even\", \"odd\")\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nNote that missing values will be propagated into the output.\nI recommend using ifelse() only when the yes and no vectors are the same type as it is otherwise hard to predict the output type. See https://vctrs.r-lib.org/articles/stability.html#ifelse for additional discussion.\n\n\n\n\n\n\n\n\n\n2.2.2 Loops\n\n2.2.2.1 for-Loops\nA for loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string). For each item in vector, perform_action is called once; updating the value of item each time.\n\nRPython\n\n\nIn R, for loops are used to iterate over items in a vector. They have the following basic form:\n\nfor (item in vector) perform_action\n\n\nfor (i in 1:3) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\n\n\n\nfor item in vector \n  perform_action\n\n\nfor i in range(1, 3):\n  print(i)\n\n1\n2\n\n\n\n\n\n\n\n2.2.2.2 while-Loops\nWith the while loop we can execute a set of statements as long as a condition is TRUE:\n\nRPython\n\n\n\ni &lt;- 1\nwhile (i &lt; 6) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\ni = 1\nwhile i &lt; 6:\n  print(i)\n  i += 1\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\n2.2.2.3 terminate\n\nRPython\n\n\nThere are two ways to terminate a for loop early:\n\nnext exits the current iteration.\nbreak exits the entire for loop.\n\n\nfor (i in 1:10) {\n  if (i &lt; 3) \n    next\n\n  print(i)\n  \n  if (i &gt;= 5)\n    break\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nfor i in range(1, 10):\n    if i &lt; 3:\n        continue\n    \n    print(i)\n    \n    if i &gt;= 5:\n        break\n\n3\n4\n5",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#function",
    "href": "dataprocess/basic_r_python.html#function",
    "title": "R & Python Basic",
    "section": "2.3 Function",
    "text": "2.3 Function\nMore details of in Advanced R Chapter 6\nA function is a block of code which only runs when it is called. It can be broken down into three components:\n\nThe formals(), the list of arguments that control how you call the function.\nThe body(), the code inside the function.\nThe environment(), the data structure that determines how the function finds the values associated with the names.\n\nWhile the formals and body are specified explicitly when you create a function, the environment is specified implicitly, based on where you defined the function. This location could be within another package or within the workspace (global environment).\n\nRPython\n\n\nThe function environment always exists, but it is only printed when the function isn’t defined in the global environment.\n\nfct_add &lt;- function(x, y) {\n  # A comment\n  x + y\n}\n\n# Get the formal arguments\nformals(fct_add)\n\n$x\n\n\n$y\n\n# Get the function's source code (body)\nbody(fct_add)\n\n{\n    x + y\n}\n\n# Get the function's global environment (module-level namespace)\nenvironment(fct_add)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\n\n\n\ndef fct_add(x, y):\n    # A comment\n    return x + y\n\n# Get the formal arguments\nprint(fct_add.__code__.co_varnames)\n\n('x', 'y')\n\n# Get the function's source code (body)\nprint(fct_add.__code__.co_code)\n\nb'\\x97\\x00|\\x00|\\x01z\\x00\\x00\\x00S\\x00'\n\n# Get the function's global environment (module-level namespace)\nprint(fct_add.__globals__)\n\n{'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__spec__': None, '__annotations__': {}, '__builtins__': &lt;module 'builtins' (built-in)&gt;, 'r': &lt;__main__.R object at 0x0000018FFF41EA10&gt;, 'x': [2.1, 4.2, 3.3, -5.6, 5.4, -1, 6.7, 7.9], 'y': 1.0, 'np': &lt;module 'numpy' from 'C:\\\\Users\\\\lei\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\PYTHON~1\\\\Lib\\\\site-packages\\\\numpy\\\\__init__.py'&gt;, 'pd': &lt;module 'pandas' from 'C:\\\\Users\\\\lei\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\PYTHON~1\\\\Lib\\\\site-packages\\\\pandas\\\\__init__.py'&gt;, 'vct_Test': [1, 5, 7], 'ary_Test': array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]]), 'mat_Test': array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12],\n       [13, 14, 15, 16],\n       [17, 18, 19, 20],\n       [21, 22, 23, 24]]), 'lst_Test': [[1, 3, 5], 'abc', False], 'df_Test':   name  age\n0  Bob   12\n1  Tom   13, 'a2': array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]]), 'colnames': ['A', 'B', 'C'], 'a3': array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]]), 'df':    x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c, 'a': 6, 'b': 8, 'x_a': array([2.1, 4.2, 3.3, 5.4]), 'idx': 0    False\n1     True\n2     True\nName: x, dtype: bool, 'idx_x_gt_1': 0    False\n1     True\n2     True\nName: x, dtype: bool, 'idx_z_eq_a': 0     True\n1    False\n2    False\nName: z, dtype: bool, 'math': &lt;module 'math' (built-in)&gt;, 'x_option': &lt;function x_option at 0x0000018F924FFCE0&gt;, 'i': 5, 'fct_add': &lt;function fct_add at 0x0000018F924D9DA0&gt;}\n\n\n\n\n\n\n2.3.1 Call\n\nRPython\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2, ...)\n\nTry using seq(), which makes regular sequences of numbers:\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe often omit the names of the first several arguments in function calls, so we can rewrite this as follows:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can also check the arguments and other information with:\n?seq\nThe “help” windows shows as:\n\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2)\n\n\nsequence = list(range(1, 11))\nprint(sequence)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\n\n\n\n2.3.2 Define\n\nRPython\n\n\nUse the function() keyword:\n\nmy_add1 &lt;- function(x) {\n  x + 1\n}\n\ncalling the function my_add1:\n\nmy_add1(2)\n\n[1] 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn R, the return statement is not essential for a function to yield a value as its result. By default, R will return the result of the last command within the function as its output.\n\n\n\n\nIn Python a function is defined using the def keyword:\n\ndef my_add(x):\n  return x + 1\n\ncalling the function my_add1:\n\nprint(my_add(2))\n\n3\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe return statement is essential for a function to yield a value as its result.",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#naming-rules",
    "href": "dataprocess/basic_r_python.html#naming-rules",
    "title": "R & Python Basic",
    "section": "3.1 Naming rules",
    "text": "3.1 Naming rules\n\nRPython\n\n\n\nmust start with a letter\ncan only contain letters, numbers, underscores _, and dot .\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Reserved Words\n\nTRUE FALSE\nNULL Inf NaN NA NA_real NA_complex_ NA_character_\nif else\nfor while repeat\nnext break\nfunction\nin\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\naFew.People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\n_start_with_underscores\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nmore Reserved Words in:\nhelp(\"reserved\")\n\n\n\nmust start with a letter or the underscore character _\ncan only contain letters, numbers, and underscores _\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Python keywords (35 keywors in Python 3.8)\n\nTrue False\nNone\nif else elif\nfor while repeat\ntry break continue finally\ndef\nin and or not\nreturn\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\n_start_with_underscores\notherPeopleUseCamelCase\naFew_People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\nwant.contain.dot\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nMore Keywords in:\n\nhelp(\"keywords\")",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#naming-conventions",
    "href": "dataprocess/basic_r_python.html#naming-conventions",
    "title": "R & Python Basic",
    "section": "3.2 Naming Conventions",
    "text": "3.2 Naming Conventions\n\nCamel Case\n\nEach word, except the first, starts with a capital letter:\nmyVariableName\n\nPascal Case\n\nEach word starts with a capital letter:\nMyVariableName\n\nSnake Case\n\nEach word is separated by an underscore character:\nmy_variable_name",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html",
    "href": "dataprocess/eda_basic.html",
    "title": "Exploratory data analysis (EDA)",
    "section": "",
    "text": "Before conducting any formal analysis, it is essential to first develop a preliminary understanding of the dataset. This step involves exploring the basic structure, summarizing key characteristics, and evaluating data quality. Therefore, Exploratory Data Analysis (EDA) serves as a crucial first stage in any data analysis process.\nDuring the initial phase of EDA, it is encouraged to freely investigate any hypothesis or question that arises (Wickham, Cetinkaya-Rundel, and Grolemund 2023).\nStatistical summaries provide the most fundamental insights into a dataset and are typically the easiest to obtain. More comprehensive summaries—such as quantiles, distributions, and visual representations—are discussed in the companion sections Statistic Basics and Graphical Statistics.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#dataset-description",
    "href": "dataprocess/eda_basic.html#dataset-description",
    "title": "Exploratory data analysis (EDA)",
    "section": "2.1 Dataset Description",
    "text": "2.1 Dataset Description\nFor demonstration purposes, we will use a synthetic dataset derived from daily temperature data recorded at the Düsseldorf station of the DWD, covering the period from 2005 to 2024.\nIn the correlation section, we will additionally use discharge data from two stations located in the Ruhr River basin.\nThe following R packages are required and will be loaded below:\n\n# Load required libraries for data manipulation, visualization, and time series handling\nlibrary(tidyverse)   # Data wrangling and visualization framework\ntheme_set(theme_bw()) # Set a clean and minimal default ggplot2 theme\nlibrary(plotly)      # Interactive visualization\nlibrary(xts)         # Time series data structure and analysis\nlibrary(patchwork)   # Combine multiple ggplots into one layout\n\nTo visualize the dataset in its basic form, we will define a few helper functions that plot the time series and display key patterns in the data.\n\n\nCode\ncolor_RUB_blue &lt;- \"#17365c\"\ncolor_RUB_green &lt;- \"#8dae10\"\ncolor_TUD_middleblue &lt;- \"#006ab2\"\ncolor_TUD_lightblue &lt;- \"#009de0\"\ncolor_TUD_green &lt;- \"#007d3f\"\ncolor_TUD_lightgreen &lt;- \"#69af22\"\ncolor_TUD_orange &lt;- \"#ee7f00\"\ncolor_TUD_pink &lt;- \"#EC008D\"\ncolor_TUD_purple &lt;- \"#54368a\"\ncolor_TUD_redpurple &lt;- \"#93107d\"\ncolor_SafetyOrange &lt;- \"#ff5e00\"\n\nplotly_xts_single &lt;- function(xts_Data) {\n  # Check input\n  if (!xts::is.xts(xts_Data))\n    stop(\"Input must be an xts object.\")\n  if (NCOL(xts_Data) != 1)\n    stop(\"xts object must contain exactly one column.\")\n  \n  # Extract data frame for ggplot\n  df &lt;- data.frame(\n    Date = zoo::index(xts_Data),\n    Value = as.numeric(xts_Data)\n  )\n  colname &lt;- colnames(xts_Data)[1]\n  \n  # Create ggplot\n  p &lt;- ggplot(df, aes(x = Date, y = Value)) +\n    geom_line(color = \"#17365c\") +\n    labs(\n      xts_Data = \"Date\",\n      y = colname\n    )\n  ggplotly(p)\n}\n\nplot_xts_outlier &lt;- function(xts_Data, idx_Outlier) {\n  # Input checks\n  if (!xts::is.xts(xts_Data))\n    stop(\"Input must be an xts object.\")\n  if (NCOL(xts_Data) != 1)\n    stop(\"xts object must contain exactly one column.\")\n  \n  # Align the two time series to the common time range\n\n  xts_Data_Outlier &lt;- xts_Data\n  xts_Data_Outlier[] &lt;- NA\n  xts_Data_Outlier[idx_Outlier] &lt;- xts_Data[idx_Outlier]\n  xts_Data[idx_Outlier] &lt;- NA\n  # Create ggplot\n  p &lt;- ggplot() +\n    geom_line(aes(index(xts_Data), y = xts_Data, color = \"Normal\")) +\n    geom_line(aes(index(xts_Data_Outlier), y = xts_Data_Outlier, color = \"Outlier\")) +\n    scale_color_manual(\"\", values = c(\"Outlier\" = \"#EC008D\", \"Normal\" = \"#17365c\")) +\n    labs(\n      xts_Data = \"Date\",\n      y = colnames(xts_Data)\n    )\n  \n  # Convert to interactive plotly\n  ggplotly(p)\n}\n\n\nA basic overview of the dataset is presented below using an interactive line plot, which allows us to visually explore the temporal variation of daily temperature values over the observation period.\n\n# Read the synthetic temperature dataset and convert it into an xts time series object\nxts_Outlier &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Outlier.csv\") |&gt; \n  as.xts()\n\n\n# Visualize the daily temperature time series interactively\nplotly_xts_single(xts_Outlier)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#preliminary-global-checks-for-invalid-data",
    "href": "dataprocess/eda_basic.html#preliminary-global-checks-for-invalid-data",
    "title": "Exploratory data analysis (EDA)",
    "section": "2.2 Preliminary Global Checks for Invalid Data",
    "text": "2.2 Preliminary Global Checks for Invalid Data\n\n2.2.1 Marked Values\nIn some datasets, specific “marked” values are used to indicate special meanings, often falling outside the normal measurement range. For example:\n- -999 may represent missing data,\n- -777 may indicate extremely high or low measurements.\nThese coded values must be identified and handled appropriately before analysis. If no additional information or suitable replacements are available and the dataset is sufficiently large, these marked values can be directly treated as unknown. In practice, this means replacing codes like -999 or -777 with proper missing value indicators (e.g., NA in R) so that statistical software correctly interprets them instead of treating them as valid numeric values.\nTypically, marked values are documented in a metadata file, README, or on the data provider’s website.\nIf no documentation exists, they can often be detected through basic visual inspection, appearing as extreme outliers or long constant sequences in a time series plot.\nTo replace these marked values, a simple equality check (==) can locate them, and they can then be replaced with NA:\n\n# Extract a subset of the dataset for 2005–2006\nxts_Outlier_Marked &lt;- xts_Outlier[\"2005-01-01/2006-12-31\"]\n\n# Identify marked values (-999) and replace them with NA\nidx_Outlier_Marked &lt;- which(xts_Outlier_Marked == -999)\nxts_Outlier_Marked[idx_Outlier_Marked] &lt;- NA\n\nThe following plot illustrates this process: the original data (with marked values) is shown in pink, while the cleaned version (after replacing marked values with NA) is shown in dark blue.\n\nplot_xts_outlier(xts_Outlier[\"2005-01-01/2006-12-31\"], idx_Outlier_Marked)\n\n\n\n\n\n\n\n2.2.2 Basic Rational Range\nMost measured variables have a reasonable or “rational” range. For example:\n\nEnvironmental variables like precipitation or discharge should always be positive.\nAir temperature has a wide global range (e.g., −50°C to 50°C), but for a specific location, this range can be narrowed.\n\nTo filter unrealistic values, we can restrict all measurements to fall within a defined interval. For the Düsseldorf station, a reasonable daily temperature range is -15 °C to 50 °C. Values outside this range can be identified using logical operators (&gt;, &lt;) and replaced with NA to indicate invalid data:\n\n# Extract a subset for the year 2012\nxts_Outlier_Range &lt;- xts_Outlier[\"2012-01-01/2012-12-31\"]\n\n# Identify values outside the rational range and set them to NA\nidx_outlier_Range &lt;- which(xts_Outlier_Range &gt; 38 | xts_Outlier_Range &lt; -15)\nxts_Outlier_Range[idx_outlier_Range] &lt;- NA\n\nThe cleaned results after applying this rational range filter are shown below:\n\nplot_xts_outlier(xts_Outlier[\"2012-01-01/2012-12-31\"], idx_outlier_Range)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#outlier-identification-with-statistical-methods",
    "href": "dataprocess/eda_basic.html#outlier-identification-with-statistical-methods",
    "title": "Exploratory data analysis (EDA)",
    "section": "2.3 Outlier Identification with Statistical Methods",
    "text": "2.3 Outlier Identification with Statistical Methods\nIn addition to global range checks, general statistical methods can help identify unusual values more objectively.\nThese approaches are particularly useful for obtaining a first overview of the dataset, especially when we lack deep prior knowledge.\n\n2.3.1 Z-Score Method\nThe Z-score method measures how far each data point is from the mean, in units of standard deviation.\nValues with very high or very low Z-scores are far from the center of the distribution and may be considered outliers.\nA common rule of thumb is that points with an absolute Z-score greater than 3 are potential outliers.\nNote that this threshold assumes approximately normal data — for non-normal distributions, the Z-score may be less reliable.\n\n2.3.1.1 Checking the Normality of Temperature at the Station\n\n# Extract temperature data from 2013–2024\nnum_Hist &lt;- xts_Outlier[\"2013-01-01/2024-12-31\"] |&gt; as.numeric()\n\n# Plot histogram\nggplot() +\n  geom_histogram(aes(num_Hist), color = color_RUB_blue, fill = color_RUB_green) +\n  labs(x = \"Temperature (°C)\", y = \"Frequency\", title = \"Histogram of Daily Temperature\")\n\n\n\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a variable follows a normal distribution. It produces two key outputs:\n\nW (Shapiro-Wilk statistic): measures how closely the sample data resemble a normal distribution. Values near 1 indicate approximate normality.\np-value: indicates the statistical significance of the deviation from normality. A small p-value (typically &lt; 0.05) suggests significant deviation from normality.\n\n\nNote: The W statistic reflects the degree of normality, while the p-value also considers sample size. Large datasets can produce very small p-values even when W is close to 1.\n\n\n# Perform Shapiro-Wilk test for normality\nshapiro.test(num_Hist)\n\n\n    Shapiro-Wilk normality test\n\ndata:  num_Hist\nW = 0.9919, p-value = 4.143e-15\n\n\nAlthough the p-value may be smaller than 0.05, combining it with the histogram and W-value shows that the data are sufficiently symmetric for Z-score based outlier detection to remain useful.\n\n\n2.3.1.2 Outlier ckeck with Z-Score\n\n# Subset the data for 2012\nxts_Outlier_Statistic_Z &lt;- xts_Outlier[\"2012-01-01/2012-12-31\"]\n\n# Compute Z-scores and identify outliers\nnum_ZScores &lt;- scale(xts_Outlier_Statistic_Z)\nthreshold_Z &lt;- 3\nidx_Outlier_Z &lt;- abs(num_ZScores) &gt; threshold_Z\n\n# Replace detected outliers with NA\nxts_Outlier_Statistic_Z[idx_Outlier_Z] &lt;- NA\n\n\n# Plot time series after Z-score outlier removal\nplot_xts_outlier(\n  xts_Outlier[\"2012-01-01/2012-12-31\"],\n  idx_Outlier_Z\n)\n\n\n\n\n\n\n\n\n2.3.2 Interquartile Range (IQR) Method\nThe Interquartile Range (IQR) method is another widely used approach for outlier detection. It focuses on the spread of the middle 50% of the data (between the first quartile Q1 and third quartile Q3). The IQR is calculated as:\nThe IQR is calculated as:\n\\[\n\\text{IQR} = Q3 - Q1\n\\]\nOutliers are typically defined as values lying below or above:\n\\[\n\\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR} \\quad , \\quad\n\\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR}\n\\]\nValues outside this range are considered potential outliers and can be handled appropriately.\n\n# Subset the data for the year 2012\nxts_Outlier_Statistic_IQR &lt;- xts_Outlier[\"2012-01-01/2012-12-31\"]\n\n# Calculate the first (Q1) and third (Q3) quartiles\nnum_Q1 &lt;- quantile(xts_Outlier_Statistic_IQR, 0.25, na.rm = TRUE)\nnum_Q3 &lt;- quantile(xts_Outlier_Statistic_IQR, 0.75, na.rm = TRUE)\n\n# Compute the Interquartile Range (IQR)\nnum_IQR &lt;- num_Q3 - num_Q1\n\n# Define lower and upper bounds for outlier detection\nbound_Lower &lt;- num_Q1 - 1.5 * num_IQR\nbound_Upper &lt;- num_Q3 + 1.5 * num_IQR\n\n# Identify outliers outside the IQR bounds\nidx_Outlier_IQR &lt;- (xts_Outlier_Statistic_IQR &lt; bound_Lower) | (xts_Outlier_Statistic_IQR &gt; bound_Upper)\n\n# Replace identified outliers with NA\nxts_Outlier_Statistic_IQR[idx_Outlier_IQR] &lt;- NA\n\n\n# Plot the time series after removing IQR-based outliers\nplot_xts_outlier(\n  xts_Outlier[\"2012-01-01/2012-12-31\"],\n  idx_Outlier_IQR\n)\n\n\n\n\n\n\n\nCode\n# Visualize outliers in a boxplot\nggplot() +\n  geom_boxplot(\n    aes(x = xts_Outlier[\"2012-01-01/2012-12-31\"] |&gt; as.numeric()),  # Convert time series to numeric vector for plotting\n    color = \"#17365c\",                     # Box color\n    outlier.color = \"#EC008D\"              # Highlight outliers in pink\n  ) +\n  labs(\n    x = \"Temperature (°C)\",\n    y = NULL,\n    title = \"Boxplot of Daily Temperature with Outliers Highlighted\"\n  )",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#outlier-identification-with-additional-knowledge",
    "href": "dataprocess/eda_basic.html#outlier-identification-with-additional-knowledge",
    "title": "Exploratory data analysis (EDA)",
    "section": "2.4 Outlier Identification with Additional Knowledge",
    "text": "2.4 Outlier Identification with Additional Knowledge\nIn many cases, outliers are more complex and cannot be detected using simple statistical rules alone.\nTo identify these unusual values effectively, we need to combine domain knowledge, experience, and a deeper understanding of the data.\n\n2.4.1 Constant Sequences over Long Periods in Time Series\nSome unusual situations in time series data appear as constant values over long periods.\nThese may indicate sensor malfunctions, data transmission errors, or missing updates.\nDetecting such patterns is an important part of exploratory data analysis (EDA).\n\n# Subset data for 2010\nxts_Outlier_TS_Const &lt;- xts_Outlier[\"2010-01-01/2010-12-31\"]\n\n# Identify sequences of repeated values\nint_Equal &lt;- rle(as.numeric(xts_Outlier_TS_Const))\nn_Threshold &lt;- 7  # Threshold for long constant sequences\nidx_Long &lt;- which(int_Equal$lengths &gt;= n_Threshold)\n\n# Convert run-length indices to actual positions\nidx_Outlier_Const &lt;- unlist(\n  lapply(idx_Long, \\(i) {\n    start_pos &lt;- sum(int_Equal$lengths[seq_len(i - 1)]) + 1\n    end_pos &lt;- sum(int_Equal$lengths[seq_len(i)])\n    start_pos:end_pos\n  })\n)\n\n# Replace long constant sequences with NA\nxts_Outlier_TS_Const[idx_Outlier_Const] &lt;- NA\n\n\n# Plot time series highlighting constant sequence outliers\nplot_xts_outlier(xts_Outlier[\"2010-01-01/2010-12-31\"], idx_Outlier_Const)\n\n\n\n\n\n\n\n2.4.2 Out of Trend or Seasonality\nTime series data typically consist of trend, seasonality, and residual components. By examining the trend and seasonal patterns, we can detect outliers that appear within global ranges but deviate strongly from expected patterns.\nIn order to detedc the distance between raw values and tresnd, the remeinder vlaues (Raw - Trend) could be used to located the outlier.\n\n# Subset 2008 data and compute residuals after removing trend\nxts_Outlier_TS_Trend &lt;- xts_Outlier[\"2008-01-01/2008-12-31\"]\nxts_Trend &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_TrendSeason.csv\") |&gt; as.xts()\n# Remaeinder\nxts_Remainder &lt;- xts_Outlier_TS_Trend - xts_Trend\n\nFor the continued anayls we will with IQR-Apporch:\n\nnum_Q1 &lt;- quantile(xts_Remainder, 0.25, na.rm = TRUE)\nnum_Q3 &lt;- quantile(xts_Remainder, 0.75, na.rm = TRUE)\n\n# Compute the Interquartile Range (IQR)\nnum_IQR &lt;- num_Q3 - num_Q1\n\n# Define lower and upper bounds for outlier detection\nbound_Lower &lt;- num_Q1 - 1.5 * num_IQR\nbound_Upper &lt;- num_Q3 + 1.5 * num_IQR\n\n# Identify outliers outside the IQR bounds\nidx_Outlier_Trend &lt;- (xts_Remainder &lt; bound_Lower) | (xts_Remainder &gt; bound_Upper)\n\n# Replace identified outliers with NA\nxts_Outlier_TS_Trend[idx_Outlier_Trend] &lt;- NA\n\n\n\nCode\n# Plot raw data and seasonal trend\nxts_Outlier_Trend &lt;- (xts_Outlier[\"2008-01-01/2008-12-31\"])[idx_Outlier_Trend]\n  # Create ggplot\n  gp_Trend &lt;- ggplot() +\n    geom_line(aes(index(xts_Outlier_TS_Trend), y = xts_Outlier_TS_Trend, color = \"Normal\")) +\n    geom_line(aes(index(xts_Outlier_Trend), y = xts_Outlier_Trend, color = \"Outlier\")) +\n    geom_line(aes(index(xts_Trend), y = xts_Trend, color = \"Seasonality\")) +\n    scale_color_manual(\"\", values = c(\"Outlier\" = \"#EC008D\", \"Normal\" = \"#17365c\", \"Seasonality\" = \"#8dae10\")) +\n    labs(\n      xts_Data = \"Date\",\n      y = \"Temperature (°C)\"\n    )\n  \n  # Convert to interactive plotly\n  ggplotly(gp_Trend)\n\n\n\n\n\n\n\n\nCode\n# Prepare data for combined boxplot\ndf_Box_Trend &lt;- data.frame(\n  Value = c(as.numeric(xts_Outlier_TS_Trend), as.numeric(xts_Remainder)),\n  Component = rep(c(\"Original\", \"Remainder (Seasonality Removed)\"),\n                  each = NROW(xts_Outlier_TS_Trend))\n)\n\n# Boxplot of original and residual data\nggplot(df_Box_Trend, aes(y = Value)) +\n  geom_boxplot(color = \"#17365c\", outlier.colour = \"#EC008D\") +\n  facet_wrap(~ Component, ncol = 2, scales = \"free_y\") +\n  labs(y = \"Temperature (°C)\", x = NULL) +\n  theme_bw() +\n  theme(\n    axis.x.title = element_blank(),\n    axis.x.text = element_blank(),\n    axis.x.ticks = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.background = element_rect(fill = \"gray90\", color = NA),\n    strip.text = element_text(face = \"bold\"),\n    plot.title = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nAs the two plots at above showed, in the raw tiem serise, the values between 01.11.2008 and 20.11.2008 will not be rechgnized as outlier, but wenn wen check with the seasonality information, the outlier situaion is scatuall verry obvirous.\n\n\n2.4.3 Relation with Additional Variables\nOutliers can also be identified by examining relationships between variables. A data point far from the expected relationship between two or more correlated variables may indicate unusual behavior.\nFor the Ruhr Basin, two stations—upstream (Bachum) and downstream (Villigst)—usually exhibit a strong correlation (~0.99). However, Bachum occasionally reports unusually high values. Comparing with Villigst allows detection of these outliers.\n\n# Load discharge data for both stations\nxts_Bachum &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Bachum.csv\") |&gt; as.xts()\nxts_Villigst &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Villigst.csv\") |&gt; as.xts()\n\n\n\nCode\n# Prepare time series and scatter plot data\ndf_Bachum &lt;- data.frame(Date = index(xts_Bachum), Value = as.numeric(xts_Bachum[,1]), Station = \"Bachum\")\ndf_Villigst &lt;- data.frame(Date = index(xts_Villigst), Value = as.numeric(xts_Villigst[,1]), Station = \"Villigst\")\ndf_Relation &lt;- bind_rows(df_Bachum, df_Villigst)\n\n# Time series plot\ngp_TSline_Rrelation &lt;- ggplot(df_Relation, aes(x = Date, y = Value, color = Station)) +\n  geom_line(linewidth = 0.5) +\n  facet_wrap(~Station, ncol = 1, scales = \"free_y\") +\n  scale_color_manual(values = c(\"Bachum\" = \"#1f77b4\", \"Villigst\" = \"#17365c\")) +\n  labs(x = NULL, y = \"Discharge (m³/s)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# Scatter plot of Bachum vs Villigst\ndf_Scatter &lt;- data.frame(Bachum = as.numeric(xts_Bachum), Villigst = as.numeric(xts_Villigst))\ngp_Scatter_Relation &lt;- ggplot(df_Scatter, aes(x = Villigst, y = Bachum)) +\n  geom_point(alpha = 0.4, color = \"#8dae10\") +\n  geom_abline(intercept = 4.1699263, slope = 0.7132356, color = \"#EC008D\") +\n  labs(x = \"Villigst\", y = \"Bachum\")\n\n# Combine both plots using patchwork\n(gp_TSline_Rrelation | gp_Scatter_Relation) +\n  plot_layout(widths = c(3,2))",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#filling-gaps-with-constant-values",
    "href": "dataprocess/eda_basic.html#filling-gaps-with-constant-values",
    "title": "Exploratory data analysis (EDA)",
    "section": "3.1 Filling Gaps with Constant Values",
    "text": "3.1 Filling Gaps with Constant Values\nThe simplest method is to fill missing values with a global statistic such as the mean or median. This approach preserves the overall statistical properties of the dataset and is easy to apply.\nHowever, the disadvantage is also clear: such replacements do not reflect local variations, trends, or relationships with other variables.\n\n# Calculate mean and median of non-missing values\nnum_Mean &lt;- mean(xts_Missing_T, na.rm = TRUE)\nnum_Median &lt;- median(xts_Missing_T, na.rm = TRUE)\n\n# Create copies for mean and median imputation\nxts_Missing_T_Mean &lt;- xts_Missing_T\nxts_Missing_T_Median &lt;- xts_Missing_T\n\n# Identify positions of missing values\nidx_Missing &lt;- is.na(xts_Missing_T)\n\n# Replace missing values with mean or median\nxts_Missing_T_Mean[idx_Missing] &lt;- num_Mean\nxts_Missing_T_Median[idx_Missing] &lt;- num_Median\n\n\n\nCode\n# Extract filled values for plotting\nxts_Filling_Mean &lt;- xts_Missing_T_Mean[idx_Missing]\nxts_Filling_Median &lt;- xts_Missing_T_Median[idx_Missing]\n\n# Plot original series and imputed values\ngp_Fill1 &lt;- ggplot() +\n  geom_line(aes(x = index(xts_Missing_T), y = xts_Missing_T), color = color_RUB_blue) +\n  geom_line(aes(x = index(xts_Filling_Mean), y = xts_Filling_Mean, color = \"Mean\")) +\n  geom_line(aes(x = index(xts_Filling_Median), y = xts_Filling_Median, color = \"Median\")) +\n  scale_color_manual(\"\", values = c(\"Mean\" = color_TUD_pink, \"Median\" = color_RUB_green)) +\n  labs(x = \"Date\", y = \"Temperature (°C)\")\n\n# Convert ggplot to an interactive plotly object\nggplotly(gp_Fill1)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#simple-interpolation-in-the-time-dimension",
    "href": "dataprocess/eda_basic.html#simple-interpolation-in-the-time-dimension",
    "title": "Exploratory data analysis (EDA)",
    "section": "3.2 Simple Interpolation in the Time Dimension",
    "text": "3.2 Simple Interpolation in the Time Dimension\nFor most state variables, which are strongly influenced by their previous states (e.g., temperature, river discharge, or other continuous processes), we can fill gaps by interpolating between neighboring values under the assumption of temporal continuity.\nCommon interpolation approaches include: - Using the previous (forward) value\n- Using the next (backward) value\n- Using the mean of the forward and backward values\n- Linear interpolation between adjacent observations\nAmong these, linear interpolation is the most commonly used method in practice.\n\n# Identify positions of missing values\nidx_Missing &lt;- which(is.na(xts_Missing_T))\n\n# Determine indices of values before and after the missing block\nidx_Forward &lt;- idx_Missing[1] - 1           # Index just before the first missing value\nidx_Backward &lt;- idx_Missing[length(idx_Missing)] + 1  # Index just after the last missing value\n\n# Extract the corresponding temperature values\nnum_Forward &lt;- xts_Missing_T[idx_Forward]   # Value before missing section\nnum_Backward &lt;- xts_Missing_T[idx_Backward] # Value after missing section\n\n# Compute mean of forward and backward values\nnum_MeanForBack &lt;- mean(c(num_Forward, num_Backward))\n\n# Create copies for each filling approach\nxts_Missing_T_MeanForBack &lt;- xts_Missing_T\nxts_Missing_T_Forward &lt;- xts_Missing_T\nxts_Missing_T_Backward &lt;- xts_Missing_T\n\n# Fill missing values using different methods\nxts_Missing_T_Forward[idx_Missing] &lt;- num_Forward          # Fill with overall mean\nxts_Missing_T_Backward[idx_Missing] &lt;- num_Backward      # Fill with overall median\nxts_Missing_T_MeanForBack[idx_Missing] &lt;- num_MeanForBack  # Fill with mean of adjacent values\n\n\n# Create copies for linear interpolation approach\nxts_Missing_T_Linear &lt;- xts_Missing_T\n# Fill missing values by linear interpolation\nxts_Missing_T_Linear &lt;- na.approx(xts_Missing_T)\n\n\n\nCode\n# Extract filled values for plotting\nxts_Filling_Forward &lt;- xts_Missing_T_Forward[idx_Missing]\nxts_Filling_Backward &lt;- xts_Missing_T_Backward[idx_Missing]\nxts_Filling_MeanForBack &lt;- xts_Missing_T_MeanForBack[idx_Missing]\nxts_Filling_Linear &lt;- xts_Missing_T_Linear[idx_Missing]\n\n# Plot original series and filled values\ngp_Fill2 &lt;- ggplot() +\n  geom_line(aes(x = index(xts_Missing_T), y = xts_Missing_T),\n            color = color_RUB_blue) +\n  geom_line(aes(x = index(xts_Filling_Forward), y = xts_Filling_Forward, color = \"Forward\")) +\n  geom_line(aes(x = index(xts_Filling_Backward), y = xts_Filling_Backward, color = \"Backward\")) +\n  geom_line(aes(x = index(xts_Filling_MeanForBack), y = xts_Filling_MeanForBack, color = \"Forward–Backward Mean\")) +\n  geom_line(aes(x = index(xts_Filling_Linear), y = xts_Filling_Linear, color = \"Linear interpolation\")) +\n  scale_color_manual(\n    \"\",\n    values = c(\n      \"Forward\" = color_TUD_pink,\n      \"Backward\" = color_RUB_green,\n      \"Forward–Backward Mean\" = color_SafetyOrange,\n      \"Linear interpolation\" = color_TUD_lightblue\n    )\n  ) +\n  labs(x = \"Date\", y = \"Temperature (°C)\")\n\n# Convert ggplot to interactive plotly\nggplotly(gp_Fill2)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#model-based-methods",
    "href": "dataprocess/eda_basic.html#model-based-methods",
    "title": "Exploratory data analysis (EDA)",
    "section": "3.3 Model-Based Methods",
    "text": "3.3 Model-Based Methods\nBy analyzing the time series components—such as trend and seasonality—or by exploring relationships with additional variables, we can construct simple models to estimate missing values. In such cases, the available data serve as model inputs, while the identified trend or relationship acts as a predictive model to fill the unknown values.\n\n3.3.1 Filling with Trend and Seasonality\nIf the time series has a clear trend and seasonal pattern, missing values can be estimated using the fitted trend and seasonality components. This ensures that the filled data remain consistent with the temporal structure of the dataset.\n\n# Load pre-calculated trend and seasonality components\nxts_TrendSeason &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_TrendSeason.csv\") |&gt; as.xts()\n# Create copies for linear interpolation approach\nxts_Missing_T_TrendSeason &lt;- xts_Missing_T\n# Fill missing values using the corresponding trend component\nxts_Missing_T_TrendSeason[idx_Missing] &lt;- xts_TrendSeason[idx_Missing]\n\n\n\nCode\n# Extract filled values for plotting\nxts_Filling_TrendSeason &lt;- xts_Missing_T_TrendSeason[idx_Missing]\nxts_Filling_Linear &lt;- xts_Missing_T_Linear[idx_Missing]\n\n# Plot original series and filled values\ngp_Fill3 &lt;- ggplot() +\n  geom_line(aes(x = index(xts_Missing_T), y = xts_Missing_T),\n            color = color_RUB_blue) +\n  geom_line(aes(x = index(xts_Filling_TrendSeason), y = xts_Filling_TrendSeason, color = \"Trend + Seasonality\")) +\n  geom_line(aes(x = index(xts_Filling_Linear), y = xts_Filling_Linear, color = \"Linear interpolation\")) +\n  scale_color_manual(\n    \"\",\n    values = c(\n      \"Trend + Seasonality\" = color_RUB_green,\n      \"Linear interpolation\" = color_TUD_lightblue\n    )\n  ) +\n  labs(x = \"Date\", y = \"Temperature (°C)\")\n\n# Convert ggplot to interactive plotly\nggplotly(gp_Fill3)\n\n\n\n\n\n\n\n\n3.3.2 Filling with Linear Relationships\nWhen there is a strong linear relationship between variables, we can use regression or correlation-based models to predict missing values. For example, missing discharge values might be estimated based on precipitation or temperature using a simple linear model.\n\n# Load time series data\nxts_Bachum_Missing &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Bachum_Missing.csv\") |&gt; as.xts()\nxts_Villigst &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Villigst.csv\") |&gt; as.xts()\n\n# Identify indices of missing values in Bachum series\nidx_Missing_Q &lt;- which(is.na(xts_Bachum_Missing))\n\n# Compute correlation between Bachum and Villigst, ignoring missing values\ncor(xts_Bachum_Missing, xts_Villigst, use = \"complete.obs\") # 0.9847129\n\n          X2\nV1 0.9847129\n\n# Fit linear regression to predict Bachum from Villigst\nlm_Bachum_Villigst &lt;- lm(xts_Bachum_Missing ~ xts_Villigst)\nlm_Coeffi &lt;- lm_Bachum_Villigst$coefficients # Intercept = b, Slope = a\n\n# Fill missing Bachum values using regression based on Villigst\nxts_Bachum_Missing_Corelation &lt;- xts_Bachum_Missing\nxts_Bachum_Missing_Corelation[idx_Missing_Q] &lt;- \n  lm_Coeffi[1] + lm_Coeffi[2] * xts_Villigst[idx_Missing_Q]  # y = a*x + b\n\n# Fill missing Bachum values using linear interpolation\nxts_Bachum_Missing_LinearInterpolate &lt;- na.approx(xts_Bachum_Missing)\n\n\n\nCode\n# Extract filled values for plotting\nxts_Filling_Corelation &lt;- xts_Bachum_Missing_Corelation[idx_Missing_Q]\nxts_Filling_Linear &lt;- xts_Bachum_Missing_LinearInterpolate[idx_Missing_Q]\n# Plot original series and imputed series\ngp_Bachum_Fill &lt;- ggplot() +\n  geom_line(aes(x = index(xts_Bachum_Missing), y = xts_Bachum_Missing),\n            color = color_RUB_blue) +\n  geom_line(aes(x = index(xts_Filling_Corelation), y = xts_Filling_Corelation,\n                color = \"Regression (Villigst)\")) +\n  geom_line(aes(x = index(xts_Filling_Linear), y = xts_Filling_Linear,\n                color = \"Linear Interpolation\")) +\n  scale_color_manual(\n    \"\",\n    values = c(\n      \"Regression (Villigst)\" = color_RUB_green,\n      \"Linear Interpolation\" = color_TUD_lightblue\n    )\n  ) +\n  labs(x = \"Date\", y = \"Discharge (m³/s)\")\n\n# Convert ggplot to interactive plotly object\nggplotly(gp_Bachum_Fill)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/NetCDF.html",
    "href": "dataprocess/NetCDF.html",
    "title": "NetCDF",
    "section": "",
    "text": "NetCDF stands for “Network Common Data Form.” It is a file format that is designed to store large arrays of data, primarily used in scientific and engineering applications. NetCDF files are self-describing, meaning they contain metadata along with the data, which makes it easier to understand the contents. NetCDF is particularly well-suited for storing multi-dimensional data, such as time series, spatial data, and climate model outputs. It can handle data with complex structures like grids, which are common in environmental and geospatial datasets.\nIn simple terms, NetCDF is a file format for storing multi-dimensional arrays of data along with metadata.\nMore Details in unidata.\nNetCDF files have a hierarchical structure, consisting of dimensions, variables, and attributes. Dimensions define the size of arrays, variables hold the data, and attributes provide additional information about the data.\nWith these three components, you can efficiently handle the import, creation, and export of data in the NetCDF format.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "NetCDF"
    ]
  },
  {
    "objectID": "dataprocess/NetCDF.html#library",
    "href": "dataprocess/NetCDF.html#library",
    "title": "NetCDF",
    "section": "1 Library",
    "text": "1 Library\n\nRPython\n\n\nThe ncdf4 R package is a powerful tool for working with NetCDF data in R, allowing you to read, write, and manipulate datasets in this format with ease and efficiency.\n\nlibrary(ncdf4)\nlibrary(tidyverse)\n\n# Define the NetCDF file path\nfn_NetCDF &lt;- \"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\"\n\n\n\nThe netCDF4 Python Library is a powerful tool for working with NetCDF data in R, allowing you to read, write, and manipulate datasets in this format with ease and efficiency.\n\nimport netCDF4 as nc\nimport numpy as np\n\n# Define the NetCDF file path\nfn_NetCDF = \"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\"\n\n\n\n\nThe Test data minibeispiel_NetCDF.nc is avable from Github data_share, but it can not be direcly read from Git hub so you need download to local.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "NetCDF"
    ]
  },
  {
    "objectID": "dataprocess/NetCDF.html#import",
    "href": "dataprocess/NetCDF.html#import",
    "title": "NetCDF",
    "section": "2 Import",
    "text": "2 Import\n\n2.1 Open\nThe first step in working with NetCDF files is to open the file using the nc_open() function. However, it’s important to note that opening the file doesn’t directly load its contents into the R environment. Instead, it establishes a connection between the file and the R session and effectively locks the file for reading or writing operations.\n\nRPython\n\n\n\n# Open the NetCDF file\nnc_Test &lt;- nc_open(fn_NetCDF)\n\n\n\n\n# Open the NetCDF file\nnc_Test = nc.Dataset(fn_NetCDF, \"r\")\n\n\n\n\n\n\n2.2 Basic Information\nAfter opening a NetCDF file in R, you can access the basic information about the dataset, which is contained in a list. This information typically includes details about three components: dimensions, variables, and attributes of the NetCDF file.\n\nRPython\n\n\n\n# Access the dimensions\n# nc_Test$dim\nnc_Test$dim |&gt; names()\n\n[1] \"latitude\"  \"longitude\" \"time\"     \n\n# Access the variables\n# nc_Test$var\nnc_Test$var |&gt; names()\n\n[1] \"T0\"  \"crs\"\n\nnc_Test$var$T0$size\n\n[1] 6 8 3\n\n# Access attributes\nncatt_get(nc_Test, 0)\n\n$title\n[1] \"Multidimensional data example\"\n\n$author\n[1] \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\"\n\n\n\n\n\n# Access the dimensions\nprint(nc_Test.dimensions)\n\n{'latitude': \"&lt;class 'netCDF4.Dimension'&gt;\": name = 'latitude', size = 6, 'longitude': \"&lt;class 'netCDF4.Dimension'&gt;\": name = 'longitude', size = 8, 'time': \"&lt;class 'netCDF4.Dimension'&gt;\" (unlimited): name = 'time', size = 3}\n\n# Access the variables\nprint(nc_Test.variables)\n\n{'latitude': &lt;class 'netCDF4.Variable'&gt;\nfloat64 latitude(latitude)\n    units: degrees_north\n    long_name: latitude\nunlimited dimensions: \ncurrent shape = (6,)\nfilling on, default _FillValue of 9.969209968386869e+36 used, 'longitude': &lt;class 'netCDF4.Variable'&gt;\nfloat64 longitude(longitude)\n    units: degrees_east\n    long_name: longitude\nunlimited dimensions: \ncurrent shape = (8,)\nfilling on, default _FillValue of 9.969209968386869e+36 used, 'time': &lt;class 'netCDF4.Variable'&gt;\nint32 time(time)\n    units: day since 1961-01-01 00:00:00 +00\n    long_name: time\nunlimited dimensions: time\ncurrent shape = (3,)\nfilling on, default _FillValue of -2147483647 used, 'T0': &lt;class 'netCDF4.Variable'&gt;\nfloat32 T0(time, longitude, latitude)\n    units: cel\n    _FillValue: -9999.0\nunlimited dimensions: time\ncurrent shape = (3, 8, 6)\nfilling on, 'crs': &lt;class 'netCDF4.Variable'&gt;\nfloat32 crs()\n    long_name: coordinate reference system\n    EPSG: EPSG:4236\nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of 9.969209968386869e+36 used}\n\n# Get the size of the \"T0\" variable\nprint(nc_Test.variables[\"T0\"].size)\n\n144\n\n# Access attributes associated with the NetCDF file\nprint(nc_Test.__dict__)\n\n{'title': 'Multidimensional data example', 'author': 'Kan, Lei, kan.lei@ruhr-uni-bochum.de'}\n\n\n\n\n\n\n\n2.3 Values and Attributes\nWith the basic information about variables obtained, you can access the values and attributes of each variable as needed. You can also obtain specific subsets of variables using start points and counts for each dimension.\nAdditionally, dimensions are treated as variables in the NetCDF structure, making it easier to work with them.\n\nRPython\n\n\n\nncvar_get()\n\nstart: The starting point of every dimension to load variable values\ncount: The length of every dimension to read\n\nncatt_get()\n\n\n# Retrieve the variable \"T0\" WHOLE\nncvar_get(nc_Test, \"T0\")\n\n, , 1\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]       [,7]\n[1,] 0.6815190 0.6685974 0.3355391 0.8300888 0.4126872 0.4213721 0.40900978\n[2,] 0.5834032 0.1514063 0.3042631 0.2520407 0.5793471 0.1833323 0.32067558\n[3,] 0.4924265 0.7376668 0.3327484 0.9493681 0.2489835 0.5988685 0.07783964\n[4,] 0.8509517 0.6454237 0.5295522 0.8479783 0.4104529 0.4381394 0.19551247\n[5,] 0.9409876 0.8425627 0.8565235 0.4752189 0.2917338 0.4781619 0.81465298\n[6,] 0.2088355 0.6121973 0.3734793 0.7684925 0.9713812 0.5124385 0.34575224\n          [,8]\n[1,] 0.5420827\n[2,] 0.8335595\n[3,] 0.9659727\n[4,] 0.6240343\n[5,] 0.7685761\n[6,] 0.3649738\n\n, , 2\n\n          [,1]      [,2]       [,3]      [,4]       [,5]      [,6]      [,7]\n[1,] 0.2292564 0.5033694 0.48205003 0.8213873 0.21405630 0.8678198 0.7539087\n[2,] 0.6349170 0.4978186 0.56034225 0.4690158 0.95863396 0.1501258 0.2760119\n[3,] 0.2507486 0.8313300 0.04853146 0.1445253 0.08056273 0.3183620 0.7758245\n[4,] 0.5768890 0.2531487 0.45174122 0.1941784 0.02582907 0.4415914 0.9977322\n[5,] 0.7781205 0.6768693 0.71639782 0.8491389 0.13584627 0.2038828 0.1653473\n[6,] 0.3596051 0.2155040 0.62368399 0.5900931 0.57847399 0.6779157 0.4215007\n           [,8]\n[1,] 0.80507427\n[2,] 0.36874512\n[3,] 0.21089411\n[4,] 0.32393828\n[5,] 0.49121958\n[6,] 0.05253027\n\n, , 3\n\n           [,1]       [,2]       [,3]       [,4]      [,5]      [,6]      [,7]\n[1,] 0.44514486 0.51928353 0.57821035 0.79330933 0.4629536 0.5375589 0.9546921\n[2,] 0.43322235 0.92014349 0.52168131 0.18247831 0.9246678 0.3676251 0.8033844\n[3,] 0.46280038 0.08913017 0.76012933 0.46169522 0.8953038 0.1482120 0.5176442\n[4,] 0.38045707 0.69357723 0.12975638 0.23547187 0.4842421 0.6838360 0.6942847\n[5,] 0.09263945 0.19688506 0.02503374 0.49694207 0.6021117 0.3664415 0.3513815\n[6,] 0.52355707 0.57813245 0.61153209 0.03659012 0.9769987 0.9546434 0.5306273\n          [,8]\n[1,] 0.3938288\n[2,] 0.4538694\n[3,] 0.9583837\n[4,] 0.5619988\n[5,] 0.4093841\n[6,] 0.6018101\n\n# Retrieve a subset of the variable \"T0\"\n# This subset starts at position (1, 1, 1) and has a count of (2, 3, 1) along each dimension\nncvar_get(nc_Test, \"T0\", start = c(1, 1, 1), count = c(2, 3, 1))\n\n          [,1]      [,2]      [,3]\n[1,] 0.6815190 0.6685974 0.3355391\n[2,] 0.5834032 0.1514063 0.3042631\n\n# Retrieve attributes associated with the variable \"T0\"\nncatt_get(nc_Test, \"T0\")\n\n$units\n[1] \"cel\"\n\n$`_FillValue`\n[1] -9999\n\n\n\n\n\nnc.variables[\"var_Name\"]\nnc.variables[\"var_Name\"].__dict__\n\n\n# Retrieve the entire \"T0\" variable\nt0_variable = nc_Test.variables[\"T0\"][:]\nprint(\"T0 variable (whole):\", t0_variable)\n\nT0 variable (whole): [[[0.68151903 0.58340317 0.4924265  0.85095173 0.9409876  0.20883553]\n  [0.6685974  0.15140632 0.7376668  0.64542365 0.84256274 0.61219734]\n  [0.33553913 0.30426314 0.33274835 0.52955216 0.8565235  0.3734793 ]\n  [0.8300888  0.2520407  0.9493681  0.8479783  0.47521892 0.7684925 ]\n  [0.4126872  0.5793471  0.24898352 0.41045293 0.29173383 0.97138125]\n  [0.42137206 0.18333228 0.59886855 0.43813944 0.47816187 0.5124385 ]\n  [0.40900978 0.32067558 0.07783964 0.19551247 0.814653   0.34575224]\n  [0.54208267 0.83355945 0.96597266 0.62403435 0.76857615 0.3649738 ]]\n\n [[0.22925638 0.63491696 0.2507486  0.576889   0.77812046 0.35960513]\n  [0.5033694  0.4978186  0.83133    0.25314873 0.6768693  0.21550402]\n  [0.48205003 0.56034225 0.04853146 0.45174122 0.7163978  0.623684  ]\n  [0.8213873  0.4690158  0.14452533 0.1941784  0.8491389  0.5900931 ]\n  [0.2140563  0.95863396 0.08056273 0.02582907 0.13584627 0.578474  ]\n  [0.8678198  0.15012585 0.31836203 0.44159144 0.20388278 0.6779157 ]\n  [0.7539087  0.2760119  0.7758245  0.9977322  0.16534728 0.4215007 ]\n  [0.8050743  0.36874512 0.21089411 0.32393828 0.49121958 0.05253027]]\n\n [[0.44514486 0.43322235 0.46280038 0.38045707 0.09263945 0.52355707]\n  [0.51928353 0.9201435  0.08913017 0.69357723 0.19688506 0.57813245]\n  [0.57821035 0.5216813  0.76012933 0.12975638 0.02503374 0.6115321 ]\n  [0.79330933 0.18247831 0.46169522 0.23547187 0.49694207 0.03659012]\n  [0.46295357 0.9246678  0.8953038  0.48424208 0.6021117  0.97699875]\n  [0.53755885 0.36762506 0.14821199 0.68383604 0.36644155 0.95464337]\n  [0.9546921  0.8033844  0.5176442  0.69428474 0.35138154 0.53062725]\n  [0.3938288  0.45386937 0.95838374 0.56199884 0.4093841  0.6018101 ]]]\n\n# Retrieve a subset of the \"T0\" variable\n# This subset starts at position (0, 0, 0) and has a count of (2, 3, 1) along each dimension\nprint(t0_variable[0:2, 0:3, 0:1])\n\n[[[0.68151903]\n  [0.6685974 ]\n  [0.33553913]]\n\n [[0.22925638]\n  [0.5033694 ]\n  [0.48205003]]]\n\n# Access attributes associated with the \"T0\" variable\nprint(nc_Test.variables[\"T0\"].__dict__)\n\n{'units': 'cel', '_FillValue': np.float32(-9999.0)}\n\n\n\n\n\n\n\n2.4 Close\nWhen working with NetCDF files in R using the ncdf4 package, it’s crucial to remember that opening a file establishes a connection. This prevents data corruption and conflicts. To finish, always close the file using nc_close() once you’ve completed your operations.\n\nRPython\n\n\n\n# Close the NetCDF file\nnc_close(nc_Test)\n\n\n\n\n# Close the NetCDF file\nnc_Test.close()",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "NetCDF"
    ]
  },
  {
    "objectID": "dataprocess/NetCDF.html#create-and-export",
    "href": "dataprocess/NetCDF.html#create-and-export",
    "title": "NetCDF",
    "section": "3 Create and Export",
    "text": "3 Create and Export\nIn this section, we will walk you through the steps to create a NetCDF file with your data. By following these steps, you’ll be able to prepare your data and save it in the NetCDF format for further analysis or sharing.\n\n3.1 Create new empty NetCDF file (Python)\n\nRPython\n\n\nIn R, you need after defining the dimensions and variables to create the file in the system. See Section 3.4.\n\n\nIn Python, you first need to create (connect) a new empty file in the system and an object in Python.\n\n# Create a NetCDF file\nnc_Create = nc.Dataset(\"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF_Py.nc\", \"w\")\n\n\n\n\n\n\n3.2 Define the Dimensions\nThe initial step in creating a NetCDF dataset is dimension definition:\n\nRPython\n\n\n\nncdim_def()\n\n\n# Define dimension metadata\nnum_Dim_Lon &lt;- seq(11.72, 11.79, 0.01)\nnum_Dim_Lat &lt;- seq(50.08, 50.13, 0.01)\nnum_Dim_Time &lt;- 1:3\n\ndim_lon &lt;- ncdim_def(\"longitude\", \"degrees_east\",\n                     num_Dim_Lon,\n                     longname = \"longitude\")\ndim_lat &lt;- ncdim_def(\"latitude\", \"degrees_north\",\n                     num_Dim_Lat,\n                     longname = \"latitude\")\ndim_time &lt;- ncdim_def(\"time\", \"day since 1961-01-01 00:00:00 +00\",\n                      num_Dim_Time, unlim=TRUE,\n                      longname = \"time\")\n\n\n\n\nnc.createDimension()\n\n\n# Define dimension metadata\nnum_Dim_Lon = np.arange(11.72, 11.8, 0.01)\nnum_Dim_Lat = np.arange(50.08, 50.14, 0.01)\nnum_Dim_Time = np.arange(1, 4)\n\n\n# Define dimensions\nnc_Create.createDimension(\"longitude\", len(num_Dim_Lon))\n\n\"&lt;class 'netCDF4.Dimension'&gt;\": name = 'longitude', size = 9\n\nnc_Create.createDimension(\"latitude\", len(num_Dim_Lat))\n\n\"&lt;class 'netCDF4.Dimension'&gt;\": name = 'latitude', size = 7\n\nnc_Create.createDimension(\"time\", len(num_Dim_Time))  # Use None for unlimited dimension\n\n\"&lt;class 'netCDF4.Dimension'&gt;\": name = 'time', size = 3\n\n\ndim_lon = nc_Create.createVariable(\"longitude\", \"f4\", \"longitude\")\ndim_lat = nc_Create.createVariable(\"latitude\", \"f4\", \"latitude\")\ndim_time = nc_Create.createVariable(\"time\", \"i\", \"time\") \n\ndim_lon[:] = num_Dim_Lon\ndim_lat[:] = num_Dim_Lat\ndim_time[:] = num_Dim_Time\n\nCompared to R, in Python, you need to create a variable with the same name to store the values of the dimension. In Python, a pure dimension will only consider the dimension’s size and name.\n\n\n\nIn this example, we will create a 3D array with latitude, longitude, and time dimensions.\n\n\n3.3 Define the Variales\nThe next step is to define a variable, but you don’t need to assign values to it at this stage. There are three common attributes (name, units and dimensions) that are essential for every variable and should always be defined. Other user-defined attributes can be added later as needed.\n\nRPython\n\n\n\nncvar_def()\n\nname\nunits\ndim\n\n\nYou also have the option to create a dimension with no data values, effectively making it a null dimension. However, you can still set attributes for this dimension to store non-array information.\nAfter defining all the variables, it’s necessary to gather them into a list.\n\n# Define a variable named \"T0\" with the units \"cel\" and dimensions dim_lat, dim_lon, and dim_time.\n# The missing value for this variable is set to -9999.\nvar_T0 &lt;- ncvar_def(\"T0\", \"cel\", list(dim_lat, dim_lon, dim_time), -9999)\n\n# Define a variable named \"crs\" with no units and no dimensions (empty list).\n# This variable is defined as NULL initially.\nvar_crs &lt;- ncvar_def(\"crs\", \"\", list(), NULL)\n\n# Combine variables into a list\nvars &lt;- list(var_T0, var_crs)\n\n\n\n\nnc.createVariable()\n\nname\nunits\ndim\n\n\n\n# Define variables\nvar_T0 = nc_Create.createVariable(\"T0\", \"f4\", (\"latitude\", \"longitude\", \"time\"))\nvar_T0.units = \"cel\"\nvar_T0.missing_value = -9999\n\nvar_crs = nc_Create.createVariable(\"crs\", \"S1\")  # Create an empty variable\n\n\n\n\n\n\n3.4 Create new empty NetCDF file (R)\n\nR\n\n\nYou can now create a NetCDF file with the (list of) variables you have:\n\nnc_create(filename, vars)\n\n\nnc_Create &lt;- nc_create(\"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\", vars)\n\n\n\n\n\n\n3.5 Put the Data\nAfter creating the NetCDF file, it will be an empty file in your local folder. The next step is to populate the file with data for each of the variables. This involves specifying the values for each variable and writing them to the file.\n\nRPython\n\n\n\nncvar_put()\n\n\nncvar_put(nc_Create, var_T0, runif(length(num_Dim_Lat) * length(num_Dim_Lon) * length(num_Dim_Time)))\n\n\n\n\n# Add data to the \"T0\" variable (random data)\nvar_T0[:] = np.random.rand(len(num_Dim_Lat), len(num_Dim_Lon), len(num_Dim_Time))\n\n\n\n\n\n\n3.6 Put Attributes\nWhen populating a NetCDF file, it’s essential to not only specify the variable data values but also the attributes associated with those variables. Attributes provide crucial metadata that describes the data, such as units, long names, and other relevant information.\n\nRPython\n\n\n\nncatt_put()\n\nAbsolutely, you can set attributes not only for individual variables.\n\n# Add the \"long_name\" and \"EPSG\" attributes to the variable \"var_crs\"\nncatt_put(nc_Create, var_crs, \"long_name\", \"coordinate reference system\")\nncatt_put(nc_Create, var_crs, \"EPSG\", \"EPSG:4236\")\n\n\n\n\nvar_crs.long_name = \"coordinate reference system\"\nvar_crs.EPSG = \"EPSG:4236\"\n\n\n\n\nBut also for the entire NetCDF file as global attributes. Global attributes provide overarching information about the dataset, such as its title, source, creation date, and any other relevant details.\n\nRPython\n\n\n\n# Add the \"title\" and \"author\" global attributes to the NetCDF file\nncatt_put(nc_Create, 0, \"title\", \"Multidimensional data example\")\nncatt_put(nc_Create, 0, \"author\", \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\")\n\n\n\n\n# Add global attributes\nnc_Create.title = \"Multidimensional data example\"\nnc_Create.author = \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\"\n\n\n\n\n\n\n3.7 Close\nAt the end, make sure to close the connections to your NetCDF files.\n\nRPython\n\n\n\nnc_close(nc_Create)\n\n\n\n\n# Close the NetCDF file\nnc_Create.close()\n\n\n\n\nOnce you’ve gone through these steps, you’ll have a well-maintained NetCDF file that can be easily used for any further processing, transformations, or visualization.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "NetCDF"
    ]
  },
  {
    "objectID": "dataprocess/spatial_extract.html",
    "href": "dataprocess/spatial_extract.html",
    "title": "Values Extract",
    "section": "",
    "text": "Spatial data extraction is the process of obtaining meaningful information from spatial datasets based on their geographic position. It is used whenever we want to retrieve values from a raster at specific locations, summarize information inside a polygon, mask a raster by an area, or link attributes between different spatial layers. In general, spatial extraction can be divided into two major groups: raster data extraction and vector data extraction.\nSpatial data extraction is especially important when we need to aggregate basic information for specific regions, or when we need to obtain consistent information from different spatial layers for comparison or further analysis.\nIn this exercis, we will work with the R package terra, which provides efficient tools for handling raster and vector spatial data. For visualization, we will use the tidyterra package, which allows terra objects to be plotted within the familiar ggplot2 framework.\n\n# Load packages\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(patchwork)\n\n\n\nCode\ncolor_RUB_blue &lt;- \"#17365c\"\ncolor_RUB_green &lt;- \"#8dae10\"\ncolor_TUD_middleblue &lt;- \"#006ab2\"\ncolor_TUD_lightblue &lt;- \"#009de0\"\ncolor_TUD_green &lt;- \"#007d3f\"\ncolor_TUD_lightgreen &lt;- \"#69af22\"\ncolor_TUD_orange &lt;- \"#ee7f00\"\ncolor_TUD_pink &lt;- \"#EC008D\"\ncolor_TUD_purple &lt;- \"#54368a\"\ncolor_TUD_redpurple &lt;- \"#93107d\"\ncolor_SafetyOrange &lt;- \"#ff5e00\"\ncolor_DRESDEN &lt;- c(\"#03305D\", \"#28618C\", \"#539DC5\", \"#84D1EE\", \"#009BA4\", \"#13A983\", \"#93C356\", \"#BCCF02\")\n\n\nFor the exercises, we will use simple synthetic datasets with random locations and values:\n\nTwo polygons used as extraction masks\n\nAn original raster with 1° spatial resolution\n\nA set of points\n\nA set of polygons\n\n\nrst_Random  &lt;- rast(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/rst_Extract_Random.asc\")\nnames(rst_Random) &lt;- \"RandomValues\"\nvct_Grid    &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Extract_Grid.geojson\")\nvct_Region    &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Extract_Region.geojson\")\nvct_Point   &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Extract_Point.geojson\")\nvct_Voronoi &lt;- vect(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/vct_Extract_Voronoi.geojson\")\n\n\n\nCode\ngp_Raster &lt;- ggplot() +\n  geom_spatraster(data = rst_Random) +\n  geom_spatvector(data = vct_Grid, fill = NA, color = \"gray86\") +\n  geom_spatvector_text(data = vct_Grid, aes(label = lyr.1), color = \"gray86\") +\n  geom_spatvector(data = vct_Region, color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN,\n                       limits = c(1, 100)) +\n  ggtitle(\"Raster\") +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\n\ngp_Polygon &lt;- ggplot() +\n  geom_spatvector(data = vct_Grid, fill = NA, color = \"gray86\") +\n  geom_spatvector_text(data = vct_Grid, aes(label = lyr.1), color = \"gray86\") +\n  geom_spatvector(data = vct_Region, aes(color = Region, fill = Region), alpha = .5) +\n  scale_color_manual(values = c(a = color_RUB_green, b = color_RUB_blue)) +\n  scale_fill_manual(values = c(a = color_RUB_green, b = color_RUB_blue)) +\n  ggtitle(\"Regions\") +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\ngp_Point &lt;- ggplot() +\n  geom_spatvector(data = vct_Point, aes(fill = Values),\n                  shape = 24, size = 4) +\n  geom_spatvector(data = vct_Region, color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Point') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\ngp_Voronoi &lt;- ggplot() +\n  geom_spatvector(data = vct_Voronoi, aes(fill = Values)) +\n  geom_spatvector(data = vct_Region, color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Polygon') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\n# Combine plots\n((gp_Polygon + theme(axis.text.x = element_blank())) | (gp_Raster + theme(axis.text.x = element_blank(), axis.text.y = element_blank()))) / (gp_Point | (gp_Voronoi + theme(axis.text.y = element_blank()))) + plot_layout(guides = \"collect\")",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/spatial_extract.html#rough-with-original-resolution",
    "href": "dataprocess/spatial_extract.html#rough-with-original-resolution",
    "title": "Values Extract",
    "section": "2.1 Rough with original resolution",
    "text": "2.1 Rough with original resolution\nThe first method uses the raster at its original resolution. However, when the spatial resolution is coarse, the selected grid cells may not accurately represent the region of interest. This is a common issue in meteorological data, where spatial resolution is often relatively low because temporal resolution is prioritized over fine spatial detail.\nFor the SELECT operation, two common methods are used: Touch and Center-point.\n\nTouch: all grid cells that intersect the region are selected.\n\nCenter-point: only grid cells whose center point falls within the region are selected.\n\nBoth methods, however, can produce implausible cases:\n\nUsing Touch, cells with only a small portion inside the region (e.g., Cell 4) are still selected.\n\nUsing Center-point, a cell with a small fraction inside the region (e.g., Cell 5, one-eighth) is counted as a full cell, while a cell with most of its area inside the region (e.g., Cell 18, three-quarters) may be ignored if its center lies outside.\n\nIn summary, using the original raster resolution is reasonable only when the mismatch between grid cells and region boundaries is small.\n\nTouch is suitable for extreme statistics, such as maximum or minimum values, because it includes all intersecting cells.\n\nCenter-point is generally better for calculating averages, as the over-selection and under-selection at the boundaries can balance out, reducing deviation.\n\n\n# Apply a mask to 'rst_Random' using the second polygon in 'vct_Region'\n# 'touches = TRUE' means that cells that **touch the polygon boundary** will also be included\nrst_CropTouch &lt;- mask(rst_Random, vct_Region[2], touches = TRUE)\n\n# Calculate the global mean of the masked raster 'rst_CropTouch'\n# 'na.rm = TRUE' ensures that NA values are ignored in the calculation\nglobal(rst_CropTouch, fun = mean, na.rm = TRUE)\n\n                 mean\nRandomValues 43.21053\n\n# Apply a mask to 'rst_Random' using the second polygon in 'vct_Region'\n# 'touches = FALSE' means that **only cells fully inside** the polygon are included\nrst_CropCenter &lt;- mask(rst_Random, vct_Region[2], touches = FALSE)\n\n# Calculate the global mean of the masked raster 'rst_CropCenter'\n# Again, NA values are ignored\nglobal(rst_CropCenter, fun = mean, na.rm = TRUE)\n\n             mean\nRandomValues   47\n\n\n\n\nCode\ngp_CropTouch &lt;- ggplot() +\n  geom_spatraster(data = rst_CropTouch) +\n  geom_spatvector(data = vct_Region[2], color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Mask with \"Touch\"') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\ngp_CropCenter &lt;- ggplot() +\n  geom_spatraster(data = rst_CropCenter) +\n  geom_spatvector(data = vct_Region[2], color = color_TUD_pink, linewidth = .5, fill = NA) +\n  geom_spatvector_text(data = vct_Grid[c(5, 18)], aes(label = lyr.1), color = color_TUD_pink) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Mask with \"Center\"') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\n\n(gp_CropTouch | gp_CropCenter) + plot_layout(guides = \"collect\")",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/spatial_extract.html#refine-resolution",
    "href": "dataprocess/spatial_extract.html#refine-resolution",
    "title": "Values Extract",
    "section": "2.2 Refine Resolution",
    "text": "2.2 Refine Resolution\nThe second method is straightforward: we increase the raster resolution, for example by making it 10 times finer in each dimension, which results in 100 times more grid cells overall.\nConceptually, this method does not differ from the first method, but it helps to reduce the mismatch between grid cells and the region of interest. This approach is particularly useful when using software that lacks dedicated spatial analysis tools—such as Matlab without the Spatial Analysis Toolbox. By simply refining the raster (e.g., replicating each cell 10 times in both rows and columns), we can achieve a finer resolution without relying on specialized spatial functions.\n\n# Disaggregate the raster 'rst_Random' by a factor of 10\n# This increases the resolution, creating smaller cells (finer raster)\nrst_Disagg &lt;- disagg(rst_Random, 10)\n\n# Mask the disaggregated raster using the second polygon in 'vct_Region'\n# 'touches = TRUE' includes cells that touch the polygon boundary\nrst_CropTouch_Disagg &lt;- mask(rst_Disagg, vct_Region[2], touches = TRUE)\n\n# Calculate the global mean of the masked raster\n# 'na.rm = TRUE' ensures missing values are ignored\nglobal(rst_CropTouch_Disagg, fun = mean, na.rm = TRUE)\n\n                 mean\nRandomValues 47.21555\n\n# Mask the disaggregated raster using the second polygon\n# 'touches = FALSE' includes only cells completely inside the polygon\nrst_CropCenter_Disagg &lt;- mask(rst_Disagg, vct_Region[2], touches = FALSE)\n\n# Calculate the global mean of the masked raster\nglobal(rst_CropCenter_Disagg, fun = mean, na.rm = TRUE)\n\n                 mean\nRandomValues 47.76703\n\n\n\n\nCode\ngp_CropTouch_Disagg &lt;- ggplot() +\n  geom_spatraster(data = rst_CropTouch_Disagg) +\n  geom_spatvector(data = vct_Region[2], color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Mask with \"Touch\" (disaggregated raster)') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\ngp_CropCenter_Disagg &lt;- ggplot() +\n  geom_spatraster(data = rst_CropCenter_Disagg) +\n  geom_spatvector(data = vct_Region[2], color = color_TUD_pink, linewidth = .5, fill = NA) +\n  geom_spatvector_text(data = vct_Grid[c(5, 18)], aes(label = lyr.1), color = color_TUD_pink) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Mask with \"Center\" (disaggregated raster)') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\n\n(gp_CropTouch_Disagg | gp_CropCenter_Disagg) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\nAs illustrated in the figure, the accuracy is significantly improved, and the deviation is expected to remain below 1%.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/spatial_extract.html#exact-extraction-with-polygons",
    "href": "dataprocess/spatial_extract.html#exact-extraction-with-polygons",
    "title": "Values Extract",
    "section": "2.3 Exact Extraction with Polygons",
    "text": "2.3 Exact Extraction with Polygons\nThe weighted mean generally provides more accurate results than a simple numerical average. In spatial analysis, the key aspect of a weighted mean is the choice of weights, which are typically based on the proportion of each grid cell’s area within the region of interest. Therefore, the main task in this method is to calculate the area of each raster value that lies within the target region.\nTo perform this calculation, it is often necessary to convert raster cells into vector polygons. There are two common approaches:\n\nAssign all cells with the same value to a single polygon. This method is convenient for categorical data with only a few distinct values.\n\nConvert each raster cell into an individual rectangle polygon and calculate the proportion of its area that falls within the region. This method is implemented in the terra package in R. However, a small deviation can occur when using longitude-latitude coordinates (CRS), because the actual area of each grid cell is not uniform, leading to minor inaccuracies in the weight calculation.\n\n\n# Convert the raster 'rst_Random' into polygons\n# Each raster cell becomes a polygon\nvct_Random &lt;- as.polygons(rst_Random)\n\n# Intersect the raster polygons with the first polygon in 'vct_Region'\n# Only the parts of 'vct_Random' that overlap with 'vct_Region[1]' are kept\nvct_Random_Poly1 &lt;- terra::intersect(vct_Random, vct_Region[1])\n\n# Intersect the raster polygons with the second polygon in 'vct_Region'\n# Only the parts of 'vct_Random' that overlap with 'vct_Region[2]' are kept\nvct_Random_Poly2 &lt;- terra::intersect(vct_Random, vct_Region[2])\n\n# Calculate the mean value of the attribute 'RandomValues' in the first intersected polygon\n# 'RandomValues' comes from the original raster values when it was converted to polygons\nmean(vct_Random_Poly1$RandomValues)\n\n[1] 49.77778\n\n# Calculate the mean value of the attribute 'RandomValues' in the second intersected polygon\nmean(vct_Random_Poly2$RandomValues)\n\n[1] 43.21053\n\n\n\n\nCode\ngp_CropExact_Poly1 &lt;- ggplot() +\n  geom_spatvector(data = vct_Random_Poly1, aes(fill = RandomValues)) +\n  geom_spatvector(data = vct_Region[1], color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Mask with vector-cell') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\ngp_CropExact_Poly2 &lt;- ggplot() +\n  geom_spatvector(data = vct_Random_Poly2, aes(fill = RandomValues)) +\n  geom_spatvector(data = vct_Region[2], color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Mask with vector-cell') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\n\n(gp_CropExact_Poly1 | gp_CropExact_Poly2) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\nIn the illustration, each raster value has been converted into a single polygon with the same value.\nHere’s a polished, academic version of your section in R Markdown style:",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/spatial_extract.html#exact-extraction-with-scale-product",
    "href": "dataprocess/spatial_extract.html#exact-extraction-with-scale-product",
    "title": "Values Extract",
    "section": "2.4 Exact Extraction with Scale Product",
    "text": "2.4 Exact Extraction with Scale Product\nThis method is designed specifically for meteorological data that span large temporal scales. It is also the most efficient method in practice.\nThe theory and formulation can be expressed as:\n\\[\n\\vec{\\Omega}_{[time, region]} = \\vec{A}_{[time, grid]} \\cdot \\vec{W}_{[grid, region]}\n\\]\nWhere:\n\n\\(\\vec{\\Omega}_{[time, region]}\\) = values for each region over time\n\n\\(\\vec{A}_{[time, grid]}\\) = matrix of all values for each grid over time [time, grid]\n\n\\(\\vec{W}_{[grid, region]}\\) = weight matrix representing the contribution of each grid to each region [grid, region]\n\n\n2.4.1 Weight Matrix\nThe weights are calculated as the proportion of each grid cell that lies within a region relative to the total area of the region. Note that this considers only the portion of the grid inside the region, not the entire grid.\nExample weight matrix (weight_grid):\n            [R1]      [R2]\n [G1]      0.000      0.00\n [G2] 134364.119 189431.77\n [G3] 212464.416      0.00\n [G4]   2747.413      0.00\n [G5] 150176.618      0.00\n [G6]      0.000  45011.22\nG represents grid cells and R represents regions.\n\n\n2.4.2 Value Matrix\nExample of a value matrix (mat_value):\n     [G1] [G2] [G3] [G4] [G5] [G6] \n[T1]    2    1    3    4    1    1  \n[T2]    3    1    2    4    1    1  \nT represents time steps.\nBy multiplying the value matrix by the weight matrix, we obtain the weighted regional values over time. This approach is particularly effective for large-scale temporal datasets.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/spatial_extract.html#extract-from-polygons",
    "href": "dataprocess/spatial_extract.html#extract-from-polygons",
    "title": "Values Extract",
    "section": "3.1 Extract from Polygons",
    "text": "3.1 Extract from Polygons\nWhen extracting a single attribute value, we can use the function intersect() to find the overlap between the data polygons and the region polygons. After the intersection, statistical summaries (e.g., mean, sum, or count) can be calculated for each region.\n\n# Intersect the Voronoi polygons with the second polygon in 'vct_Region'\n# Only the parts of 'vct_Voronoi' that overlap with 'vct_Region[2]' are kept\nvct_Voronoi_Poly2 &lt;- terra::intersect(vct_Voronoi, vct_Region[2])\n\n# Calculate the mean value of the attribute 'Values' in the second intersected polygon\nmean(vct_Voronoi_Poly2$Values)\n\n[1] 42.26673\n\n\n\n\nCode\ngp_CropVoronoi_Poly2 &lt;- ggplot() +\n  geom_spatvector(data = vct_Voronoi_Poly2, aes(fill = Values)) +\n  geom_spatvector(data = vct_Region[2], color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Mask with polygons') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\n\n(gp_Voronoi | gp_CropVoronoi_Poly2) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\nWhen we need to extract multiple attributes simultaneously, we can apply the same concept as in the previous section Exact with Scale Product:\n\\[\n\\vec{\\Omega}_{[attribute, region]} = \\vec{A}_{[attribute, polygon]} \\cdot \\vec{W}_{[polygon, region]}\n\\]\nWhere:\n\n\\(\\vec{\\Omega}_{[attribute, region]}\\) = values for each region for all attributes\n\n\\(\\vec{A}_{[attribute, polygon]}\\) = matrix of attribute values for each polygon [attribute, polygon]\n\n\\(\\vec{W}_{[polygon, region]}\\) = weight matrix representing the contribution of each polygon to each region [polygon, region]\n\nThe procedure consists of three main steps:\n\nWeight-Matrix creation: Use intersect() to calculate the proportion of each polygon that lies within each region, forming the matrix [polygon, region].\n\nValue-Matrix creation: Compile the attribute values of all polygons into a matrix [attribute, polygon].\n\nScale product: Multiply the Value-Matrix by the Weight-Matrix to obtain the weighted attribute values for each region.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/spatial_extract.html#extract-from-points",
    "href": "dataprocess/spatial_extract.html#extract-from-points",
    "title": "Values Extract",
    "section": "3.2 Extract from Points",
    "text": "3.2 Extract from Points\n\n3.2.1 Numerical Mean\nThe simplest and most direct method is to calculate the numerical mean of all points located within a given region:\n\n# Intersect the point vector with the second polygon in 'vct_Region'\n# Only points that fall inside 'vct_Region[2]' are kept\nvct_Point_Poly2 &lt;- terra::intersect(vct_Point, vct_Region[2])\n\n# Calculate the mean value of the attribute 'Values' in the second intersected polygon\nmean(vct_Point_Poly2$Values)\n\n[1] 63.80587\n\n\n\n\nCode\ngp_CropPoint_Poly2 &lt;- ggplot() +\n  geom_spatvector(data = vct_Point_Poly2, aes(fill = Values),\n                  shape = 24, size = 4) +\n  geom_spatvector(data = vct_Region[2], color = color_TUD_pink, linewidth = .5, fill = NA) +\n  scale_fill_gradientn(\"Value\", \n                       colors = color_DRESDEN, \n                       na.value = \"transparent\",\n                       limits = c(1, 100)) +\n  ggtitle('Mask with polygons') +\n  coord_sf(xlim = c(0, 5), ylim = c(0, 5), expand = FALSE) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_text(angle = 90, hjust = .5))\n\n\n(gp_Point | gp_CropPoint_Poly2) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\nThe procedure for calculating the numerical mean consists of two steps:\n\nIntersect the points with the regions, selecting only those points that fall within each region.\n\nCalculate the mean value of the selected points for each region.\n\nThe limitations of this approach are clear: points located near the boundaries of a region may be ignored, and some regions may contain no points at all.\nTo overcome these issues, point data can be converted into polygon form (e.g., Thiessen polygons) or raster form (via interpolation), and then analyzed using the extraction methods described earlier. For more details, see Spatial Interpolation.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html",
    "href": "dataprocess/statistic_basic.html",
    "title": "Statistic Basic",
    "section": "",
    "text": "One of the most important tasks while analyzing any time series is to describe and summarize the time series data in forms, which easily convey their important characteristics.\nKey statistical characteristics often described include: a measure of the central tendency of the data, a measure of spread or variability, a measure of the symmetry of the data distribution, and perhaps estimates of extremes such as some large or small percentile (Snedecor and Cochran 1980).\n\n\nAccording to Helsel and Hirsch (2020), the data about which a statement or summary is to be made are called ‘population’ or sometimes ‘target population’. It may be impossible both physically and economically to collect all data of interest. Alternatively, a subset of the entire data called ‘sample’ is selected and measured in such a way that conclusions about the sample may be extended to the entire population.\n\n\n\nIn statistics, measures of location or central tendency are used to summarize and describe the central or typical value in a dataset. Here are the six common measures of location (Machiwal and Jha 2012):\n\nMean: The mean, often referred to as the average, is calculated by summing all the values in a dataset and dividing by the number of values. It represents the balance point of the data.\nMedian: The median is the middle value when the data is sorted in ascending order. It’s less sensitive to extreme values (outliers) than the mean and is a good measure of the central value when the data is skewed.\nMode: The mode is the value that appears most frequently in the dataset. There can be multiple modes in a dataset, and it’s useful for categorical or discrete data.\nGeometric Mean: The geometric mean is used for data that is not normally distributed, such as financial returns or growth rates. It’s calculated by taking the nth root of the product of n values.\nTrimmed Mean: The trimmed mean is a variation of the mean that removes a certain percentage of extreme values (usually a specified percentage from both tails of the distribution) before calculating the mean. This makes it more robust to outliers.\n\nAmong these measures, the mean and median are the most widely used for summarizing data.\n\n\nThe arithmetic mean (\\(\\overline{{x}}\\)) is calculated by summing up of all data values, \\(x_{\\mathrm{i}}\\) and dividing the sum by the sample size \\(n\\):\n\\[\n{\\overline{{x}}}=\\sum_{i=1}^{n}{\\frac{x_{\\mathrm{i}}}{n}}\n\\]\n\n\n\nThe median is the middle value in a dataset when the data is ordered from smallest to largest. It’s a robust measure of central tendency that is not influenced by extreme values (outliers).\nFor an ordered dataset with ‘n’ values:\n\nIf ‘n’ is odd, the median is the middle value: \\[\n\\text{M} = x_{\\frac{n+1}{2}}\n\\]\nIf ‘n’ is even, the median is the average of the two middle values: \\[\n\\text{M} = \\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}\n\\]\n\n\n\n\nThe geometric mean (GM) is often used to compute summary statistic for positively skewed datasets (Machiwal and Jha 2012).\n\\[\n{\\mathrm{GM}}={\\mathrm{exp}}\\left[\\sum_{i=1}^{n}{\\frac{\\ln\\left(x_{\\mathrm{i}}\\right)}{n}}\\right]\n\\]\nFor the positively skewed data series, the GM is usually fairly close to the median of the series. In fact, the GM is an unbiased estimate of the median when the logarithms of the datasets are symmetric (Helsel et al. 2020).\n\n\n\n\n\n\nThe ‘sample variance’ and ‘sample standard deviation’ (square root of sample variance) are classical measures of spread (dispersion), which are the most common measures of dispersion (Machiwal and Jha 2012).\n\\[\ns^{2}=\\sum_{i=1}^{n}\\frac{\\left(x_{\\mathrm{i}}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}\n\\]\n\\[\ns={\\sqrt{\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}}}}\n\\]\n\n\n\nRobust measures of spreading about the mean include ‘range’, ‘interquartile range’, ‘coefficient of variation’ and ‘median absolute deviation’ (Machiwal and Jha 2012).\n\n\nQuantiles are values that divide a dataset into equally sized subsets. Common quantiles include quartiles (dividing data into four parts), quintiles (dividing into five parts), deciles (dividing into ten parts), and percentiles (dividing into one hundred parts).\n\nSort the dataset in ascending order.\nCompute the index ‘i’ as\n\n\\[\ni = \\text{round}((n+1) \\cdot q)\n\\]\n\nIf ‘i’ is an integer, the quantile is\n\n\\[\n\\text{Q}(q) = x_i\n\\] - If ‘i’ is not an integer, the quantile is interpolated as\n\\[\n\\text{Q}(q) = x_{\\lfloor i \\rfloor} + (i - \\lfloor i \\rfloor) \\cdot (x_{\\lfloor i \\rfloor + 1} - x_{\\lfloor i \\rfloor})\n\\]\nQuantiles are used to understand the spread and distribution of data and are often used in box plots and histograms to visualize data distribution.\n\n\n\nThe coefficient of variation (CV) gives a normalized measure of spreading about the mean, and is estimated as (Machiwal and Jha 2012):\n\\[\n\\mathbf{C}\\mathbf{V}(\\vartheta_{0})={\\frac{s}{\\bar{x}}}\\times100\n\\]\nHydrologic variables with larger CV values are more variable than those with smaller values. Wilding (in (SoilSpatialVariability_nielsen_1985?)) suggested a classification scheme for identifying the extent of variability for soil properties based on their CV values, where CV values of 0-15, 16-35 and &gt;36 indicate little, moderate and high variability, respectively.\n\n\n\nQuartile coefficient (QC) of dispersion is another descriptive statistic which measures dispersion and is used to make comparison within and between datasets. The test-statistic is computed using the first (P25) and third (P75) quartiles for each data set. The quartile coefficient of dispersion (QC) is given as (Machiwal and Jha 2012):\n\\[\n\\text{QC}={\\frac{P_{75}-P_{25}}{P_{75}+P_{25}}}\n\\]\n\n\n\n\n\nHydrologic time series data are usually skewed, which means that data in the time series are not symmetric around the mean or median, with extreme values extending out longer in one direction (Machiwal and Jha 2012).\n\n\nIt is defined as the adjusted third moment about the mean divided by the cube of the standard deviation (s), and is mathematically expressed as follows:\n\\[\ng={\\frac{n}{\\left(n-1\\right)\\,\\left(n-2\\right)}}\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{3}}{s^{3}}}\n\\]\nA positively skewed distribution of hydrologic time series with right extended tail has a positive coefficient of skewness, whereas a time series with negative-skewed distribution with left extended tail has a negative coefficient of skewness (Machiwal and Jha 2012).\n\n\n\nA robust measure of skewness is the ‘quartile skew coefficient (QS)’, which is defined as the difference in distances of the upper and lower quartiles from the median, divided by the IQR (Kenney John F 1939). Mathematically, it is expressed as:\n\\[\n\\text{QS}=\\frac{\\left(P_{75}-P_{50}\\,\\right)-\\left(P_{50}-P_{25}\\,\\right)}{P_{75}-P_{25}}\n\\]",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html#population-and-sample",
    "href": "dataprocess/statistic_basic.html#population-and-sample",
    "title": "Statistic Basic",
    "section": "",
    "text": "According to Helsel and Hirsch (2020), the data about which a statement or summary is to be made are called ‘population’ or sometimes ‘target population’. It may be impossible both physically and economically to collect all data of interest. Alternatively, a subset of the entire data called ‘sample’ is selected and measured in such a way that conclusions about the sample may be extended to the entire population.",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-location",
    "href": "dataprocess/statistic_basic.html#measures-of-location",
    "title": "Statistic Basic",
    "section": "",
    "text": "In statistics, measures of location or central tendency are used to summarize and describe the central or typical value in a dataset. Here are the six common measures of location (Machiwal and Jha 2012):\n\nMean: The mean, often referred to as the average, is calculated by summing all the values in a dataset and dividing by the number of values. It represents the balance point of the data.\nMedian: The median is the middle value when the data is sorted in ascending order. It’s less sensitive to extreme values (outliers) than the mean and is a good measure of the central value when the data is skewed.\nMode: The mode is the value that appears most frequently in the dataset. There can be multiple modes in a dataset, and it’s useful for categorical or discrete data.\nGeometric Mean: The geometric mean is used for data that is not normally distributed, such as financial returns or growth rates. It’s calculated by taking the nth root of the product of n values.\nTrimmed Mean: The trimmed mean is a variation of the mean that removes a certain percentage of extreme values (usually a specified percentage from both tails of the distribution) before calculating the mean. This makes it more robust to outliers.\n\nAmong these measures, the mean and median are the most widely used for summarizing data.\n\n\nThe arithmetic mean (\\(\\overline{{x}}\\)) is calculated by summing up of all data values, \\(x_{\\mathrm{i}}\\) and dividing the sum by the sample size \\(n\\):\n\\[\n{\\overline{{x}}}=\\sum_{i=1}^{n}{\\frac{x_{\\mathrm{i}}}{n}}\n\\]\n\n\n\nThe median is the middle value in a dataset when the data is ordered from smallest to largest. It’s a robust measure of central tendency that is not influenced by extreme values (outliers).\nFor an ordered dataset with ‘n’ values:\n\nIf ‘n’ is odd, the median is the middle value: \\[\n\\text{M} = x_{\\frac{n+1}{2}}\n\\]\nIf ‘n’ is even, the median is the average of the two middle values: \\[\n\\text{M} = \\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}\n\\]\n\n\n\n\nThe geometric mean (GM) is often used to compute summary statistic for positively skewed datasets (Machiwal and Jha 2012).\n\\[\n{\\mathrm{GM}}={\\mathrm{exp}}\\left[\\sum_{i=1}^{n}{\\frac{\\ln\\left(x_{\\mathrm{i}}\\right)}{n}}\\right]\n\\]\nFor the positively skewed data series, the GM is usually fairly close to the median of the series. In fact, the GM is an unbiased estimate of the median when the logarithms of the datasets are symmetric (Helsel et al. 2020).",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-spreaddispersion",
    "href": "dataprocess/statistic_basic.html#measures-of-spreaddispersion",
    "title": "Statistic Basic",
    "section": "",
    "text": "The ‘sample variance’ and ‘sample standard deviation’ (square root of sample variance) are classical measures of spread (dispersion), which are the most common measures of dispersion (Machiwal and Jha 2012).\n\\[\ns^{2}=\\sum_{i=1}^{n}\\frac{\\left(x_{\\mathrm{i}}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}\n\\]\n\\[\ns={\\sqrt{\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}}}}\n\\]\n\n\n\nRobust measures of spreading about the mean include ‘range’, ‘interquartile range’, ‘coefficient of variation’ and ‘median absolute deviation’ (Machiwal and Jha 2012).\n\n\nQuantiles are values that divide a dataset into equally sized subsets. Common quantiles include quartiles (dividing data into four parts), quintiles (dividing into five parts), deciles (dividing into ten parts), and percentiles (dividing into one hundred parts).\n\nSort the dataset in ascending order.\nCompute the index ‘i’ as\n\n\\[\ni = \\text{round}((n+1) \\cdot q)\n\\]\n\nIf ‘i’ is an integer, the quantile is\n\n\\[\n\\text{Q}(q) = x_i\n\\] - If ‘i’ is not an integer, the quantile is interpolated as\n\\[\n\\text{Q}(q) = x_{\\lfloor i \\rfloor} + (i - \\lfloor i \\rfloor) \\cdot (x_{\\lfloor i \\rfloor + 1} - x_{\\lfloor i \\rfloor})\n\\]\nQuantiles are used to understand the spread and distribution of data and are often used in box plots and histograms to visualize data distribution.\n\n\n\nThe coefficient of variation (CV) gives a normalized measure of spreading about the mean, and is estimated as (Machiwal and Jha 2012):\n\\[\n\\mathbf{C}\\mathbf{V}(\\vartheta_{0})={\\frac{s}{\\bar{x}}}\\times100\n\\]\nHydrologic variables with larger CV values are more variable than those with smaller values. Wilding (in (SoilSpatialVariability_nielsen_1985?)) suggested a classification scheme for identifying the extent of variability for soil properties based on their CV values, where CV values of 0-15, 16-35 and &gt;36 indicate little, moderate and high variability, respectively.\n\n\n\nQuartile coefficient (QC) of dispersion is another descriptive statistic which measures dispersion and is used to make comparison within and between datasets. The test-statistic is computed using the first (P25) and third (P75) quartiles for each data set. The quartile coefficient of dispersion (QC) is given as (Machiwal and Jha 2012):\n\\[\n\\text{QC}={\\frac{P_{75}-P_{25}}{P_{75}+P_{25}}}\n\\]",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-skewness",
    "href": "dataprocess/statistic_basic.html#measures-of-skewness",
    "title": "Statistic Basic",
    "section": "",
    "text": "Hydrologic time series data are usually skewed, which means that data in the time series are not symmetric around the mean or median, with extreme values extending out longer in one direction (Machiwal and Jha 2012).\n\n\nIt is defined as the adjusted third moment about the mean divided by the cube of the standard deviation (s), and is mathematically expressed as follows:\n\\[\ng={\\frac{n}{\\left(n-1\\right)\\,\\left(n-2\\right)}}\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{3}}{s^{3}}}\n\\]\nA positively skewed distribution of hydrologic time series with right extended tail has a positive coefficient of skewness, whereas a time series with negative-skewed distribution with left extended tail has a negative coefficient of skewness (Machiwal and Jha 2012).\n\n\n\nA robust measure of skewness is the ‘quartile skew coefficient (QS)’, which is defined as the difference in distances of the upper and lower quartiles from the median, divided by the IQR (Kenney John F 1939). Mathematically, it is expressed as:\n\\[\n\\text{QS}=\\frac{\\left(P_{75}-P_{50}\\,\\right)-\\left(P_{50}-P_{25}\\,\\right)}{P_{75}-P_{25}}\n\\]",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html",
    "href": "dataprocess/timeserises_analyse.html",
    "title": "Time Series Analyse",
    "section": "",
    "text": "A time series is often adequately described as a function of four components: trend, seasonality, dependent stochastic component and independent residual component (Machiwal and Jha 2012). It can be mathematically expressed as (Shahin, Oorschot, and Lange 1993):\n\\[\nx_{\\mathrm{t}}=T_{\\mathrm{t}}+S_{\\mathrm{t}}+\\varepsilon_{\\mathrm{t}}+\\eta_{\\mathrm{t}}\n\\]\nwhere\n\n\\(T_{\\mathrm{t}}\\) = trend component,\n\\(S_{\\mathrm{t}}\\) = seasonality,\n\\(\\varepsilon_{\\mathrm{t}}\\) = dependent stochastic component, and\n\\(\\eta_{\\mathrm{t}}\\) = independent residual component.\n\nThe first two components can be treat as systematic pattern, which are deterministic in nature, whereas the stochastic component accounts for the random error.\n\n\nFor demonstration purposes, we will use a synthetic dataset derived from daily temperature data recorded at the Düsseldorf station of the DWD, covering the period from 2005 to 2024.\nThe following R packages are required and will be loaded below:\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\nlibrary(xts)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(car)\n\nxts_Component &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Component.csv\") |&gt; as.xts()\n\nThe components of the dataset from 2020 to 2024 are shown as follows:\n\n\nCode\n# --- 1. Fit a loess trend to the time series\n# Use a span of 1 (full smoothing window) to estimate the trend\ntrend_Fit100 &lt;- stats::loess(coredata(xts_Component) ~ as.numeric(index(xts_Component)), span = 1)\n\n# Extract the fitted trend values\ntrend_Loess100 &lt;- trend_Fit100$fitted\nxts_Trend &lt;- xts(trend_Loess100, order.by = index(xts_Component))\n# Detrend the original series by subtracting the fitted trend\nxts_Detrend &lt;- xts_Component - trend_Loess100\n\n# Extract the time index from the xts object\ntime_index &lt;- index(xts_Component)\n\n# --- 2. Compute seasonal component based on day of the year\n# Convert dates to day-of-year (1–365/366)\nseason_Day_Index &lt;- as.numeric(format(time_index, \"%j\"))\n\n# Compute the mean of the detrended series for each day-of-year\n# This gives the seasonal effect for each day\nseason_Day &lt;- ave(xts_Detrend, season_Day_Index, FUN = mean, na.rm = TRUE)\n\n# Compute the residual component (remainder) after removing trend and seasonal effects\nremainder_Day &lt;- xts_Detrend - season_Day\n\n# --- 3. Combine results into a tidy data frame for plotting\ndf_TS_Plot &lt;- data.frame(\n  date = index(xts_Component[\"2020-01-01/2024-12-31\"]),             # time index\n  Original = xts_Component[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),# original time series\n  Trend = xts_Trend[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),  # extracted trend\n  Seasonal = season_Day[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),   # extracted seasonal component\n  Residual = remainder_Day[\"2020-01-01/2024-12-31\"] |&gt; as.numeric() # residual component\n) |&gt;\n  pivot_longer(\n    cols = c(Original, Trend, Seasonal, Residual), # pivot components into long format\n    names_to = \"Component\",                        # column indicating component type\n    values_to = \"Value\"                            # column containing values\n  )\n\n# --- 4. Plot the time series decomposition using ggplot2\ngp_TS &lt;- ggplot(df_TS_Plot, aes(x = date, y = Value)) +\n  geom_line(aes(color = Component)) +                # plot lines colored by component\n  facet_wrap(~Component, ncol = 1, scales = \"free_y\") + # separate facet for each component, free y-scale\n  labs(\n    x = NULL, \n    y = \"Value\", \n    title = \"Time Series Decomposition\"\n  ) +\n  theme(\n    strip.text = element_text(face = \"bold\", size = 12), # bold facet labels\n    panel.grid.minor = element_blank()                  # remove minor grid lines\n  )\n\n# Convert ggplot object to interactive plotly plot\nggplotly(gp_TS)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#dataset-description",
    "href": "dataprocess/timeserises_analyse.html#dataset-description",
    "title": "Time Series Analyse",
    "section": "",
    "text": "For demonstration purposes, we will use a synthetic dataset derived from daily temperature data recorded at the Düsseldorf station of the DWD, covering the period from 2005 to 2024.\nThe following R packages are required and will be loaded below:\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\nlibrary(xts)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(car)\n\nxts_Component &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Component.csv\") |&gt; as.xts()\n\nThe components of the dataset from 2020 to 2024 are shown as follows:\n\n\nCode\n# --- 1. Fit a loess trend to the time series\n# Use a span of 1 (full smoothing window) to estimate the trend\ntrend_Fit100 &lt;- stats::loess(coredata(xts_Component) ~ as.numeric(index(xts_Component)), span = 1)\n\n# Extract the fitted trend values\ntrend_Loess100 &lt;- trend_Fit100$fitted\nxts_Trend &lt;- xts(trend_Loess100, order.by = index(xts_Component))\n# Detrend the original series by subtracting the fitted trend\nxts_Detrend &lt;- xts_Component - trend_Loess100\n\n# Extract the time index from the xts object\ntime_index &lt;- index(xts_Component)\n\n# --- 2. Compute seasonal component based on day of the year\n# Convert dates to day-of-year (1–365/366)\nseason_Day_Index &lt;- as.numeric(format(time_index, \"%j\"))\n\n# Compute the mean of the detrended series for each day-of-year\n# This gives the seasonal effect for each day\nseason_Day &lt;- ave(xts_Detrend, season_Day_Index, FUN = mean, na.rm = TRUE)\n\n# Compute the residual component (remainder) after removing trend and seasonal effects\nremainder_Day &lt;- xts_Detrend - season_Day\n\n# --- 3. Combine results into a tidy data frame for plotting\ndf_TS_Plot &lt;- data.frame(\n  date = index(xts_Component[\"2020-01-01/2024-12-31\"]),             # time index\n  Original = xts_Component[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),# original time series\n  Trend = xts_Trend[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),  # extracted trend\n  Seasonal = season_Day[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),   # extracted seasonal component\n  Residual = remainder_Day[\"2020-01-01/2024-12-31\"] |&gt; as.numeric() # residual component\n) |&gt;\n  pivot_longer(\n    cols = c(Original, Trend, Seasonal, Residual), # pivot components into long format\n    names_to = \"Component\",                        # column indicating component type\n    values_to = \"Value\"                            # column containing values\n  )\n\n# --- 4. Plot the time series decomposition using ggplot2\ngp_TS &lt;- ggplot(df_TS_Plot, aes(x = date, y = Value)) +\n  geom_line(aes(color = Component)) +                # plot lines colored by component\n  facet_wrap(~Component, ncol = 1, scales = \"free_y\") + # separate facet for each component, free y-scale\n  labs(\n    x = NULL, \n    y = \"Value\", \n    title = \"Time Series Decomposition\"\n  ) +\n  theme(\n    strip.text = element_text(face = \"bold\", size = 12), # bold facet labels\n    panel.grid.minor = element_blank()                  # remove minor grid lines\n  )\n\n# Convert ggplot object to interactive plotly plot\nggplotly(gp_TS)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#stationarity-tests",
    "href": "dataprocess/timeserises_analyse.html#stationarity-tests",
    "title": "Time Series Analyse",
    "section": "2.1 Stationarity Tests",
    "text": "2.1 Stationarity Tests\n\n2.1.1 Augmented Dickey-Fuller (ADF) Test\nThe Augmented Dickey-Fuller (ADF) test checks whether a time series has a unit root, i.e., whether it is non-stationary.\n\nNull hypothesis (H₀): The series has a unit root → non-stationary\n\nAlternative hypothesis (H₁): The series is stationary\n\nInterpretation:\n\np-value &lt; 0.05 → reject H₀ → series is stationary\n\np-value ≥ 0.05 → fail to reject H₀ → series is likely non-stationary\n\n\nadf.test(coredata(xts_Component))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  coredata(xts_Component)\nDickey-Fuller = -5.7176, Lag order = 19, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n2.1.2 KPSS Test\nThe KPSS (Kwiatkowski–Phillips–Schmidt–Shin) test is complementary to the ADF test.\nIt tests whether a series is stationary around a level or trend.\n\nNull hypothesis (H₀): The series is stationary\n\nAlternative hypothesis (H₁): The series is non-stationary\n\nInterpretation:\n\np-value ≥ 0.05 → fail to reject H₀ → series is stationary\n\np-value &lt; 0.05 → reject H₀ → series is non-stationary\n\n\nkpss.test(coredata(xts_Component), null = \"Level\")\n\n\n    KPSS Test for Level Stationarity\n\ndata:  coredata(xts_Component)\nKPSS Level = 0.4444, Truncation lag parameter = 11, p-value = 0.05802",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#trend",
    "href": "dataprocess/timeserises_analyse.html#trend",
    "title": "Time Series Analyse",
    "section": "2.2 Trend",
    "text": "2.2 Trend\nTrend analysis helps reveal long-term changes in a time series by smoothing out short-term fluctuations.\nDifferent methods can be used depending on the data characteristics and analysis goals — from flexible non-parametric smoothing to simple linear regression or aggregation-based trends.\n\n2.2.1 LOESS Fit\nThe LOESS (Locally Estimated Scatterplot Smoothing) method provides a flexible, non-linear fit that adapts to local patterns in the data.\nIt is especially useful when the trend is not strictly linear but varies over time.\nThe span parameter controls the smoothness — higher values produce smoother curves.\n\n# LOESS with a wide smoothing window (span = 1)\ntrend_Fit100 &lt;- stats::loess(coredata(xts_Component) ~ as.numeric(index(xts_Component)), span = 1)\ntrend_Loess100 &lt;- trend_Fit100$fitted\n\n# LOESS with a narrower smoothing window (span = 0.5)\ntrend_Fit50 &lt;- stats::loess(coredata(xts_Component) ~ as.numeric(index(xts_Component)), span = 0.5)\ntrend_Loess50 &lt;- trend_Fit50$fitted\n\n\n\n2.2.2 Linear Regression Fit\nA linear regression trend assumes that the time series follows a steady, linear increase or decrease over time. This simple model is often used as a baseline or to capture overall tendencies.\nThis approach assumes linearity — it may not capture seasonal or cyclical behavior, but it provides a clear long-term direction.\n\n# Fit a linear regression model on time (index) and data values\ntrend_LM &lt;- stats::lm(coredata(xts_Component) ~ as.numeric(index(xts_Component)))\n\n# Extract fitted (predicted) trend values\ntrend_Lin &lt;- trend_LM$fitted.values\n\n\n\n2.2.3 Period Means\nFor datasets with strong seasonality or noise, calculating period averages (e.g., yearly means) can be an effective way to reveal large-scale trends. A linear fit on these aggregated means reduces variability and highlights gradual changes over longer timescales.\nYearly-mean trends are particularly useful when short-term fluctuations (like daily or monthly variability) obscure the long-term signal.\n\n# Aggregate the data to yearly means\nxts_Year &lt;- apply.yearly(xts_Component, colMeans)\n\n# Set the date to mid-year for better visual alignment\nindex(xts_Year) &lt;- as.Date(paste0(format(index(xts_Year), \"%Y\"), \"-07-01\"))\n\n# Fit a linear model to the yearly means\ntrend_YearLM &lt;- stats::lm(coredata(xts_Year) ~ as.numeric(index(xts_Year)))\n\n# Extract the fitted trend line for yearly data\ntrend_YearLin &lt;- trend_YearLM$fitted.values\n\n\n\n2.2.4 Trend Visualization\nThe following plot compares different trend estimation approaches — LOESS fits, linear regression, and yearly mean trends.\n\n\nCode\n# Convert daily xts data to a data frame for plotting\ndf_TS_Trend_Daily &lt;- data.frame(\n  Date = index(xts_Component) |&gt; as_date(),\n  Loess100 = trend_Loess100,     # LOESS with span = 1.0\n  Loess50  = trend_Loess50,      # LOESS with span = 0.5\n  Lin      = trend_Lin           # Linear regression (full dataset)\n)\n\n# Convert yearly aggregated data\ndf_TS_Trend_Yearly &lt;- data.frame(\n  Date = index(xts_Year) |&gt; as_date(),\n  Value = coredata(xts_Year) |&gt; as.numeric(),  # Yearly means\n  YearLin = trend_YearLin                      # Linear regression on yearly means\n)\n\n# Combine and plot all trend lines for comparison\ngp_trend &lt;- ggplot() +\n  geom_line(data = df_TS_Trend_Yearly, aes(x = Date, y = Value, color = \"Yearly mean\"), linewidth = 0.6) +\n  geom_line(data = df_TS_Trend_Daily, aes(x = Date, y = Loess100, color = \"LOESS (span = 1.0)\"), linewidth = 1) +\n  geom_line(data = df_TS_Trend_Daily, aes(x = Date, y = Loess50,  color = \"LOESS (span = 0.5)\"), linewidth = 1) +\n  geom_line(data = df_TS_Trend_Daily, aes(x = Date, y = Lin,      color = \"Linear (full)\"), linewidth = 1) +\n  geom_line(data = df_TS_Trend_Yearly, aes(x = Date, y = YearLin, color = \"Linear (yearly)\"), linewidth = 1) +\n  scale_color_manual(\n    name = \"Trend Type\",\n    values = c(\n      \"Yearly mean\"       = \"gray80\",\n      \"LOESS (span = 1.0)\" = \"#1b9e77\",\n      \"LOESS (span = 0.5)\" = \"#7570b3\",\n      \"Linear (full)\"      = \"#d95f02\",\n      \"Linear (yearly)\"    = \"#e7298a\"\n    )\n  ) +\n  labs(\n    x = \"Date\",\n    y = \"Temperature (°C)\",\n    title = \"Comparison of Trend Estimation Methods\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank()\n  )\n\n# Convert static ggplot to interactive plotly visualization\nggplotly(gp_trend)\n\n\n\n\n\n\nThe smoother LOESS curves (with different spans) show local variations, while the linear fits highlight long-term directional changes. The yearly mean trend provides an aggregated perspective, reducing short-term variability.\nHere’s an improved and polished version of your text and code section with clearer explanations, smoother academic phrasing, and well-structured code comments suitable for a Quarto/R Markdown document:",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#t-test-and-anova",
    "href": "dataprocess/timeserises_analyse.html#t-test-and-anova",
    "title": "Time Series Analyse",
    "section": "3.1 t-test and ANOVA",
    "text": "3.1 t-test and ANOVA\nTo investigate differences between groups, we can apply t-tests and Analysis of Variance (ANOVA):\n\nThe t-test is used when comparing the means of two groups, such as upstream vs. downstream stations.\nThe ANOVA test generalizes this to three or more groups, for example when comparing data across multiple years or months.\n\nBoth tests assess whether the group means differ significantly from each other. To interpret their results, we consider the hypotheses and the resulting p-value:\n\nNull hypothesis (H₀): All group means are equal (no significant difference).\nAlternative hypothesis (H₁): At least one group mean is different.\n\nInterpretation:\n\nA large p-value (typically &gt; 0.05) → Fail to reject H₀ → No significant difference between groups.\nA small p-value (&lt; 0.05) → Reject H₀ → Significant difference exists between groups, suggesting that group characteristics (e.g., mean temperature) vary meaningfully.\n\nWhen using these tests, it is also good practice to check homogeneity of variances (e.g., with the Levene test), since standard ANOVA assumes that all groups have similar variance.",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#levenes-test-for-equal-variances",
    "href": "dataprocess/timeserises_analyse.html#levenes-test-for-equal-variances",
    "title": "Time Series Analyse",
    "section": "3.2 Levene’s Test for Equal Variances",
    "text": "3.2 Levene’s Test for Equal Variances\nThe Levene Test is a robust and widely used method to assess the homogeneity of variance among groups.\n\nNull hypothesis (H₀): All group variances are equal.\nAlternative hypothesis (H₁): At least one group has a different variance.\n\nInterpretation:\n\nA large p-value (typically &gt; 0.05) → Fail to reject H₀ → Variances are not significantly different.\nA small p-value (&lt; 0.05) → Reject H₀ → Variances are significantly different, and equal-variance methods (like standard ANOVA) should be applied cautiously.",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#example-with-different-grouping",
    "href": "dataprocess/timeserises_analyse.html#example-with-different-grouping",
    "title": "Time Series Analyse",
    "section": "3.3 Example with different Grouping",
    "text": "3.3 Example with different Grouping\n\n3.3.1 Example: Annual Grouping\nWe first test whether yearly data show significant mean or variance differences.\n\n# Group data by year\ngroup_Homo_Year &lt;- factor(format(index(xts_Component), \"%Y\"))\n\n# One-way ANOVA: check for mean differences between years\nsummary(aov(xts_Component ~ group_Homo_Year))\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ngroup_Homo_Year   19   3815   200.8   4.472 2.99e-10 ***\nResiduals       7285 327107    44.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Levene’s test: check for homogeneity of variances across years\nleveneTest(y = xts_Component |&gt; as.numeric(), group = group_Homo_Year)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup   19  7.2229 &lt; 2.2e-16 ***\n      7285                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.3.2 Example: Two Time Periods (Decades)\nWe can also split the dataset into two large periods (e.g., first vs. second half) and test for differences.\n\n# Divide dataset into two halves\nn_Data &lt;- nrow(xts_Component)\ngroup_Homo_Decade &lt;- factor(ifelse(1:n_Data &lt;= n_Data/2, \"FirstHalf\", \"SecondHalf\"))\n\n# t-test: compare mean values between the two halves\nt.test(xts_Component ~ group_Homo_Decade)\n\n\n    Welch Two Sample t-test\n\ndata:  xts_Component by group_Homo_Decade\nt = -5.258, df = 7298.9, p-value = 1.498e-07\nalternative hypothesis: true difference in means between group FirstHalf and group SecondHalf is not equal to 0\n95 percent confidence interval:\n -1.1348668 -0.5184657\nsample estimates:\n mean in group FirstHalf mean in group SecondHalf \n                10.98817                 11.81484 \n\n# Levene’s test: compare variances between the two halves\nleveneTest(y = xts_Component |&gt; as.numeric(), group = group_Homo_Decade)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value Pr(&gt;F)\ngroup    1  0.6557 0.4181\n      7303               \n\n\n\n\n3.3.3 Example: Random Two-Group Test\nTo check how the tests behave under random grouping (without actual structure), we can randomly assign each observation to one of two groups.\n\nset.seed(666)\n\n# Randomly assign data to two groups\ngroup_Homo_Random &lt;- sample(1:2, size = n_Data, replace = TRUE, prob = rep(1/2, 2)) |&gt; factor()\n\n# Apply t-test and Levene’s test\nt.test(xts_Component ~ group_Homo_Random)\n\n\n    Welch Two Sample t-test\n\ndata:  xts_Component by group_Homo_Random\nt = -2.0798, df = 7302.5, p-value = 0.03758\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.63620769 -0.01881909\nsample estimates:\nmean in group 1 mean in group 2 \n       11.23827        11.56579 \n\nleveneTest(y = xts_Component |&gt; as.numeric(), group = group_Homo_Random)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value Pr(&gt;F)\ngroup    1       0 0.9974\n      7303               \n\n\n\n\n3.3.4 Example: Random Multi-Group Test (ANOVA)\nFinally, we create five random groups to illustrate multi-group testing.\n\nset.seed(2025)\n\n# Randomly assign data to five groups\ngroup_Homo_Random &lt;- sample(1:5, size = n_Data, replace = TRUE, prob = rep(1/5, 5)) |&gt; factor()\n\n# ANOVA: compare means among five random groups\nsummary(aov(xts_Component ~ group_Homo_Random))\n\n                    Df Sum Sq Mean Sq F value Pr(&gt;F)\ngroup_Homo_Random    4     89   22.34   0.493  0.741\nResiduals         7300 330833   45.32               \n\n# Levene’s test: check variance homogeneity among groups\nleveneTest(y = xts_Component |&gt; as.numeric(), group = group_Homo_Random)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value Pr(&gt;F)\ngroup    4  0.0849 0.9871\n      7300               \n\n\nWhile random groupings typically show no meaningful differences, structured groupings (e.g., by year or region) can highlight systematic trends or changes in variance, which may indicate shifts in climate, instrumentation, or data quality.\nIn practical applications, especially when preparing data for machine learning or statistical modeling, it is often necessary to divide the dataset into separate parts (e.g., training and testing). Before doing so, we can check whether these parts are homogeneous in their statistical properties. The following example divides the dataset into two halves and tests whether their means and variances differ significantly:\nWhen dividing a dataset into training and testing subsets (as commonly done in machine learning), it is important to ensure that both subsets are statistically homogeneous. If the two parts of the dataset differ significantly in their mean or variance, the trained model may not generalize well to unseen data — a problem known as data leakage or sampling bias.\nBy performing homogeneity tests (such as the t-test or Levene’s test) before splitting or after sampling, we can verify that both parts of the dataset come from similar distributions. This ensures that the model learns general patterns rather than artifacts caused by unequal group characteristics.",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#seasonality-based-on-daily-data-with-different-resolutions",
    "href": "dataprocess/timeserises_analyse.html#seasonality-based-on-daily-data-with-different-resolutions",
    "title": "Time Series Analyse",
    "section": "4.1 Seasonality based on daily data with different resolutions",
    "text": "4.1 Seasonality based on daily data with different resolutions\nIn this part, we remove the long-term trend and extract the repeating seasonal components at different temporal resolutions (daily, weekly, and monthly).\n\n# Remove the long-term LOESS trend to isolate seasonal fluctuations\nxts_Detrend &lt;- xts_Component - trend_Loess100\n\n# Aggregate to monthly and weekly mean values for coarser seasonal patterns\nxts_Detrend_Month &lt;- apply.monthly(xts_Detrend, colMeans)\nxts_Detrend_Week &lt;- apply.weekly(xts_Detrend, colMeans)\n\n# Extract numeric and temporal indices\nnum_Detrend &lt;- as.numeric(xts_Detrend)\ntime_index &lt;- index(xts_Component)\nseason_Day_Index &lt;- as.numeric(format(time_index, \"%j\"))   # day of year (1–365)\nseason_Week_Index &lt;- as.numeric(format(time_index, \"%V\"))  # week number (1–53)\nseason_Month_Index &lt;- as.numeric(format(time_index, \"%m\")) # month (1–12)\n\n# Compute mean seasonal cycles for each resolution\nseason_Day &lt;- ave(num_Detrend, season_Day_Index, FUN = mean, na.rm = TRUE)\nseason_Week &lt;- ave(num_Detrend, season_Week_Index, FUN = mean, na.rm = TRUE)\nseason_Month &lt;- ave(num_Detrend, season_Month_Index, FUN = mean, na.rm = TRUE)\n\nThe resulting seasonal cycles help visualize how the detrended temperature data fluctuate across different time scales.\n\n\nCode\n# Combine daily, weekly, and monthly seasonal signals into one data frame\n\ndf_Season &lt;- data.frame(\nDate = time_index,\nDay = season_Day,\nWeek = season_Week,\nMonth = season_Month\n) |&gt; filter(Date &gt;= as.Date(\"2023-01-01\"), Date &lt;= as.Date(\"2024-12-31\"))\n\n# Reshape to long format for ggplot visualization\n\ndf_Season_Long &lt;- df_Season |&gt;\nselect(Date, Day, Week, Month) |&gt;\npivot_longer(cols = c(Day, Week, Month), names_to = \"Seasonality\", values_to = \"Value\")\n\n# Plot different seasonal resolutions over time\n\ngp_Season1 &lt;- ggplot(df_Season_Long, aes(x = Date, y = Value, color = Seasonality)) +\ngeom_line() +\nlabs(\ntitle = \"Seasonality Component (2023–2024)\",\nx = \"Date\",\ny = \"Temperature (°C)\",\ncolor = \"\"\n) +\ntheme(\nlegend.position = \"top\",\nlegend.title = element_blank()\n)\n\nggplotly(gp_Season1)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#seasonality-with-smoothing",
    "href": "dataprocess/timeserises_analyse.html#seasonality-with-smoothing",
    "title": "Time Series Analyse",
    "section": "4.2 Seasonality with smoothing",
    "text": "4.2 Seasonality with smoothing\nShort-term noise can obscure seasonal patterns. To clarify them, a rolling mean (7-day window) is applied to smooth the daily seasonality.\n\n# Convert to xts and apply 7-day rolling mean to smooth short-term variability\nxts_Season &lt;- as.xts(df_Season)\nxts_Season_Roll &lt;- rollmean(xts_Season, 7)\ndf_Season_Roll &lt;- as.data.frame(xts_Season_Roll)\ndf_Season_Roll$Date &lt;- index(xts_Season_Roll)\n\n\n\nCode\n# Reshape smoothed seasonal data for plotting\n\ndf_Season_Roll_Long &lt;- df_Season_Roll |&gt;\nselect(Date, Day, Week, Month) |&gt;\npivot_longer(cols = c(Day, Week, Month), names_to = \"Season_Rollality\", values_to = \"Value\")\n\n# Plot smoothed (rolling mean) seasonal components\n\ngp_Season2 &lt;- ggplot(df_Season_Roll_Long, aes(x = Date, y = Value, color = Season_Rollality)) +\ngeom_line() +\nlabs(\ntitle = \"Season_Rollality Component (2023–2024)\",\nx = \"Date\",\ny = \"Temperature (°C)\",\ncolor = \"\"\n) +\ntheme(\nlegend.position = \"top\",\nlegend.title = element_blank()\n)\nggplotly(gp_Season2)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#seasonality-based-on-different-resolutions",
    "href": "dataprocess/timeserises_analyse.html#seasonality-based-on-different-resolutions",
    "title": "Time Series Analyse",
    "section": "4.3 Seasonality based on different resolutions",
    "text": "4.3 Seasonality based on different resolutions\nFinally, to compare the influence of aggregation levels, we compute and visualize seasonal cycles derived from monthly and weekly means. Aggregating data changes the apparent smoothness and can affect the detectability of seasonal components.\n\n# Aggregate to monthly and weekly means\nxts_Detrend_Month &lt;- apply.monthly(xts_Detrend, colMeans, na.rm = TRUE)\nxts_Detrend_Week  &lt;- apply.weekly(xts_Detrend, colMeans, na.rm = TRUE)\n\n# Adjust indices to represent middle of month or week\nindex(xts_Detrend_Month) &lt;- as.Date(format(index(xts_Detrend_Month), \"%Y-%m-15\"))\nindex(xts_Detrend_Week) &lt;- index(xts_Detrend_Week) - 3  # approximate mid-week\n\n# Compute mean seasonality at each resolution\nidx_Month_Middle &lt;- as.numeric(format(index(xts_Detrend_Month), \"%m\"))\nidx_Week_Middle  &lt;- as.numeric(format(index(xts_Detrend_Week), \"%V\"))\n\nseason_MonthMean &lt;- ave(as.numeric(coredata(xts_Detrend_Month)), idx_Month_Middle, FUN = mean, na.rm = TRUE)\nseason_WeekMean  &lt;- ave(as.numeric(coredata(xts_Detrend_Week)), idx_Week_Middle, FUN = mean, na.rm = TRUE)\n\nThese comparisons illustrate how temporal resolution affects the perception of periodic patterns. Monthly averages emphasize long-term seasonal cycles, while weekly data preserve more short-term variations within the broader annual trend.\n\n\nCode\n# Combine and visualize monthly vs weekly seasonality\n\ndf_PeriodMean &lt;- rbind(\ndata.frame(Date = index(xts_Detrend_Month), Seasonality = season_MonthMean, Type = \"Monthly\"),\ndata.frame(Date = index(xts_Detrend_Week),  Seasonality = season_WeekMean,  Type = \"Weekly\")\n) |&gt; filter(Date &gt;= as.Date(\"2023-01-01\"), Date &lt;= as.Date(\"2024-12-31\"))\n\ngp_Season3 &lt;- ggplot(df_PeriodMean, aes(x = Date, y = Seasonality, color = Type)) +\ngeom_line() +\nlabs(\ntitle = \"Weekly and Monthly Seasonality (2023–2024)\",\nx = \"Date\",\ny = \"Temperature (°C)\",\ncolor = \"\"\n) +\ntheme(\nlegend.position = \"top\",\nlegend.title = element_blank()\n)\nggplotly(gp_Season3)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#autocorrelation-function-acf",
    "href": "dataprocess/timeserises_analyse.html#autocorrelation-function-acf",
    "title": "Time Series Analyse",
    "section": "5.1 Autocorrelation Function (ACF)",
    "text": "5.1 Autocorrelation Function (ACF)\nThe Autocorrelation Function (ACF) shows the correlation between a time series and its lagged values.\nMathematically, for a time series \\(x_t\\) of length \\(n\\), the autocorrelation at lag \\(k\\) is defined as:\n\\[\n\\rho_k = \\frac{\\text{Cov}(x_t, x_{t+k})}{\\text{Var}(x_t)} = \\frac{\\sum_{t=1}^{n-k} (x_t - \\bar{x})(x_{t+k} - \\bar{x})}{\\sum_{t=1}^{n} (x_t - \\bar{x})^2}\n\\]\nwhere \\(\\bar{x}\\) is the mean of the series.\nHow to read the ACF plot:\n\nThe x-axis shows the lag (number of time steps).\n\nThe y-axis shows the correlation at each lag.\n\nThe horizontal dashed lines indicate the 95% confidence interval (\\(\\frac{1.96}{\\sqrt{n}}\\)). Values outside these lines are usually considered statistically significant autocorrelations.\n\nA slowly decaying ACF suggests a strong trend or persistent correlation.\n\nA spike at a specific lag may indicate seasonality or periodic patterns.\n\nIf the ACF drops quickly to zero, the series is likely random or has weak temporal dependence.\n\n\nnum_ACF &lt;- acf(coredata(xts_Component), 400, plot = FALSE)\ndf_ACF &lt;- with(num_ACF, data.frame(lag = lag, acf = acf))\ngp_ACF &lt;- ggplot(df_ACF, aes(x = lag, y = acf)) +\n  geom_segment(aes(x = lag, xend = lag, y = 0, yend = acf), color = \"#17365c\") +\n  geom_hline(yintercept = 0, color = \"black\") +\n  geom_hline(yintercept = c(2, -2) / sqrt(length(xts_Component)), \n             linetype = \"dashed\", color = \"#EC008D\") +\n  labs(title = \"Autocorrelation Function (ACF)\",\n       x = \"Lag\",\n       y = \"ACF\")\nggplotly(gp_ACF)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#partial-autocorrelation-function-pacf",
    "href": "dataprocess/timeserises_analyse.html#partial-autocorrelation-function-pacf",
    "title": "Time Series Analyse",
    "section": "5.2 Partial Autocorrelation Function (PACF)",
    "text": "5.2 Partial Autocorrelation Function (PACF)\nThe Partial Autocorrelation Function (PACF) measures the correlation between \\(x_t\\) and \\(x_{t+k}\\) after removing the effects of all intermediate lags \\(1, 2, ..., k-1\\).\nThe Partial Autocorrelation Function (PACF) is similar to the ACF but shows the correlation between the series and its lagged values after removing the effect of shorter lags.\nIn other words, the PACF tells us the direct influence of a specific lag on the current value.\nThe PACF plot is very useful to identify the order of autoregressive (AR) processes in time series models.\nFor example, if the PACF cuts off after lag 1, it indicates that an AR(1) model might describe the data well.\nTogether with the ACF, the PACF gives a complete picture of the temporal dependence structure of the dataset.\nHow to read the PACF plot:\n\nThe x-axis shows the lag, and the y-axis shows the partial correlation.\n\nThe horizontal dashed lines indicate the 95% confidence interval. Values outside are statistically significant.\n\nA sharp cutoff after lag \\(p\\) indicates an AR(\\(p\\)) process.\n\nSignificant spikes at multiple lags may suggest the series requires a more complex model, such as ARMA.\n\nPACF helps distinguish the direct influence of a lag from indirect effects through shorter lags.\n\nBy examining both the ACF and PACF plots together, we can identify trends, seasonality, and the appropriate order of autoregressive and moving average models for time series analysis.\n\nnum_PACF &lt;- pacf(coredata(xts_Component), 400, plot = FALSE)\ndf_PACF &lt;- with(num_PACF, data.frame(lag = lag, acf = acf))\ngp_PACF &lt;- ggplot(df_PACF, aes(x = lag, y = acf)) +\n  geom_segment(aes(x = lag, xend = lag, y = 0, yend = acf), color = \"#17365c\") +\n  geom_hline(yintercept = 0, color = \"black\") +\n  geom_hline(yintercept = c(2, -2) / sqrt(length(xts_Component)), \n             linetype = \"dashed\", color = \"#EC008D\") +\n  labs(title = \"Autocorrelation Function (ACF)\",\n       x = \"Lag\",\n       y = \"PACF\")\nggplotly(gp_PACF)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html",
    "href": "dataprocess/timeserises_visual.html",
    "title": "Visualization",
    "section": "",
    "text": "Visualizing time series is crucial for identifying patterns, trends, and anomalies in data over time. Here are some key considerations and methods for visualizing time series data.\nIn this Artikel we will use the R package ggplot (tidyverse) for plotig and the results data fro HBV Light as the data:\n\n# Library\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(ggh4x) # difference area\nlibrary(reshape2)\n\n# File name\nfn_ResultsHBV &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/tbl_HBV_Results.txt\"\n# Load Data\ndf_ResultHBV &lt;- read_table(fn_ResultsHBV)\n\n# Convert Date column to a Date type\ndf_ResultHBV$Date &lt;- as_date(df_ResultHBV$Date |&gt; as.character(), format = \"%Y%m%d\")\n\nidx_1979 &lt;- which(df_ResultHBV$Date &gt;= as_date(\"1979-01-01\") & df_ResultHBV$Date &lt;= as_date(\"1979-12-31\"))\ndf_Plot &lt;- df_ResultHBV[idx_1979, ]",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#basic-line",
    "href": "dataprocess/timeserises_visual.html#basic-line",
    "title": "Visualization",
    "section": "2.1 Basic Line",
    "text": "2.1 Basic Line\n\nggplot(df_Plot, aes(x = Date)) +\n  geom_line(aes(y = Qobs, color = \"Obs.\")) +\n  geom_line(aes(y = Qsim, color = \"Sim.\"))",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#line-plot-with-shaded-difference-area",
    "href": "dataprocess/timeserises_visual.html#line-plot-with-shaded-difference-area",
    "title": "Visualization",
    "section": "2.2 Line Plot with Shaded Difference Area",
    "text": "2.2 Line Plot with Shaded Difference Area\n\nggplot(df_Plot, aes(x = Date)) +\n  geom_line(aes(y = Qobs, color = \"Obs.\")) +\n  geom_line(aes(y = Qsim, color = \"Sim.\")) +\n  stat_difference(aes(ymin = Qsim, ymax = Qobs), alpha = .5)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#line-cluster",
    "href": "dataprocess/timeserises_visual.html#line-cluster",
    "title": "Visualization",
    "section": "2.3 Line Cluster",
    "text": "2.3 Line Cluster\n\n# Melting the data for ggplot\ndf_Plot_Melt &lt;- reshape2::melt(df_Plot[,c(\"Date\", \"Qsim\", \"Precipitation\", \"AET\")], id = \"Date\")\n# Plot\nggplot(df_Plot_Melt, aes(x = Date, y = value, color = variable, group = variable)) +\n  geom_line()",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#basic-bar",
    "href": "dataprocess/timeserises_visual.html#basic-bar",
    "title": "Visualization",
    "section": "3.1 Basic Bar",
    "text": "3.1 Basic Bar\n\nggplot(df_Plot, aes(x = Date)) +\n  geom_col(aes(y = Precipitation))",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#stacked-bar",
    "href": "dataprocess/timeserises_visual.html#stacked-bar",
    "title": "Visualization",
    "section": "3.2 Stacked Bar",
    "text": "3.2 Stacked Bar\n\n# Snowfall and rain fall caculate\ndf_Plot &lt;- df_Plot |&gt; mutate(Snowfall = pmax(0, c(0, diff(Snow))), \n                             Rainfall = Precipitation - Snowfall) \ndf_Plot_Melt2 &lt;- reshape2::melt(df_Plot[1:120, c(\"Date\", \"Snowfall\", \"Rainfall\")], id = \"Date\")\n# Plot\nggplot(df_Plot_Melt2, aes(x = Date, y = value, fill = variable, group = variable)) +\n  geom_col(position=\"stack\")",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#dodge",
    "href": "dataprocess/timeserises_visual.html#dodge",
    "title": "Visualization",
    "section": "3.3 Dodge",
    "text": "3.3 Dodge\n\n# Plot\nggplot(df_Plot_Melt2, aes(x = Date, y = value, fill = variable, group = variable)) +\n  geom_col(position=\"dodge\")",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#stacked-area",
    "href": "dataprocess/timeserises_visual.html#stacked-area",
    "title": "Visualization",
    "section": "6.1 Stacked Area",
    "text": "6.1 Stacked Area\nA stacked area plot is a type of data visualization that displays the cumulative contribution of different groups to a total over time or another continuous variable. Each group’s contribution is represented as a colored area, and these areas are stacked on top of each other.\n\n# Data\nmelt_Balance_Q &lt;- df_Plot[,c(\"Date\", \"AET\", \"Q0\", \"Q1\", \"Q2\" )] |&gt; melt(id = \"Date\")\n# Plot\nggplot(melt_Balance_Q, aes(Date, value, fill = variable)) +\n  geom_area()",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#percent-area",
    "href": "dataprocess/timeserises_visual.html#percent-area",
    "title": "Visualization",
    "section": "6.2 Percent Area",
    "text": "6.2 Percent Area\nA percent stacked area plot is a variation of the stacked area plot where the y-axis represents percentages, showcasing the proportion of each group relative to the total at each point in time. This type of plot is particularly useful when you want to emphasize the relative distribution of different groups over time.\n\n# Data\nmelt_Balance_Q$perc &lt;- melt_Balance_Q$value / rowSums(df_Plot[,c(\"AET\", \"Q0\", \"Q1\", \"Q2\" )])\n# PLot\nggplot(melt_Balance_Q, aes(Date, perc, fill = variable)) +\n  geom_area()",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/visual_plotElements.html",
    "href": "dataprocess/visual_plotElements.html",
    "title": "The elements of a plot",
    "section": "",
    "text": "At the beginning of plotting, we can first examine the elements of a plot that provide an overview. These include the background, panel, title, and axes. While they may not be as visually prominent as the main plot elements (points or lines), they are crucial for extracting meaningful information from a plot.\nIn this section, we will leverage concepts from ggplot2. These concepts are fundamental not only to ggplot2 but also to other plotting engines. This article will primarily focus on the section on Theme Elements in the book “ggplot2: Elegant Graphics for Data Analysis (3e)” by Hadley Wickham (2009).\nUnder ggplot2, these elements are divided into five groups: plot, axis, legend, panel, and facet. However, for a single plot, we will not delve into the fifth facet group.\n\nTo illustrate these elements, we will use a random dataset to create a scatter plot as an example.\n\n\n\n\n\n\nImportant\n\n\n\nThis article will focus on illustrating the elements, while the techniques for manipulating and changing the content of these elements will be covered in a separate article. With this article, you can get a basic impression of the plot.\n\n\n\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Create a sample dataset\ndata &lt;- data.frame(\n  x = rnorm(50),\n  y = rnorm(50),\n  group = rep(c(\"A\", \"B\"), each = 25)\n)\n\nThe original plot appears as follows:\n\ngp_Test &lt;- ggplot(data, aes(x, y, color = group)) +\n  geom_point()\n\ngp_Test\n\n\n\n\nOriginal\n\n\n\n\n\n1 Plot elements\nThe “plot” represents the entire plot, basically, defining the background on which all other elements are drawn. There are three main elements related to the plot:\n\nplot.background: background rectangle area\nplot.title: title for the whole plot\nplot.margin: margins around the plot\n\n\nplot.backgroundplot.titleplot.margin\n\n\n\ngp_Test + \n  theme(plot.background = element_rect(fill = \"red\"))\n\n\n\n\nBackground in red\n\n\n\n\n\n\n\ngp_Test + \n  ggtitle(\"Title in red\") +\n  theme(plot.title = element_text(color = \"red\"))\n\n\n\n\nTitle in red\n\n\n\n\n\n\n\ngp_Test + \n  theme(plot.background = element_rect(fill = \"red\"),\n        plot.margin = margin(10,10,10,10, \"mm\"))\n\n\n\n\nBackground in red and margin in 10 mm\n\n\n\n\n\n\n\n\n\n2 Axis elements\nThe “axis” in a plot provides a crucial reference for interpreting the data or a scale for measurement. It consists of tick marks, labels, and a title. The axis allows viewers to understand the quantitative values represented in the plot, aiding in data analysis and visualization.\nThere are four main elements related to the axis:\n\naxis.line: line parallel to axis\naxis.text: tick labels (axis.text.x, axis.text.y)\naxis.title: axis titles (axis.title.x, axis.title.y)\naxis.ticks: axis tick marks\n\naxis.ticks.length: length of tick marks\n\n\n\naxis.lineaxis.textaxis.titleaxis.ticks\n\n\n\ngp_Test + \n  theme(axis.line = element_line(color = \"red\", linewidth = 2))\n\n\n\n\nAxis line in red, line width in 2\n\n\n\n\n\n\n\ngp_Test + \n  theme(axis.text = element_text(color = \"red\", size = 15))\n\n\n\n\nTick labels in red and font size in 15\n\n\n\n\n\n\n\ngp_Test + \n  theme(axis.title = element_text(color = \"red\", size = 15))\n\n\n\n\nAxis titles in red and font size in 15\n\n\n\n\n\n\n\ngp_Test + \n  theme(axis.ticks = element_line(color = \"red\", linewidth = 2),\n        axis.ticks.length = unit(2, \"mm\"))\n\n\n\n\nAxis tick marks in red, line width in 2, length in 2 mm\n\n\n\n\n\n\n\n\n\n3 Legend elements\nThe “legend” elements control the appearance of all legends. You can also modify the appearance of individual legends by modifying the same elements in guide_legend() or guide_colourbar() (Wickham 2009).\nThere are four main elements related to the legend:\n\nlegend.background: legend background\n\nlegend.margin: legend margin\n\nlegend.key: background of legend keys\n\nlegend.key.size: legend key size\nlegend.key.height: legend key height\nlegend.key.width: legend key width\n\nlegend.text: legend labels\nlegend.title: legend name\n\n\nlegend.backgroundlegend.keylegend.textlegend.title\n\n\n\ngp_Test + \n  theme(legend.background = element_rect(fill = \"red\"),\n        legend.margin = margin(10,10,10,10, \"mm\"))\n\n\n\n\nLegend background in red and margin in 10 mm\n\n\n\n\n\n\n\ngp_Test + \n  theme(legend.key = element_rect(fill = \"red\"),\n        legend.key.size = unit(10, \"mm\"))\n\n\n\n\nBackground of legend keys in red and legend keys size in 10 mm\n\n\n\n\n\n\n\ngp_Test + \n  theme(legend.text = element_text(color = \"red\", size = 15))\n\n\n\n\nLegend labels in red and font size in 15\n\n\n\n\n\n\n\ngp_Test + \n  theme(legend.title = element_text(color = \"red\", size = 15))\n\n\n\n\nLegend name in red and font size in 15\n\n\n\n\n\n\n\n\n\n4 Panel elements\nThe “panel” in a plot is the central area where the main data representation.\nThere are four main elements related to the panel:\n\npanel.background: panel background (under data)\npanel.border: panel border (over data)\npanel.grid.major (panel.grid.minor): major / minor grid lines\naspect.ratio: plot aspect ratio\n\n\npanel.backgroundpanel.borderpanel.gridaspect.ratio\n\n\n\ngp_Test + \n  theme(panel.background = element_rect(fill = \"red\"))\n\n\n\n\nLegend background in red and margin in 10 mm\n\n\n\n\n\n\n\ngp_Test + \n  theme(panel.border = element_rect(color = \"red\", fill = NA, linewidth = 2))\n\n\n\n\nPanel border in red and line width in 2\n\n\n\n\nIf the fill parameter is not set to NA (transparent), it will cover the main plot:\n\ngp_Test + \n  theme(panel.border = element_rect(color = \"red\", fill = \"green\", linewidth = 2))\n\n\n\n\nPanel border in red and line width in 2, but the area fill in green\n\n\n\n\n\n\n\ngp_Test + \n  theme(panel.grid.major = element_line(color = \"red\", linewidth = 2))\n\n\n\n\nMajor grid lines in red and line width in 15\n\n\n\n\n\n\n\ngp_Test + \n  theme(aspect.ratio = 2)\n\n\n\n\nAspect ratio in 1\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nWickham, Hadley. 2009. Ggplot2: Elegant Graphics for Data Analysis. New York, NY: Springer. https://doi.org/10.1007/978-0-387-98141-3.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "The elements of a plot"
    ]
  },
  {
    "objectID": "dataset/hydro.html",
    "href": "dataset/hydro.html",
    "title": "Hydro",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/hydro.html#h2",
    "href": "dataset/hydro.html#h2",
    "title": "Hydro",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/meteo.html",
    "href": "dataset/meteo.html",
    "title": "Meteological",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/meteo.html#h2",
    "href": "dataset/meteo.html#h2",
    "title": "Meteological",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "mldl/index.html",
    "href": "mldl/index.html",
    "title": "ML / DL",
    "section": "",
    "text": "Hydrological modeling is a scientific approach aimed at simulating and comprehending the dynamics of the Earth’s water cycle. On this page, we will compile information on hydrological models, as well as the processes involved in their application. This includes running the model, evaluating its performance, calibrating and conducting sensitivity analyses, and ultimately validating the model’s results. Through this comprehensive exploration, we aim to provide valuable insights into the world of hydrological modeling.",
    "crumbs": [
      "ML / DL"
    ]
  },
  {
    "objectID": "mldl/ml_clustering.html",
    "href": "mldl/ml_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "library(tidymodels)\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(tidyclust)\nlibrary(embed)\n# library(cluster)\nlibrary(dbscan)\nlibrary(ggdendro)\nlibrary(patchwork)\nlibrary(ggforce)\nlibrary(reshape2)\n\n\ncolor_RUB_blue &lt;- \"#17365c\"\ncolor_RUB_green &lt;- \"#8dae10\"\ncolor_TUD_pink &lt;- \"#EC008D\"\ncolor_DRESDEN &lt;- c(\"#03305D\", \"#28618C\", \"#539DC5\", \"#84D1EE\", \"#009BA4\", \"#13A983\", \"#93C356\", \"#BCCF02\")\n\n\ndf_Substance &lt;- read.csv(\"../data_share/df_2010_bafg.csv\", row.names = 1)",
    "crumbs": [
      "ML / DL",
      "Clustering"
    ]
  },
  {
    "objectID": "mldl/ml_clustering.html#final-data",
    "href": "mldl/ml_clustering.html#final-data",
    "title": "Clustering",
    "section": "2.1 Final data",
    "text": "2.1 Final data\n\ndf_final &lt;- df_filtered[df_filtered$sampling_location != \"KARLSRUHE\", ]",
    "crumbs": [
      "ML / DL",
      "Clustering"
    ]
  },
  {
    "objectID": "mldl/ml_clustering.html#k-test",
    "href": "mldl/ml_clustering.html#k-test",
    "title": "Clustering",
    "section": "5.1 k test",
    "text": "5.1 k test\n\ndf_Kmeans_Elbow &lt;- tibble(k = 2:8, wss_value = NA)\ndf_Kmeans_Silhouette &lt;- tibble(\n  k = 2:8, \n  avg_silhouette = NA\n)\n\nfor (i in 2:8) {\n  set.seed(123)\n  fit_Temp &lt;- workflow() |&gt;\n    add_recipe(rcp_Clust_PCA) |&gt;\n    add_model(k_means(num_clusters = i) |&gt; set_engine(\"stats\")) |&gt;\n    fit(data = df_Clust_Substance)\n  \n  df_Kmeans_Elbow$wss_value[df_Kmeans_Elbow$k == i] &lt;- fit_Temp |&gt; \n    extract_fit_engine() |&gt; \n    pluck(\"tot.withinss\")\n  \n\n  clusters &lt;- fit_Temp |&gt; extract_cluster_assignment()\n  # Prepare PCA data\n\n  # Calculate silhouette scores\n  sil &lt;- cluster::silhouette(as.numeric(clusters$.cluster), dist(df_Clust_PCA))\n  \n  # Store average silhouette width\n  df_Kmeans_Silhouette$avg_silhouette[df_Kmeans_Silhouette$k == i] &lt;- \n    mean(sil[, 3])\n  \n}\n\n\n# Plot elbow curve\nggplot(df_Kmeans_Elbow, aes(x = k, y = wss_value)) +\n  geom_line(color = color_RUB_blue) +\n  geom_point(color = color_RUB_green) +\n  labs(title = \"Elbow Method\", \n       x = \"Number of Clusters (k)\", \n       y = \"Inertia (Within-Cluster Sum of Squares)\")\n\n\n\n\n\n\n\n\n\nggplot(df_Kmeans_Silhouette, aes(x = k, y = avg_silhouette)) +\n  geom_line(color = color_RUB_blue) +\n  geom_point(color = color_RUB_green) +\n  labs(\n    title = \"Average Silhouette Score\",\n    x = \"Number of Clusters (k)\",\n    y = \"Average Silhouette Width\"\n  )\n\n\n\n\n\n\n\n\n\n# Model specification\nmdl_Kmeans &lt;- k_means(num_clusters = 4)\n\n# Workflow\nwflow_Kmeans &lt;- workflow() |&gt;\n  add_recipe(rcp_Clust_PCA) |&gt;\n  add_model(mdl_Kmeans)\n\n# Fit the model\nfit_Kmeans &lt;- fit(wflow_Kmeans, data = df_Clust_Substance)\nfit_Kmeans$fit\n\n$actions\n$actions$model\n$spec\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = 4\n\nComputational engine: stats \n\n\n$formula\nNULL\n\nattr(,\"class\")\n[1] \"action_model\" \"action_fit\"   \"action\"      \n\n\n$fit\ntidyclust cluster object\n\nK-means clustering with 4 clusters of sizes 4, 7, 2, 3\n\nCluster means:\n        PC1       PC2       PC3\n4  2.006725 -1.106215 -1.226535\n1 -1.419340 -0.971521  1.020376\n3 -2.595506  1.650320 -3.148342\n2  2.366497  2.641622  1.353397\n\nClustering vector:\n [1] 1 2 2 2 1 3 2 2 3 2 2 1 1 4 4 4\n\nWithin cluster sum of squares by cluster:\n[1]  8.321554 21.836762  1.931344 10.228733\n (between_SS / total_SS =  76.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nattr(,\"class\")\n[1] \"stage_fit\" \"stage\"    \n\n# Extract cluster assignments\nclst_Kmeans &lt;- fit_Kmeans |&gt;\n  extract_cluster_assignment()\nclst_Kmeans_Centroids &lt;- fit_Kmeans |&gt; extract_centroids()\n\n\ndf_Kmeans_Plot &lt;- cbind(df_Clust_Plot_PCA, clst_Kmeans)\n# Scatter plot: PC1 vs PC2\ngp_Kmeans1 &lt;- ggplot(df_Kmeans_Plot, aes(x = PC2, y = PC1, color = .cluster, shape = substance)) +\n  geom_point(size = 2) +\n  scale_shape_manual(values = 1:length(unique(df_Clust_Plot_PCA$substance))) +\n  geom_point(data = clst_Kmeans_Centroids, \n             aes(x = PC2, y = PC1, color = .cluster), \n             shape = \"X\", size = 4) +\n  geom_mark_ellipse(aes(x = PC2, y = PC1, group = .cluster), alpha = 0.2, show.legend = FALSE) +\n  labs(x = \"PC2\", y = \"PC1\")\n\n# Scatter plot: PC1 vs PC3\ngp_Kmeans2 &lt;- ggplot(df_Kmeans_Plot, aes(x = PC3, y = PC1, color = .cluster, shape = substance)) +\n  geom_point(size = 2) +\n  scale_shape_manual(values = 1:length(unique(df_Clust_Plot_PCA$substance))) +\n  geom_point(data = clst_Kmeans_Centroids, \n             aes(x = PC3, y = PC1, color = .cluster), \n             shape = \"X\", size = 4) +\n  geom_mark_ellipse(aes(x = PC3, y = PC1, group = .cluster), alpha = 0.2, show.legend = FALSE) +\n  labs(x = \"PC3\", y = \"PC1\")\ngp_Kmeans1 + (gp_Kmeans2 + theme(axis.text.y = element_blank(),\n                                 axis.title.y = element_blank())) + plot_layout(guides = \"collect\")",
    "crumbs": [
      "ML / DL",
      "Clustering"
    ]
  },
  {
    "objectID": "mldl/ml_clustering.html#k-test-1",
    "href": "mldl/ml_clustering.html#k-test-1",
    "title": "Clustering",
    "section": "9.1 k test",
    "text": "9.1 k test\n\ndf_Kmeans_Elbow_UMAP &lt;- tibble(k = 2:8, wss_value = NA)\ndf_Kmeans_Silhouette_UMAP &lt;- tibble(\n  k = 2:8, \n  avg_silhouette = NA\n)\n\nfor (i in 2:8) {\n  set.seed(666)\n  fit_Temp &lt;- workflow() |&gt;\n    add_recipe(rcp_Clust_UMAP) |&gt;\n    add_model(k_means(num_clusters = i) |&gt; set_engine(\"stats\")) |&gt;\n    fit(data = df_Clust_Substance)\n  \n  df_Kmeans_Elbow_UMAP$wss_value[df_Kmeans_Elbow$k == i] &lt;- fit_Temp |&gt; \n    extract_fit_engine() |&gt; \n    pluck(\"tot.withinss\")\n  \n\n  clusters &lt;- fit_Temp |&gt; extract_cluster_assignment()\n  # Prepare UMAPA data\n  \n  # Calculate silhouette scores\n  sil &lt;- cluster::silhouette(as.numeric(clusters$.cluster), dist(df_Clust_UMAP))\n  \n  # Store average silhouette width\n  df_Kmeans_Silhouette_UMAP$avg_silhouette[df_Kmeans_Silhouette$k == i] &lt;- \n    mean(sil[, 3])\n  \n}\n\n\n# Plot elbow curve\nggplot(df_Kmeans_Elbow_UMAP, aes(x = k, y = wss_value)) +\n  geom_line(color = color_RUB_blue) +\n  geom_point(color = color_RUB_green) +\n  labs(title = \"Elbow Method\", \n       x = \"Number of Clusters (k)\", \n       y = \"Inertia (Within-Cluster Sum of Squares)\")\n\n\n\n\n\n\n\n\n\nggplot(df_Kmeans_Silhouette_UMAP, aes(x = k, y = avg_silhouette)) +\n  geom_line(color = color_RUB_blue) +\n  geom_point(color = color_RUB_green) +\n  labs(\n    title = \"Average Silhouette Score\",\n    x = \"Number of Clusters (k)\",\n    y = \"Average Silhouette Width\"\n  )\n\n\n\n\n\n\n\n\n\n# Model specification\nmdl_Kmeans &lt;- k_means(num_clusters = 4)\n\n# Workflow\nwflow_Kmeans &lt;- workflow() |&gt;\n  add_recipe(rcp_Clust_UMAP) |&gt;\n  add_model(mdl_Kmeans)\n\n# Fit the model\nfit_Kmeans_UMAP &lt;- fit(wflow_Kmeans, data = df_Clust_Substance)\nfit_Kmeans_UMAP$fit\n\n$actions\n$actions$model\n$spec\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = 4\n\nComputational engine: stats \n\n\n$formula\nNULL\n\nattr(,\"class\")\n[1] \"action_model\" \"action_fit\"   \"action\"      \n\n\n$fit\ntidyclust cluster object\n\nK-means clustering with 4 clusters of sizes 3, 4, 4, 5\n\nCluster means:\n       UMAP1      UMAP2      UMAP3\n3 -1.1843612  0.1698604  0.9132638\n1 -0.1933817 -0.6562004 -1.1678360\n4  0.9746621 -0.7275874  0.2550662\n2  0.3135469  0.9582541  0.3832226\n\nClustering vector:\n [1] 1 2 3 3 4 2 3 3 2 4 2 4 4 1 1 4\n\nWithin cluster sum of squares by cluster:\n[1] 1.830289 3.343100 3.324157 5.521436\n (between_SS / total_SS =  64.9 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nattr(,\"class\")\n[1] \"stage_fit\" \"stage\"    \n\n# Extract cluster assignments\nclst_Kmeans_UMAP &lt;- fit_Kmeans_UMAP |&gt;\n  extract_cluster_assignment()\nclst_Kmeans_Centroids_UMAP &lt;- fit_Kmeans_UMAP |&gt; extract_centroids()\n\n\ndf_Kmeans_Plot_UMAP &lt;- cbind(df_Clust_Plot_UMAP, clst_Kmeans_UMAP)\n# Scatter plot: UMAP1 vs UMAP2\ngp_Kmeans1_UMAP &lt;- ggplot(df_Kmeans_Plot_UMAP, aes(x = UMAP2, y = UMAP1, color = .cluster, shape = substance)) +\n  geom_point(size = 2) +\n  scale_shape_manual(values = 1:length(unique(df_Clust_Plot_UMAP$substance))) +\n  geom_point(data = clst_Kmeans_Centroids_UMAP, \n             aes(x = UMAP2, y = UMAP1, color = .cluster), \n             shape = \"X\", size = 4) +\n  geom_mark_ellipse(aes(x = UMAP2, y = UMAP1, group = .cluster), alpha = 0.2, show.legend = FALSE) +\n  labs(x = \"UMAP2\", y = \"UMAP1\")\n\n# Scatter plot: UMAP1 vs UMAP3\ngp_Kmeans2_UMAP &lt;- ggplot(df_Kmeans_Plot_UMAP, aes(x = UMAP3, y = UMAP1, color = .cluster, shape = substance)) +\n  geom_point(size = 2) +\n  scale_shape_manual(values = 1:length(unique(df_Clust_Plot_UMAP$substance))) +\n  geom_point(data = clst_Kmeans_Centroids_UMAP, \n             aes(x = UMAP3, y = UMAP1, color = .cluster), \n             shape = \"X\", size = 4) +\n  geom_mark_ellipse(aes(x = UMAP3, y = UMAP1, group = .cluster), alpha = 0.2, show.legend = FALSE) +\n  labs(x = \"UMAP3\", y = \"UMAP1\")\ngp_Kmeans1_UMAP + (gp_Kmeans2_UMAP + theme(axis.text.y = element_blank(),\n                                 axis.title.y = element_blank())) + plot_layout(guides = \"collect\")",
    "crumbs": [
      "ML / DL",
      "Clustering"
    ]
  },
  {
    "objectID": "mldl/ml_supervised.html",
    "href": "mldl/ml_supervised.html",
    "title": "Feature Engineering and Selection",
    "section": "",
    "text": "library(tidymodels)\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\ncolor_RUB_blue &lt;- \"#17365c\"\ncolor_RUB_green &lt;- \"#8dae10\"\ncolor_TUD_pink &lt;- \"#EC008D\"\n\ncolor_DRESDEN &lt;- c(\"#03305D\", \"#28618C\", \"#539DC5\", \"#84D1EE\", \"#009BA4\", \"#13A983\", \"#93C356\", \"#BCCF02\")\n\n\ndf_Bochum_KL &lt;- read_csv(\"../data_share/df_Bochum_KL.csv\")\n# Train–test split (75% / 25%)\nsplit_Bochum &lt;- initial_split(df_Bochum_KL, prop = 3/4)\ndf_Train &lt;- training(split_Bochum)\ndf_Test  &lt;- testing(split_Bochum)\n\n\n# Preprocessing recipe (normalize + PCA)\n\nrcp_Norm &lt;- \n  recipe(evapo_r ~ ., data = df_Train) |&gt;\n  step_YeoJohnson(all_predictors()) |&gt; \n  step_normalize(all_predictors())"
  },
  {
    "objectID": "mldl/ml_supervised.html#grid-search",
    "href": "mldl/ml_supervised.html#grid-search",
    "title": "Feature Engineering and Selection",
    "section": "2.1 Grid Search",
    "text": "2.1 Grid Search\nGrid search is when we predefine a set of parameter values to evaluate. The main choices involved in grid search are how to make the grid and how many parameter combinations to evaluate. Grid search is often judged as inefficient since the number of grid points required to cover the parameter space can become unmanageable with the curse of dimensionality. There is truth to this concern, but it is most true when the process is not optimized. This is discussed more in Chapter 13.\nIterative search or sequential search is when we sequentially discover new parameter combinations based on previous results. Almost any nonlinear optimization method is appropriate, although some are more efficient than others. In some cases, an initial set of results for one or more parameter combinations is required to start the optimization process. Iterative search is discussed more in Chapter 14."
  },
  {
    "objectID": "mldl/ml_supervised.html#hyperparameter",
    "href": "mldl/ml_supervised.html#hyperparameter",
    "title": "Feature Engineering and Selection",
    "section": "3.1 Hyperparameter",
    "text": "3.1 Hyperparameter\nFor ordinary linear regression (using the lm engine), there are no hyperparameters to tune.\nHowever, when we introduce regularization (ridge, lasso, elastic net), the model includes an additional penalty term in the loss function to prevent overfitting.\nRegularization requires the model to control how strong the penalty is and what type of penalty is used.\nThese controls are called hyperparameters, and two of them appear:\n\npenalty (lambda): determines the overall strength of the regularization\n\nmixture (alpha): determines the type of regularization (0 = ridge, 1 = lasso, values in between = elastic net)\n\nWithout regularization (ordinary linear regression), no hyperparameters are needed.\nThese hyperparameters must usually be tuned using cross-validation.\nDifferent regression methods use different loss functions and regularization strategies.\nThe key difference is whether the model introduces hyperparameters (penalty and mixture) that control the amount and type of regularization.\n\n3.1.1 Ordinary Least Squares (OLS)\n\nNo regularization → no hyperparameters.\n\nThe solution is obtained analytically by minimizing the sum of squared residuals.\n\n\\[\n\\min_{\\beta} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nBecause there is no penalty term, the model can overfit when predictors are highly correlated.\n\n\n3.1.2 Ridge Regression (L2 penalty)\n\nIntroduces one hyperparameter:\n\npenalty (lambda): strength of coefficient shrinkage\n\n\nmixture is fixed to 0, meaning pure L2 regularization.\n\n\\[\n\\min_{\\beta} \\left(\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n+ \\lambda \\sum_{j=1}^p \\beta_j^2\n\\right)\n\\]\nLarge lambda → strong shrinkage.\nCoefficients approach zero but never become exactly zero.\n\n\n3.1.3 Lasso Regression (L1 penalty)\n\nAlso uses one hyperparameter:\n\npenalty (lambda): controls how strongly coefficients are pushed toward zero\n\n\nmixture is fixed to 1, meaning pure L1 regularization.\n\n\\[\n\\min_{\\beta} \\left(\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n+ \\lambda \\sum_{j=1}^p |\\beta_j|\n\\right)\n\\]\nLarge lambda → many coefficients become exactly zero → feature selection.\n\n\n3.1.4 Elastic Net Regression (combined L1 + L2)\n\nUses two hyperparameters:\n\npenalty (lambda): global strength of shrinkage\n\nmixture (alpha): balance between L1 and L2\n\nalpha = 0 → ridge\n\nalpha = 1 → lasso\n\n0 &lt; alpha &lt; 1 → mixture\n\n\n\n\\[\n\\min_{\\beta} \\left(\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n+ \\lambda \\left[\n\\alpha \\sum_{j=1}^p |\\beta_j|\n+ (1-\\alpha) \\sum_{j=1}^p \\beta_j^2\n\\right]\n\\right)\n\\]\nElastic Net is useful when predictors are correlated and when both shrinkage and feature selection are desired.\n\n\n3.1.5 Summary of Hyperparameter Settings\n\n\n\nMethod\npenalty (lambda)\nmixture (alpha)\nRegularization\n\n\n\n\nOLS\nnot used\nnot used\nnone\n\n\nRidge\nyes\n0\nL2\n\n\nLasso\nyes\n1\nL1\n\n\nElastic Net\nyes\n0 &lt; α &lt; 1\nL1 + L2\n\n\n\nIn practice, penalty and mixture are usually selected using cross-validation, since the optimal values depend on the dataset."
  },
  {
    "objectID": "modelling/basic_concept.html",
    "href": "modelling/basic_concept.html",
    "title": "Concept of Modelling",
    "section": "",
    "text": "Within the process of hydrological modeling, there are fundamental concepts that play a crucial role. In this article, we will elucidate these concepts through illustrative figures, enhancing your comprehension. These concepts can be categorized into two main parts: the Data part, which encompasses aspects related to input data, parameters, and parameter ranges, and the Process part, which outlines the workflow of the entire modeling process. By exploring these concepts and their visual representations, you’ll gain a deeper understanding of hydrological modeling.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#boundary-condition-forcing-data",
    "href": "modelling/basic_concept.html#boundary-condition-forcing-data",
    "title": "Concept of Modelling",
    "section": "1.1 Boundary Condition (Forcing Data)",
    "text": "1.1 Boundary Condition (Forcing Data)\nFor hydrological modeling, you need data that describes the boundary conditions or forcing factors affecting the model. They define how water and other related variables enter or exit the modeled domain. This includes information on precipitation, temperature, humidity, and other meteorological variables.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#initial-condition-data-warm-up-time",
    "href": "modelling/basic_concept.html#initial-condition-data-warm-up-time",
    "title": "Concept of Modelling",
    "section": "1.2 Initial Condition Data & Warm-up Time",
    "text": "1.2 Initial Condition Data & Warm-up Time\nInitial conditions represent the state of the watershed or catchment at the beginning of the simulation.\nHowever, in many cases, the exact initial conditions are unknown or difficult to measure accurately. To address this uncertainty, hydrological models incorporate a warm-up period. This warm-up period refers to the initial phase of model simulation where the model runs to establish a stable or equilibrium state before commencing the actual simulation.\nIt’s crucial to ensure that the warm-up period is of sufficient duration to reach a stable state. Typically, this period should span at least two complete cycles of the dominant hydrological processes within the watershed. This requirement ensures that the model has the opportunity to capture the full range of variability associated with these processes.\nThe periodicity of hydrological processes is a crucial consideration when implementing a warm-up period. By assuming that the initial state during warm-up has only a minimal influence, we rely on the repeated cycles of hydrological events. During the second cycle, the system tends to reach its maximum or minimum state, which is indicative of equilibrium. Subsequently, the simulation becomes stable, and this equilibrium state serves as the initial condition for the formal simulation.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#parameter",
    "href": "modelling/basic_concept.html#parameter",
    "title": "Concept of Modelling",
    "section": "1.3 Parameter",
    "text": "1.3 Parameter\nParameters are essential components of hydrological models for several reasons. Firstly, hydrological models cannot simulate the entire complexity of the real world, so parameters are used to represent various physical characteristics and processes. Secondly, real-world measurements are often limited in scope and may not capture all relevant data across the entire watershed. Parameters help bridge these gaps by allowing models to make predictions based on available information.\nThere are three main categories of hydrological models based on their use of parameters:\n\nPhysical-based models (white box): These models aim to simulate hydrological processes with a high level of detail. They rely on deterministic formulas and aim to represent physical processes as accurately as possible. Ideally, physical-based models do not require the use of parameters, as all processes are simulated with detailed data and deterministic equations. However, in practice, some level of parameterization may still be necessary, especially for processes that are not well understood or difficult to represent mathematically.\nConceptual models (gray box): Conceptual models are the most commonly used hydrological models. They strike a balance between complexity and simplicity. These models use parameters to represent various physical characteristics of the watershed, such as soil properties, land use, and drainage patterns. Calibration, a process of adjusting parameter values to match observed data, is typically needed to make the model’s predictions consistent with real-world conditions.\nEmpirical and data-driven models (black box): Empirical models rely heavily on observed data and may not explicitly represent physical processes. Instead, they use statistical relationships to make predictions. These models often require fewer parameters than physical-based or conceptual models but may still involve parameter estimation based on data.\n\n\n1.3.1 Initial Parameters\nInitial parameters are the starting values for model parameters that are often suggested based on previous research or prior knowledge. These initial parameter values are used to test the fundamental functionality of the model and its applicability to the study area. While these initial parameters may provide a reasonable starting point, they may or may not be a good fit for the specific study area and objectives.\n\n\n1.3.2 Parameter Range\nParameter ranges define the range of allowable values that model parameters can assume within specified bounds. While a broad parameter range can provide a greater opportunity to find optimal parameter values, it also expands the search space, making it more challenging to identify the best-fitting parameters.\nParameter ranges can be categorized into two main types:\n\nPhysical Range: This refers to the range of parameter values that are physically meaningful and are constrained by the fundamental characteristics of the system being modeled. For example, hydraulic conductivity in groundwater models cannot be negative, so it has a physical lower bound of zero. Physical range limits ensure that parameter values are consistent with the underlying physical processes.\nRegional Range: In addition to the physical limits, parameters may have regional variations based on the specific characteristics of the study area. These regional variations account for local geological, climatic, or land-use differences that influence parameter values. Regional ranges help to capture the heterogeneity within a larger study domain and allow for parameterization that reflects local conditions.\n\nBalancing the scope of parameter ranges is essential in hydrological modeling. While broader ranges offer flexibility, they also increase the complexity of parameter estimation. The challenge lies in finding a balance that allows for the exploration of diverse parameter values while ensuring that the model remains physically meaningful and regionally applicable.\n\n\n1.3.3 Calibrated Parameters\nCalibrated parameters are the parameter set that has been adjusted and fine-tuned during the calibration process. These parameters represent the current best-fit values for the hydrological model in a specific study area. They are chosen to optimize the model’s performance and ensure that it provides accurate predictions and simulations based on observed data.\n\n\n1.3.4 Validated Parameters\nValidated parameters are parameters that have been **verified* through comparison with observed data to ensure that the model accurately represents the real-world system.\n\n\n1.3.5 Parameter Mapping with Groups\nParameters in hydrological modeling vary based on the physical characteristics of the system being studied. However, in reality, different regions within a study area often exhibit distinct physical characteristics. Consequently, when performing simulations, it becomes necessary to calibrate parameters for each Hydrological Response Unit (HRU), which represents a homogeneous area within the study domain.\nCalibrating parameters for every HRU can be a formidable task, as it would involve a substantial number of individual calibrations. To address this challenge, researchers often employ a strategy of grouping HRUs with similar characteristics. By doing so, they can map parameters to these groups, effectively reducing the parameter calibration space.\nOne of the most commonly used grouping criteria includes categorizing HRUs based on factors such as land use, soil type, soil class, or climate zone. These characteristics often play a significant role in shaping the hydrological behavior of a region.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#model-running",
    "href": "modelling/basic_concept.html#model-running",
    "title": "Concept of Modelling",
    "section": "2.1 Model Running",
    "text": "2.1 Model Running\nThe core step in hydrological modeling involves running the model, which treats the model as a function that calculates a process with the input data to produce output data.\n\nThis process utilizes the provided boundary conditions, initial conditions, and parameter values to simulate the hydrological processes within the watershed or catchment. Boundary conditions and initial conditions are often collectively referred to as forcing data or input data.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#evaluation",
    "href": "modelling/basic_concept.html#evaluation",
    "title": "Concept of Modelling",
    "section": "2.2 Evaluation",
    "text": "2.2 Evaluation\nAfter the model run, an evaluation process is conducted to assess the performance of the model. This involves comparing the model’s simulated output to observed data or reference values. Various performance metrics and statistical measures are used to determine how well the model simulates real-world conditions.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#uncertainty-and-sensitivity",
    "href": "modelling/basic_concept.html#uncertainty-and-sensitivity",
    "title": "Concept of Modelling",
    "section": "2.3 Uncertainty and sensitivity",
    "text": "2.3 Uncertainty and sensitivity\nUncertainty and sensitivity analysis are crucial components of hydrological modeling, helping us understand the reliability of model predictions and the influence of different input parameters. Uncertainty analysis assesses the overall uncertainty in model outputs, considering various sources of uncertainty, while sensitivity analysis identifies which input parameters have the most significant impact on model results. Together, they provide valuable insights into the robustness and reliability of hydrological models.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#calibration",
    "href": "modelling/basic_concept.html#calibration",
    "title": "Concept of Modelling",
    "section": "2.4 Calibration",
    "text": "2.4 Calibration\nCalibration is a critical step in hydrological modeling. It involves adjusting the model’s parameters to improve its accuracy and alignment with observed data. Optimization techniques are often used to find parameter values that minimize the difference between model output and observed data.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#validation",
    "href": "modelling/basic_concept.html#validation",
    "title": "Concept of Modelling",
    "section": "2.5 Validation",
    "text": "2.5 Validation\nOnce the model has been calibrated, it is essential to validate its performance. Validation involves testing the calibrated model against independent datasets or data from a different time period. This step ensures that the model’s performance is not solely tailored to the calibration data but remains reliable for a broader range of conditions.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html",
    "href": "modelling/model_caliLinearReser.html",
    "title": "Calibration Prozess",
    "section": "",
    "text": "In this article, we will learn how to manage the entire hydrological modeling process (mehr Concept Details in Concept of Modelling) with the minimal model Linear-Reservoir model, including automatic calibration.",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#load-experimental-data",
    "href": "modelling/model_caliLinearReser.html#load-experimental-data",
    "title": "Calibration Prozess",
    "section": "4.1 Load Experimental Data",
    "text": "4.1 Load Experimental Data\nIn this phase, we will utilize experimental data from a labor experiment. This dataset involves the physical simulation of a linear reservoir and provides the measured inflow and outflow data in liters per second (L/h).\nThe labor data is available on GitHub.\n\n# Load Labor Data\ndf_Labor &lt;- read_delim(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/tbl_LaborMess_LinearReservior.txt\", delim = \"\\t\")\n\n# Rename the data, in order to more flexible Manipulation\nnames(df_Labor) &lt;- c(\"t\", \"QZ\", \"QA\")\n\n# The first 10 Line\nhead(df_Labor)\n\n# A tibble: 6 × 3\n      t    QZ    QA\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0    30\n2     1     0    30\n3     2     0    29\n4     3     0    29\n5     4     0    29\n6     5     0    29",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#mapping-from-concept-to-proceeding",
    "href": "modelling/model_caliLinearReser.html#mapping-from-concept-to-proceeding",
    "title": "Calibration Prozess",
    "section": "4.2 Mapping from Concept to Proceeding",
    "text": "4.2 Mapping from Concept to Proceeding\nTo provide clarity, let’s map the conceptual understanding to the simulation before proceeding:\n\n\n1.1 -&gt; Q_In = df_Labor$QZ\n1.2 -&gt; Q_Out0 = 0\n1.3 -&gt; param_K = 50\nf1 -&gt; model_linearReservoir()\n2 -&gt; df_Labor$QZ\nf2 -&gt; NSE(), KGE()",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#running-the-model-with-forcing-data",
    "href": "modelling/model_caliLinearReser.html#running-the-model-with-forcing-data",
    "title": "Calibration Prozess",
    "section": "4.3 Running the Model with Forcing Data",
    "text": "4.3 Running the Model with Forcing Data\nAs a preliminary test, we can suggest certain parameter values, such as \\(K\\) at 90 and 60. After the simulation, we will store the results in variables num_Q_Sim and num_Q_Sim2 for further analysis.\n\n# run the model\n\nnum_Q_Sim &lt;- model_linearReservoir(df_Labor$QZ, 0, param_K =  90)\n\nnum_Q_Sim2 &lt;- model_linearReservoir(df_Labor$QZ, 0, param_K =  60)",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#visual-evaluation",
    "href": "modelling/model_caliLinearReser.html#visual-evaluation",
    "title": "Calibration Prozess",
    "section": "4.4 Visual Evaluation",
    "text": "4.4 Visual Evaluation\nBefore employing quantitative criteria, it’s beneficial to visually evaluate the simulation results using time series plots, which provide an initial sense of the model’s performance.\n\n# Visual Evaluation\nggLabor &lt;- ggplot(df_Labor) +\n  geom_line(aes(t, QZ, color = \"Inflow\")) +\n  geom_line(aes(t, num_Q_Sim, color = \"Simul1\")) +\n  geom_line(aes(t, num_Q_Sim2, color = \"Simul2\")) +\n  geom_line(aes(t, QA, color = \"Observ\")) +\n  scale_color_manual(values = c(Inflow = \"cyan\", Simul1 = \"red\", Simul2 = \"orange\", Observ = \"blue\"))+\n  labs(x = \"Time [s]\", y = \"In-/Outflow [L/h]\", color = \"Flow\")\nggplotly(ggLabor)",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#quantitative-evaluation",
    "href": "modelling/model_caliLinearReser.html#quantitative-evaluation",
    "title": "Calibration Prozess",
    "section": "4.5 Quantitative Evaluation",
    "text": "4.5 Quantitative Evaluation\nFor short or simplified time series, visual evaluation may suffice. However, when dealing with long-term data, we require standardized criteria to objectively assess the model’s performance.\nNSE and KGE are the most commonly used criteria in hydrological research. However, there are also additional criteria available in the hydroGOF package (use ?hydroGOF):\nQuantitative statistics included are: Mean Error (me), Mean Absolute Error (mae), Root Mean Square Error (rms), Normalized Root Mean Square Error (nrms), Pearson product-moment correlation coefficient (r), Spearman Correlation coefficient (r.Spearman), Coefficient of Determination (R2), Ratio of Standard Deviations (rSD), Nash-Sutcliffe efficiency (NSE), Modified Nash-Sutcliffe efficiency (mNSE), Relative Nash-Sutcliffe efficiency (rNSE), Index of Agreement (d), Modified Index of Agreement (md), Relative Index of Agreement (rd), Coefficient of Persistence (cp), Percent Bias (pbias), Kling-Gupta efficiency (KGE), the coef. of determination multiplied by the slope of the linear regression between ‘sim’ and ‘obs’ (bR2), and volumetric efficiency (VE).\n\nNSE(num_Q_Sim, df_Labor$QA)\n\n[1] 0.9234607\n\nNSE(num_Q_Sim2, df_Labor$QA)\n\n[1] 0.8752109\n\nKGE(num_Q_Sim, df_Labor$QA)\n\n[1] 0.8425622\n\nKGE(num_Q_Sim2, df_Labor$QA)\n\n[1] 0.9273187",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#create-the-fit-function",
    "href": "modelling/model_caliLinearReser.html#create-the-fit-function",
    "title": "Calibration Prozess",
    "section": "5.1 Create the Fit Function",
    "text": "5.1 Create the Fit Function\nBefore proceeding with automatic calibration, an important step is to create a function that the calibration algorithm will use. This function should take the parameter to be calibrated as an input. Thus, we need to modify our model and evaluation function into a “Fit Function.”\n\neva_fit &lt;- function(model_Param,\n                     model_Input,\n                     Q_Observ,\n                     fct_gof = NSE) {\n  Q_Simu &lt;- model_linearReservoir(model_Input, param_K = model_Param)\n  \n  - fct_gof(Q_Simu, Q_Observ)\n  \n}\n\neva_fit(60, df_Labor$QZ, df_Labor$QA)\n\n[1] -0.8752109\n\n\nThere is another critical point to consider when creating the Fit Function. Calibration algorithms need to know which criteria is better. Most calibration algorithms compare the current criteria value with the previous one (or several previous ones) and consider the minimum (or maximum) criteria value as the best. However, in the case of NSE and KGE, a better simulation results in a higher value. To handle this, we should set these criteria as negative values. By doing so, calibration algorithms like cali_UVS() can work effectively with them.",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#calibrating",
    "href": "modelling/model_caliLinearReser.html#calibrating",
    "title": "Calibration Prozess",
    "section": "5.2 Calibrating",
    "text": "5.2 Calibrating\nWith the fit function in place, we can choose a calibration algorithm to optimize our model parameter (in this case, parameter \\(K\\)).\n\nlst_Cali &lt;- cali_UVS(eva_fit, x_Min = 40, x_Max = 90, model_Input = df_Labor$QZ, Q_Observ = df_Labor$QA, fct_gof = KGE)\n\n\n  |                                                                            \n  |                                                                      |   0%",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  }
]