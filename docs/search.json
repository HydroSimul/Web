[
  {
    "objectID": "modelling/model_linearReservoir.html",
    "href": "modelling/model_linearReservoir.html",
    "title": "Linear Reservoir",
    "section": "",
    "text": "Linear Reservoir is a method and just assuming that the watershed behaves like a linear reservoir, where the outflow is proportional to the water storage within the reservoir.\n\\[\nQ_{out} = \\frac{1}{K}S(t)\n\\tag{1}\\]\nIn addition to their relationship with output and storage, linear reservoir models also adhere to the continuity equation, often referred to as the water balance equation.\n\\[\n\\frac{\\mathrm{d}S(t)}{\\mathrm{d}t} = Q_{in} - Q_{out}\n\\tag{2}\\]\nBy combining both equations, we obtain a differential equation (DGL).\n\\[\nQ_{in} = Q_{out} + K\\frac{\\mathrm{d}Q_{out}(t)}{\\mathrm{d}t}\n\\tag{3}\\]\n\\[\nQ_{out}(t)=\\int_{\\tau=t0}^{t}Q_{in}(\\tau)\\frac{1}{K}e^{-\\frac{t-\\tau}{K}}\\mathrm{d}\\tau + Q_{out}(t_0)\\frac{1}{K}e^{-\\frac{t-t0}{K}}\n\\tag{4}\\]\nWhere:\n\n\\(Q_{in}\\) is the inflow of the reservoir\n\\(Q_{out}\\) is the outflow of the reservoir\n\\(S\\) is the storage of the reservoir\n\\(K\\) is the parameter that defines the relationship between \\(Q_{out}\\) and \\(S\\)",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#analytical-solution",
    "href": "modelling/model_linearReservoir.html#analytical-solution",
    "title": "Linear Reservoir",
    "section": "2.1 Analytical Solution",
    "text": "2.1 Analytical Solution\nThe final form of the equation under the simplifying hypothesis of linear input looks like this:\n\\[\nQ_{out}(t_1) = Q_{out}(t_0) + (Q_{in}(t_1) - Q_{out}(t_0))\\cdot (1-e^{-\\frac{1}{K}}) + (Q_{in}(t_1) - Q_{in}(t_0))\\cdot [1-K(1-e^{-\\frac{1}{K}})]\n\\]\n\nlinear_reservoir_Ana &lt;- function(Q_In, Q_Out0 = 0, param_K = 1) {\n  n_Step &lt;- length(Q_In)\n  Q_Out &lt;- c(Q_Out0, rep(0, n_Step - 1))\n  \n  for (i in 2:n_Step) {\n    Q_Out[i] &lt;- Q_Out[i-1] + (Q_In[i] - Q_Out[i-1]) * (1 - exp(-1 / param_K)) + (Q_In[i] - Q_In[i-1]) * (1 - param_K * (1 - exp(-1 / param_K)))\n  }\n  \n  Q_Out\n  \n}",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#numerical-solution",
    "href": "modelling/model_linearReservoir.html#numerical-solution",
    "title": "Linear Reservoir",
    "section": "2.2 Numerical Solution",
    "text": "2.2 Numerical Solution\nWhen we simplify the difficult continuous form into a discrete form using \\(\\Delta S / \\Delta t\\) to replace \\(\\mathrm{d}S/\\mathrm{d}t\\), we can obtain the numerical (discrete) format:\n\\[\nQ_{out}(t_1) = Q_{out}(t_0) + (Q_{in}(t_0) - Q_{out}(t_0)) \\frac{1}{K + 0.5} + (Q_{in}(t_1) - Q_{in}(t_0)) \\frac{0.5}{K + 0.5}\n\\]\n\nlinear_reservoir_Num &lt;- function(Q_In, Q_Out0 = 0, param_K = 1) {\n  n_Step &lt;- length(Q_In)\n  Q_Out &lt;- c(Q_Out0, rep(0, n_Step - 1))\n  \n  for (i in 2:n_Step) {\n    Q_Out[i] &lt;- Q_Out[i-1] + (Q_In[i-1] - Q_Out[i-1]) / (param_K + 0.5) + (Q_In[i] - Q_In[i-1]) * .5 / (param_K + 0.5)\n  }\n  \n  Q_Out\n  \n}",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#compare-the-results-of-both-functions",
    "href": "modelling/model_linearReservoir.html#compare-the-results-of-both-functions",
    "title": "Linear Reservoir",
    "section": "2.3 Compare the results of both Functions",
    "text": "2.3 Compare the results of both Functions\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\nload(\"../data_share/color.Rdata\")\n\n\n\nCode\nnum_TestIn &lt;- c(rep(100, 100), 0:100, rep(0,99))\nnum_Out_Ana &lt;- linear_reservoir_Ana(num_TestIn, param_K = 60)\nnum_Out_Num &lt;- linear_reservoir_Num(num_TestIn, param_K = 60)\n\ngg_Test &lt;- ggplot() +\n  geom_line(aes(1:300, num_TestIn, color = \"Input\")) +\n  geom_line(aes(1:300, num_Out_Ana, color = \"Output\\n(Analytical)\")) +\n  geom_line(aes(1:300, num_Out_Num, color = \"Output\\n(Numerical)\")) +\n  scale_color_manual(values = c(\"cyan\", \"red\", \"orange\"))+\n  labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"\") \n\nggplotly(gg_Test)\n\n\n\n\n\n\nIn the test forcing data, there are constant, linear, and null input scenarios. In all three situations, the analytical and numerical solutions yield almost the same results. Therefore, we can use either of them for subsequent analysis.",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#boundaray-condition-forcing",
    "href": "modelling/model_linearReservoir.html#boundaray-condition-forcing",
    "title": "Linear Reservoir",
    "section": "3.1 Boundaray Condition Forcing",
    "text": "3.1 Boundaray Condition Forcing\nFor the single linear reservoir, the boundary condition is the time series of the inflow \\(Q_{in}(t)\\) (Q_In).\nIn the one-variable experiment of the boundary condition, we will consider five boundary conditions, including three constants at 10, 50, and 100 [V/L], as well as an increasing (0 to 100 [V/L]) and decreasing (100 to 0 [V/L]) series.\nThe\n\n\nCode\nnum_BC10 &lt;- rep(c(10,0), each = 100)\nnum_BC50 &lt;- rep(c(50,0), each = 100)\nnum_BC100 &lt;- rep(c(100,0), each = 100)\nnum_BCin &lt;- c(0:100, rep(0,99))\nnum_BCde &lt;- c(100:0, rep(0,99))\n\nlst_BC_in &lt;- list(num_BC10, num_BC50, num_BC100, num_BCin, num_BCde)\ndf_BC_in &lt;- bind_cols(lst_BC_in) |&gt; as.data.frame()\nnames(df_BC_in) &lt;- c(\"BC10\", \"BC50\", \"BC100\", \"BCin\", \"BCde\")\ngdf_BC_in &lt;- reshape2::melt(df_BC_in)\ngdf_BC_in$time &lt;- 1:200\ngdf_BC_in$facet &lt;- \"Q_In\"\n\nlst_BC_out &lt;- map(lst_BC_in, linear_reservoir_Num, param_K = 60)\n\ndf_BC_out &lt;- bind_cols(lst_BC_out) |&gt; as.data.frame()\nnames(df_BC_out) &lt;- c(\"BC10\", \"BC50\", \"BC100\", \"BCin\", \"BCde\")\ngdf_BC_out &lt;- reshape2::melt(df_BC_out)\ngdf_BC_out$time &lt;- 1:200\ngdf_BC_out$facet &lt;- \"Q_Out\"\ngdf_BC &lt;- rbind(gdf_BC_in, gdf_BC_out)\ngg_BC &lt;- ggplot(gdf_BC) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_manual(values = color_TUD_diskrete)+\n  facet_grid(cols = vars(facet))+\n  scale_alpha_manual(values = c(.6,1)) +\n    labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari (BC):\") \nggplotly(gg_BC)\n\n\n\n\n\n\n\n\nFigure 1: The facet labeled Q_In displays five input time series (\\(Q_{in}\\)) scenarios. In scenarios BC10, BC50, and BC100, the input remains constant for the initial 100 timesteps. BCin and BCde represent scenarios where the input increases and decreases, respectively, during the first 100 timesteps. Notably, BC50, BCin, and BCde scenarios all have the same total volume of 5000 [V]. The facet labeled Q_Out presents the corresponding simulated results, showcasing the model’s responses to different boundary conditions (\\(Q_{in}\\)).",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#innitial-condition-forcing",
    "href": "modelling/model_linearReservoir.html#innitial-condition-forcing",
    "title": "Linear Reservoir",
    "section": "3.2 Innitial Condition Forcing",
    "text": "3.2 Innitial Condition Forcing\nNormally, the initial condition represents the initial state of state variables, such as the water content of the soil or the storage of the reservoir. However, in the case of a single linear reservoir, the storage of the reservoir is simplified as the variable \\(Q_{out}\\) (Q_Out). For this one-variable experiment, \\(Q_{out}\\) will vary from 10 to 90 [V/L].\n\n\nCode\nlst_IC_in &lt;- as.list(seq(10, 90, 10))\nlst_IC_out &lt;- map(lst_IC_in, linear_reservoir_Ana, Q_In = num_BC100, param_K = 60)\n\ndf_IC_out &lt;- bind_cols(lst_IC_out) |&gt; as.data.frame()\ngdf_IC_out &lt;- reshape2::melt(df_IC_out)\ngdf_IC_out$time &lt;- 1:200\ngdf_IC_out$variable &lt;- rep(seq(10, 90, 10), each = 200)\ngg_BC &lt;- ggplot(gdf_IC_out) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_gradientn(colours = color_DRESDEN)+\n    labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari\\n(IC):\") \nggplotly(gg_BC)\n\n\n\n\n\n\n\n\nFigure 2: The different initial condition \\(Q_{out}(t_0)\\) values result in distinct outflow time series. The line colors correspond to the values of the initial condition.",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/model_linearReservoir.html#parameter",
    "href": "modelling/model_linearReservoir.html#parameter",
    "title": "Linear Reservoir",
    "section": "3.3 Parameter",
    "text": "3.3 Parameter\nIn the case of the single linear reservoir, there is only one parameter, denoted as \\(K\\) (param_K). The parameter \\(K\\) can vary widely due to differences in the scale of the simulation domain. It has physical units of time, which can be specified in units such as seconds, hours, or days depending on the scale of the hydrological model.\n\n\nCode\nlst_Param_in &lt;- as.list(seq(10, 90, 10))\nlst_Param_out &lt;- map(lst_Param_in, linear_reservoir_Ana, Q_In = num_BC100, Q_Out0 = 0)\n\ndf_Param_out &lt;- bind_cols(lst_Param_out) |&gt; as.data.frame()\ngdf_Param_out &lt;- reshape2::melt(df_Param_out)\ngdf_Param_out$time &lt;- 1:200\ngdf_Param_out$variable &lt;- rep(seq(10, 90, 10), each = 200)\ngg_BC &lt;- ggplot(gdf_Param_out) +\n  geom_line(aes(time, value, group = variable, color = variable)) +\n  scale_color_gradientn(colours = color_DRESDEN)+\n  labs(x = \"Time [T]\", y = \"Water Flow [V/T]\", color = \"Vari\\n(Param):\") \nggplotly(gg_BC)\n\n\n\n\n\n\n\n\nFigure 3: The different parameter \\(K\\) values result in distinct outflow time series. The line colors correspond to the values of the parameter.",
    "crumbs": [
      "Modelling",
      "Linear Reservoir"
    ]
  },
  {
    "objectID": "modelling/index.html",
    "href": "modelling/index.html",
    "title": "Hydrological Modelling",
    "section": "",
    "text": "Hydrological modeling is a scientific approach aimed at simulating and comprehending the dynamics of the Earth’s water cycle. On this page, we will compile information on hydrological models, as well as the processes involved in their application. This includes running the model, evaluating its performance, calibrating and conducting sensitivity analyses, and ultimately validating the model’s results. Through this comprehensive exploration, we aim to provide valuable insights into the world of hydrological modeling.",
    "crumbs": [
      "Modelling"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcom Page",
    "section": "",
    "text": "Welcome to HydroSimul: Your Gateway to Understanding Hydrological Simulation\nWe’re thrilled to have you at HydroSimul, your premier resource for delving into the realm of hydrological simulation. Here, we curate a vast collection of datasets and offer cutting-edge techniques to empower your exploration.\nAt HydroSimul, we believe in the power of collaboration. If you want to share your knowledge, experience, and skills in the field, please contact us at hydro.simul@gmail.com. We also welcome your advice.\nOur website serves as a comprehensive hub for your hydrological journey:"
  },
  {
    "objectID": "index.html#dataset-collection",
    "href": "index.html#dataset-collection",
    "title": "Welcom Page",
    "section": "1. Dataset Collection:",
    "text": "1. Dataset Collection:\nNumerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "index.html#data-processing",
    "href": "index.html#data-processing",
    "title": "Welcom Page",
    "section": "2. Data Processing:",
    "text": "2. Data Processing:\nOn this page, we provide you with a wealth of data processing tools and techniques. Discover how to adeptly clean, preprocess, and convert raw hydrological data into a valuable format suitable for analysis and modeling."
  },
  {
    "objectID": "index.html#data-analysis",
    "href": "index.html#data-analysis",
    "title": "Welcom Page",
    "section": "3. Data Analysis:",
    "text": "3. Data Analysis:\nUncover concealed statistical insights and trends within your data. Our comprehensive guides and tutorials are designed to empower you with the skills to effectively analyze hydrological data."
  },
  {
    "objectID": "index.html#hydrological-modeling",
    "href": "index.html#hydrological-modeling",
    "title": "Welcom Page",
    "section": "4. Hydrological Modeling:",
    "text": "4. Hydrological Modeling:\nTake your understanding to the next level with hydrological modeling. Explore various models and model frameworks and acquire hands-on experience in simulating complex hydrological processes. Additionally, we provide extensive resources on calibration algorithms and strategies to significantly improve your modeling outcomes."
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Dataset",
    "section": "",
    "text": "Numerous open-access datasets are readily available for use in hydrological modeling, including meteorological, hydrological, and various geophysical datasets. Within this dataset collection, we not only provide direct links to the datasets but also present essential information in a standardized format, simplifying your dataset selection process. Additionally, we strive to establish connections with research papers that have utilized these datasets and offer valuable feedback gleaned from these sources."
  },
  {
    "objectID": "dataset/geoph.html",
    "href": "dataset/geoph.html",
    "title": "Geophysical",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/geoph.html#h2",
    "href": "dataset/geoph.html#h2",
    "title": "Geophysical",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html",
    "href": "dataprocess/visual_ggplot2Basic.html",
    "title": "ggplot2 Basic",
    "section": "",
    "text": "The work of visualization involves the process of mapping the data into visualized geometry. There are three basic components to this: data, geometry, and aesthetic mappings. In this article, we will step-by-step learn about these three components.\n\nlibrary(tidyverse) # or: library(ggplot2)\n\n\n\nThe data structure in ggplot2 is organized in a data.frame or a similar structure like tibble. As an example, we will use the mpg dataset from ggplot2.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nThe dataframe can (should) contain all the information that you want to visualize.\n\n\n\nThe aesthetic mappings define the connection (mapping) between variables in the data and visual properties of geometry. The properties of the geometry are derived from the chosen geometry. For example, almost every plot maps a variable to x and y to determine the position, and color or size provides additional details.\nThis mapping is established with the aes() function. In the aes() function, you need to use the format property name = variable name to connect the data and geometry, like aes(x = displ, y = hwy).\nThe aes() function simply defines which variable is connected to which aesthetics, but for more detailed modifications of aesthetics, you can use the scale_*() functions. More details are provided in Section 2.3.\n\n\n\nWith the same dataset, we can also choose different geometries, such as scatter points, lines, or bars. In ggplot2, all the geometries are defined using the geom_*() functions, like geom_point() and geom_line(). These functions specify how the data should be visually represented.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#data",
    "href": "dataprocess/visual_ggplot2Basic.html#data",
    "title": "ggplot2 Basic",
    "section": "",
    "text": "The data structure in ggplot2 is organized in a data.frame or a similar structure like tibble. As an example, we will use the mpg dataset from ggplot2.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nThe dataframe can (should) contain all the information that you want to visualize.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#aesthetic-mappings",
    "href": "dataprocess/visual_ggplot2Basic.html#aesthetic-mappings",
    "title": "ggplot2 Basic",
    "section": "",
    "text": "The aesthetic mappings define the connection (mapping) between variables in the data and visual properties of geometry. The properties of the geometry are derived from the chosen geometry. For example, almost every plot maps a variable to x and y to determine the position, and color or size provides additional details.\nThis mapping is established with the aes() function. In the aes() function, you need to use the format property name = variable name to connect the data and geometry, like aes(x = displ, y = hwy).\nThe aes() function simply defines which variable is connected to which aesthetics, but for more detailed modifications of aesthetics, you can use the scale_*() functions. More details are provided in Section 2.3.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#geometry",
    "href": "dataprocess/visual_ggplot2Basic.html#geometry",
    "title": "ggplot2 Basic",
    "section": "",
    "text": "With the same dataset, we can also choose different geometries, such as scatter points, lines, or bars. In ggplot2, all the geometries are defined using the geom_*() functions, like geom_point() and geom_line(). These functions specify how the data should be visually represented.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#element_",
    "href": "dataprocess/visual_ggplot2Basic.html#element_",
    "title": "ggplot2 Basic",
    "section": "2.1 element_*()",
    "text": "2.1 element_*()\nIn conjunction with the theme system, the element_ functions specify the display of how non-data components of the plot are drawn (Wickham 2009).\nThere are four main elements to specify the appearance of elements for the plot, axis, and more (details in ggplot2 Theme elements):\n\nelement_blank(): draws nothing and assigns no space.\nelement_rect(): used for borders and backgrounds.\nelement_line(): defines the appearance of lines.\nelement_text(): controls the appearance of text.\n\n\nelement_blank()\n\nelement_rect(\n  fill = NULL,\n  colour = NULL,\n  linewidth = NULL,\n  linetype = NULL,\n  color = NULL,\n  inherit.blank = FALSE,\n  size = deprecated()\n)\n\nelement_line(\n  colour = NULL,\n  linewidth = NULL,\n  linetype = NULL,\n  lineend = NULL,\n  color = NULL,\n  arrow = NULL,\n  inherit.blank = FALSE,\n  size = deprecated()\n)\n\nelement_text(\n  family = NULL,\n  face = NULL,\n  colour = NULL,\n  size = NULL,\n  hjust = NULL,\n  vjust = NULL,\n  angle = NULL,\n  lineheight = NULL,\n  color = NULL,\n  margin = NULL,\n  debug = NULL,\n  inherit.blank = FALSE\n)\n\nSome common characters include:\n\nfill: fill color\ncolour or color: Line/border/text color. color is an alias for colour.\nlinewidth: Line/border size in mm.\nsize: Text size in pts.\n\nYou can find more scripts and detailed examples in the article The elements of a plot.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#title-modify",
    "href": "dataprocess/visual_ggplot2Basic.html#title-modify",
    "title": "ggplot2 Basic",
    "section": "2.2 Title Modify",
    "text": "2.2 Title Modify\nThe element_*() functions specifically define the style, but for certain text elements like titles or axis titles, you need to provide specific names. By default, these elements are named after the variables you have mapped them to.\n\nlabs(): This function allows you to modify all text related to titles, subtitles, captions, and tags for the plot, as well as aesthetic names (e.g., x, y, color, fill, etc.).\n\nFor individual elements, you can use specific functions:\n\nxlab(): Set the x-axis label.\nylab(): Set the y-axis label.\nggtitle(): Set the main plot title.\n\nBy using these functions, you can precisely control the naming and appearance of text elements in your plot.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#sec-scale",
    "href": "dataprocess/visual_ggplot2Basic.html#sec-scale",
    "title": "ggplot2 Basic",
    "section": "2.3 scale_*()",
    "text": "2.3 scale_*()\nThey take your data and turn it into something that you can see, like size, colour, position or shape. They also provide the tools that let you interpret the plot: the axes and legends. You can generate plots with ggplot2 without knowing how scales work, but understanding scales and learning how to manipulate them will give you much more control (Wickham 2009).\n\n2.3.1 Axis scale_x_*() and scale_y_*()\nThere are two main types of axes: discrete, which represents values not in numerical order, and continuous axes.\n\nscale_*_discrete()\nscale_*_continuous()\n\nscale_*_log10()\nscale_*_sqrt()\nscale_*_reverse()\n\n\nAdditionally, there are specific axes (based on continuous) for dates (scale_*_date()) and bins (scale_*_binned()), commonly used in histograms.\nThe common attributes for axes include:\n\nname: axis title\nbreaks / minor_breaks: break points for axis ticks and panel grid lines\nlabels: axis text of break points\nlimits: axis ranges\n\n\n\n2.3.2 Colour scale_color_*() and scale_fill_*()\nLike axes, color mapping also has challenges with discrete and continuous data.\nFor continuous color mapping, there are:\n\nContinuous: scale_colour_continuous()\n\ntype: custom\n\nGradient:\n\nscale_colour_gradient() for two (low and high) colors\nscale_colour_gradient2() for three (low, mid, and high) colors\nscale_colour_gradientn() for defined colors\n\nBinned: assigns discrete color bins to the continuous values\n\nscale_colour_binned()\n\n\nFor discrete data, use scale_colour_manual() to define the colors for values.\n\n\n2.3.3 Other scales\nExcept for axes and color, other aesthetics can be simpler with scale_*_manual() to define:\n\nscale_size_manual()\nscale_shape_manual()\nscale_linetype_manual()\nscale_linewidth_manual()\nscale_alpha_manual()",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#fixed-elements-for-specific-geometry",
    "href": "dataprocess/visual_ggplot2Basic.html#fixed-elements-for-specific-geometry",
    "title": "ggplot2 Basic",
    "section": "2.4 Fixed elements for specific geometry",
    "text": "2.4 Fixed elements for specific geometry\nThe scale_*() functions primarily control the attributes of elements using data. However, there are situations where you may want to set fixed values for a specific element directly. For example, you can set the linewidth for colored lines: geom_line(aes(color = dis), linewidth = 2). The settings outside the aes() function will be treated as fixed values for that particular geometry.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#syntax",
    "href": "dataprocess/visual_ggplot2Basic.html#syntax",
    "title": "ggplot2 Basic",
    "section": "3.1 Syntax",
    "text": "3.1 Syntax\nThe syntax of ggplot2 follows a layered approach, where you start with the base layer of data and progressively add additional layers to create a complex plot.\nThe basic syntax involves using the ggplot() function to initiate the plot, specifying the data and aesthetics using the aes() function, and then adding (use +) geometric elements with functions like geom_point() or geom_line(). Each added layer enhances the plot, and you can further customize it using various options. The syntax is intuitive and modular, allowing for flexible and expressive visualizations.\n\nggplot(data = my_Data, aes(x = my_X, y = my_Y)) +\n  geom_point()\n\nIn ggplot2, the aes() function (also data), which defines the aesthetic mappings, can be placed either within the ggplot() function for all geoms or inside the specific geom_*() function to apply mappings only to that geometry. This flexibility allows for clear and concise syntax, as aesthetic mappings can be specified globally for the entire plot or tailored for individual geometric layers.\n\nggplot() +\n  geom_point(data = my_Data, aes(x = my_X, y = my_Y))\n\nNot only that, but you can also divide the mapping into several parts, with common mappings in the initial ggplot() function and other specific mappings in the given geom_*() functions. This allows for flexibility and customization in defining aesthetic mappings for different geometries in the same plot.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#first-ggplot",
    "href": "dataprocess/visual_ggplot2Basic.html#first-ggplot",
    "title": "ggplot2 Basic",
    "section": "3.2 First ggplot()",
    "text": "3.2 First ggplot()\n\n3.2.1 Task\nWith the above three short introductions, we can now try the first plot with the mpg dataset.\nThe task is as follows:\n\nData: Using the mpg dataset.\n\nVariables: displ, hwy, class\n\nGeometry:\n\nColored scatter plot: geom_point()\nSmoothed line: geom_smooth()\n\nAesthetic Mappings:\n\nx-dimension with displ (applied to both geoms): x = displ\ny-dimension with hwy (applied to both geoms): y = hwy\nColored with class (only for scatter plot): color = class\n\n\n\n\n3.2.2 Codes\n\nggplot(data = mpg, aes(x = displ, y = hwy))+\n  geom_point(aes(colour = class)) + \n  geom_smooth()",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/visual_ggplot2Basic.html#customize-the-elements",
    "href": "dataprocess/visual_ggplot2Basic.html#customize-the-elements",
    "title": "ggplot2 Basic",
    "section": "3.3 Customize the elements",
    "text": "3.3 Customize the elements\nFor the continuous axis, we will store the default plot in a variable gp_Default, and then we can use + to add future custom settings.\n\ngp_Default &lt;- ggplot(data = mpg, aes(x = displ, y = hwy))+\n  geom_point(aes(colour = class)) + \n  geom_smooth() \n\n\n3.3.1 Label and Text\n\nX-Axis in \"Engine displacement (L)\"\nY-Axis in \"MPG in highway  (mi / gal)\"\nTitle in \"Fuel economy data\"\nLegend (color) title in \"Car type\"\n\n\ngp_Default +\n  labs(x = \"Engine displacement (L)\", \n       y = \"MPG in highway (mi/gal)\", \n       colour = \"Car type\",\n       title = \"Fuel economy data\")\n\n\n\n\n\n\n\n\n\n\n3.3.2 Axes\n\nX-limits: 2 to 5\nY-breaks: from 15 to 45 every 5\n\n\ngp_Default +\n  scale_x_continuous(limits = c(2, 5)) +\n  scale_y_continuous(breaks = seq(15, 45, 5)) \n\n\n\n\n\n\n\n\n\n\n3.3.3 Colour\n\nSmooth line in red\npoints color in rainbow color\n\n\ngp_Default +\n  geom_smooth(color = \"red\") +\n  scale_color_manual(values = rainbow(7))",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "ggplot2 Basic"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_process.html",
    "href": "dataprocess/timeserises_process.html",
    "title": "Basic Processing",
    "section": "",
    "text": "In this article, we will cover fundamental techniques for manipulating and analyzing time series data. This includes tasks such as creating time series, summarizing data based on time indices, identifying trends, and more.\nTime series data often comes with specific considerations related to time zones, varying numbers of days in months, and leap years.\n\nTime Zones: Time series data collected from different regions or sources may be recorded in various time zones. Converting data to a consistent time zone is crucial to ensure accurate analysis and visualization, especially for data with hourly resolution.\nVarying Days in Months: Some months have 30 days, while others have 31, and February can have 28 or 29 days in leap years. This variation should be considered when performing calculations based on monthly or daily data.\nLeap Years: Leap years, which occur every four years, add an extra day (February 29) to the calendar. Analysts need to account for leap years when working with annual time series data to avoid inconsistencies.\n\nProperly accounting for these specifics is crucial for accurate analysis and interpretation of time series data.\n\n1 Library\nTime series data structures are not standard in R, but the xts package is commonly used to work with time indices. However, it’s important to note that for processes that don’t rely on specific time indexing, the original data structure is sufficient. Time series structures are particularly useful when you need to perform time-based operations and analysis.\n\nlibrary(xts)\nlibrary(tidyverse)\n\n\n\n2 Example Files\nThe example files provided consist of three discharge time series for the Ruhr River in the Rhein basin, Germany. These data sets are sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github.\n\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Oeventrop &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Oeventrop_2761759000100.csv\"\nfn_Villigst &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Villigst_2765590000100.csv\"\n\n\n\n3 Create data\nBefore creating a time series structure, the data should be loaded into R. Time series in R can typically (only) support two-dimensional data structures, such as matrices and data frames.\nIf the date-time information is not correctly recognized during reading or if there is no time data present, you need to make sure that you have a valid time index.\nThere are two primary ways to create a time series in R:\n\nxts(): With this method, you explicitly specify the time index and create a time series object. This is useful when you have a matrix with an external time index.\nas.xts(): This method is more straightforward and is suitable when you have a data frame with a date column. The function will automatically recognize the date column and create a time series.\n\n\n# Read a CSV file as data.frame\ndf_Bachum &lt;- read_csv2(fn_Bachum, skip = 10, col_names = FALSE)\ndf_Villigst &lt;- read_csv2(fn_Villigst, skip = 10, col_names = FALSE)\n\n# Convert Date column to a Date type\ndf_Bachum$X1 &lt;- as_date(df_Bachum$X1, format = \"%d.%m.%Y\")\ndf_Villigst$X1 &lt;- as_date(df_Villigst$X1, format = \"%d.%m.%Y\")\n\n# Create an xts object\nxts_Bachum &lt;- xts(df_Bachum$X2, order.by = df_Bachum$X1)\nxts_Villigst &lt;- as.xts(df_Villigst)\n\n\n\n4 Merging Several Time Series\nIn R, the time index is consistent and follows a standardized format. This consistency in time indexing makes it easy to combine multiple time series into a single dataset based on their time index.\n\nmerge()\n\n\nxts_Rhur &lt;- merge(xts_Bachum, xts_Villigst)\nnames(xts_Rhur) &lt;- c(\"Bachum\", \"Villigst\")\n\nIt’s worth noting that when working with time series data in R, the length of the time series doesn’t necessarily have to be the same for all time series. This flexibility allows you to work with data that may have missing or varying data points over time, which is common in many real-world scenarios.\n\nlength(xts_Bachum)\n\n[1] 12053\n\nlength(xts_Villigst)\n\n[1] 11499\n\n\n\n\n5 Subsetting (Index with time)\nYou can work with time series data in R using both integer indexing, and time-based indexing using time intervals.\n\n# Create a time sequence\nts_Inteval &lt;- seq(as_date(\"1996-01-01\"), as_date(\"1996-12-31\"), \"days\")\n\n# Subset\nxts_Inteval &lt;- xts_Rhur[ts_Inteval, ]\nhead(xts_Inteval, 10)\n\n           Bachum Villigst\n1996-01-01 13.459    11.03\n1996-01-02 12.331    10.03\n1996-01-03 11.112     9.12\n1996-01-04 11.272     8.11\n1996-01-05 11.412     8.71\n1996-01-06 11.526     8.29\n1996-01-07 12.589     9.45\n1996-01-08 12.508    10.09\n1996-01-09 12.336     9.42\n1996-01-10 12.510     8.47\n\n\n\n\n6 Rolling Windows\nMoving averages are a valuable tool for smoothing time series data and uncovering underlying trends or patterns. With rolling windows, you can calculate not only the mean value but also other statistics like the median and sum. To expand the range of functions available, you can utilize the rollapply(). This enables you to apply a wide variety of functions to your time series data within specified rolling windows.\n\nrollmean()\nrollmedian()\nrollsum()\nrollmax()\n\n\nxts_RollMean &lt;- rollmean(xts_Inteval, 7)\nhead(xts_RollMean, 10)\n\n             Bachum Villigst\n1996-01-04 11.95729 9.248571\n1996-01-05 11.82143 9.114286\n1996-01-06 11.82214 9.027143\n1996-01-07 12.02186 8.934286\n1996-01-08 12.23314 9.242857\n1996-01-09 12.37214 9.238571\n1996-01-10 12.61357 9.541429\n1996-01-11 12.62643 9.535714\n1996-01-12 12.56257 9.357143\n1996-01-13 12.50186 9.242857\n\n\n\n\n7 Summary in Calendar Period\nDealing with irregularly spaced time series data can be challenging. One fundamental operation in time series analysis is applying a function by calendar period. This process helps in summarizing and analyzing time series data more effectively, even when the data points are irregularly spaced in time.\n\napply.daily()\napply.weekly()\napply.monthly()\napply.quarterly()\napply.yearly()\n\n\nxts_Month &lt;- apply.monthly(xts_Inteval, mean)\nxts_Month\n\n              Bachum  Villigst\n1996-01-31 12.478387  9.348065\n1996-02-29 15.794241 17.403448\n1996-03-31 14.244613 13.252903\n1996-04-30 10.217533  7.310667\n1996-05-31  9.331129  7.094839\n1996-06-30 10.589067  6.700667\n1996-07-31 11.607968  8.248710\n1996-08-31 12.897806  9.410968\n1996-09-30 14.516733 12.750000\n1996-10-31 18.214161 17.702903\n1996-11-30 30.673967 35.472667\n1996-12-31 35.720290 39.940645",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Basic Processing"
    ]
  },
  {
    "objectID": "dataprocess/statistic_graphic.html",
    "href": "dataprocess/statistic_graphic.html",
    "title": "Graphical Statistic",
    "section": "",
    "text": "Graphical statistic is a branch of statistics that involves using visual representations to analyze and communicate data. It provides a powerful way to convey complex information in a more understandable and intuitive form.\n\n1 Example Data\nThe example files provided consist of three discharge time series for the Ruhr River in the Rhein basin, Germany. These data sets are sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github.\n\n# Library\nlibrary(xts)\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\n\n# File name\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Oeventrop &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Oeventrop_2761759000100.csv\"\nfn_Villigst &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Villigst_2765590000100.csv\"\n\n# Load Data\ndf_Bachum &lt;- read_csv2(fn_Bachum, skip = 10, col_names = FALSE)\ndf_Oeventrop &lt;- read_csv2(fn_Oeventrop, skip = 10, col_names = FALSE)\ndf_Villigst &lt;- read_csv2(fn_Villigst, skip = 10, col_names = FALSE)\n\n# Convert Date column to a Date type\ndf_Bachum$X1 &lt;- as_date(df_Bachum$X1, format = \"%d.%m.%Y\")\ndf_Oeventrop$X1 &lt;- as_date(df_Oeventrop$X1, format = \"%d.%m.%Y\")\ndf_Villigst$X1 &lt;- as_date(df_Villigst$X1, format = \"%d.%m.%Y\")\n\n# Create an xts object\nxts_Bachum &lt;- as.xts(df_Bachum)\nxts_Oeventrop &lt;- as.xts(df_Oeventrop)\nxts_Villigst &lt;- as.xts(df_Villigst)\n\n# Merge into one data frame\nxts_Rhur &lt;- merge(xts_Bachum, xts_Oeventrop, xts_Villigst)\nnames(xts_Rhur) &lt;- c(\"Bachum\", \"Oeventrop\", \"Villigst\")\nxts_Rhur &lt;- xts_Rhur[seq(as_date(\"1991-01-01\"), as_date(\"2020-12-31\"), \"days\"), ]\n\n# Deal with negative\ndf_Ruhr &lt;- coredata(xts_Rhur)\ndf_Ruhr[df_Ruhr &lt; 0] &lt;- NA\n\n# Summary in month\nxts_Ruhr_Clean &lt;- xts(df_Ruhr, index(xts_Rhur))\ndf_Ruhr_Month &lt;- apply.monthly(xts_Ruhr_Clean, mean)\n\nIn this article, we will leverage the power of the ggplot2 library to create plots and visualizations. To achieve this, the first step is to reformat the dataframe to a structure suitable for plotting.\n\ngdf_Ruhr &lt;- reshape2::melt(data.frame(date=index(df_Ruhr_Month), df_Ruhr_Month), \"date\")\n\n\n\n2 Timeserise line\nThe time series lines will provide us with discharge from 1991-01-01 to 2020-12-31 of the three gauges.\n\ngeom_line()\n\n\ngg_TS_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_line(aes(date, value, color = variable)) +\n  labs(x = \"Date\", y = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_TS_Ruhr)\n\n\n\n\n\n\n\n3 Frequency Plots/Histogram\nHistograms and frequency plots are graphical representations of data distribution.\nHistograms display the counts (or frequency) with bars; frequency plots display the counts (or frequency) with lines.\nThe frequency plot represents the relative density of the data points by the relative height of the bars, while in a histogram, the area within the bar represents the relative density of the data points.\n\ngeom_histogram()\n\n\ngg_Hist_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_histogram(aes(value, group = variable, fill = variable, color = variable), position = \"dodge\", alpha = .5) +\n  labs(y = \"Count\", x = \"Discharge [m^3/s]\", color = \"Gauge\", fill = \"Gauge\")\n\nggplotly(gg_Hist_Ruhr)\n\n\n\n\n\n\ngeom_freqpoly()\n\n\ngg_Freq_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_freqpoly(aes(value, y = after_stat(count / sum(count)), group = variable, fill = variable, color = variable)) +\n  labs(y = \"Frequency\", x = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_Freq_Ruhr)\n\n\n\n\n\n\n\n4 Box and Whisker Plot\nA Box and Whisker Plot, also known as a box plot, is a graphical representation of the distribution of a dataset. It provides a concise summary of the dataset’s key statistical measures and helps you visualize the spread and skewness of the data (Machiwal and Jha 2012). Here’s how a typical box and whisker plot is structured:\n\nBox: The box in the middle of the plot represents the interquartile range (IQR), which contains the middle 50% of the data. The bottom edge of the box represents the 25th percentile (Q1), and the top edge represents the 75th percentile (Q3).\nWhiskers: The whiskers extend from the box and represent the range of the data, excluding outliers. They typically extend to a certain multiple of the IQR beyond the quartiles. Outliers beyond the whiskers are often plotted as individual points.\nMedian (line inside the box): A horizontal line inside the box represents the median (Q2), which is the middle value of the dataset when it’s sorted.\n\n\n\n\nFigure from Internet\n\n\n\ngg_Box_Ruhr &lt;- ggplot(gdf_Ruhr) +\n  geom_boxplot(aes(variable, value, fill = variable, color = variable), alpha = .5) +\n  labs(x = \"Gauge\", y = \"Discharge [m^3/s]\", color = \"Gauge\") +\n  theme(legend.position = \"none\")\n\nggplotly(gg_Box_Ruhr)\n\n\n\n\n\n\n\n5 Quantile Plot\nA ‘quantile plot’ can be used to evaluate the quantile information such as the median, quartiles, and interquartile range of the data points (Machiwal and Jha 2012).\n\ngeom_qq()\n\n\ngg_QQ_Ruhr &lt;- ggplot(gdf_Ruhr, aes(sample = value, color = variable)) +\n  geom_qq(alpha = .5, distribution = stats::qunif) +\n  geom_qq_line(distribution = stats::qunif) +\n  labs(x = \"Fraction\", y = \"Discharge [m^3/s]\", color = \"Gauge\")\n\nggplotly(gg_QQ_Ruhr)\n\n\n\n\n\n\n\n\n\n\nReferences\n\nMachiwal, Deepesh, and Madan Kumar Jha. 2012. Hydrologic Time Series Analysis: Theory and Practice. Neu Dehli: Captial Publishing Company.",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Graphical Statistic"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html",
    "href": "dataprocess/spatial_data.html",
    "title": "Basic Manipulation",
    "section": "",
    "text": "RPython\n\n\nThe terra package in R is a powerful and versatile package for working with geospatial data, including vector and raster data. It provides a wide range of functionality for reading, processing, analyzing, and visualizing spatial data.\nFor more in-depth information and resources on the terra package and spatial data science in R, you can explore the original website Spatial Data Science.\nFirstly load the library to the R space:\n\n# load the library\nlibrary(terra)\nlibrary(tidyverse)\n\n\n\nThe libraries for spatial data in Python are divided into several libraries, unlike the comprehensive terra library in R. For vector data, you can use the geopandas library, and for raster data, rasterio is a good choice, among others.\nFor more in-depth information and resources on the spatial data science in Python, you can explore the website Python Open Source Spatial Programming & Remote Sensing.\n\nimport os\nimport pandas as pd\nimport numpy as np\n# Vector\nimport geopandas as gpd\nfrom shapely.geometry import Point, LineString, Polygon, shape\nimport fiona\n\n# Rster\nimport rasterio\nfrom rasterio.plot import show as rast_plot\nfrom rasterio.crs import CRS\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nimport rasterio.features\nfrom rasterio.enums import Resampling\n\n# Plot\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#vector",
    "href": "dataprocess/spatial_data.html#vector",
    "title": "Basic Manipulation",
    "section": "2.1 Vector",
    "text": "2.1 Vector\nAs introduced in the section, spatial vector data typically consists of three main components:\n\nGeometry: Describes the spatial location and shape of features.\nAttributes: Non-spatial properties associated with features.\nCRS (Coordinate Reference System): Defines the spatial reference framework.\n\n\nRPython\n\n\n\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 &lt;- \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_1 &lt;- c(4484566, 4483922, 4483002, 4481929, 4481222, 4482500, 4483000, 4484666, 4484233)\ny_polygon_1 &lt;- c(5554566, 5554001, 5553233, 5554933, 5550666, 5551555, 5550100, 5551711, 5552767)\ngeometry_polygon_1 &lt;- cbind(id=1, part=1, x_polygon_1, y_polygon_1)\n# Define coordinates for the second polygon\nx_polygon_2 &lt;- c(4481929, 4481222, 4480500)\ny_polygon_2 &lt;- c(5554933, 5550666, 5552555)\ngeometry_polygon_2 &lt;- cbind(id=2, part=1, x_polygon_2, y_polygon_2)\n# Combine the two polygons into one data frame\ngeometry_polygon &lt;- rbind(geometry_polygon_1, geometry_polygon_2)\n\n# Create a vector layer for the polygons, specifying their type, attributes, CRS, and additional attributes\nvect_Test &lt;- vect(geometry_polygon, type=\"polygons\", \n                  atts = data.frame(ID_region = 1:2, Name = c(\"a\", \"b\")), \n                  crs = crs_31468)\nvect_Test$region_area &lt;- expanse(vect_Test)\n\n# Visualize the created polygons\nplot(vect_Test)\n\n\n\n\n\n\n\n\n\n\n\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 = \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_1 = [4484566, 4483922, 4483002, 4481929, 4481222, 4482500, 4483000, 4484666, 4484233]\ny_polygon_1 = [5554566, 5554001, 5553233, 5554933, 5550666, 5551555, 5550100, 5551711, 5552767]\n# Create a list of coordinate pairs for the first polygon\ngeometry_polygon_1 = Polygon([(x, y) for x, y in zip(x_polygon_1, y_polygon_1)])\n\n# Define coordinates for the second polygon\nx_polygon_2 = [4481929, 4481222, 4480500]\ny_polygon_2 = [5554933, 5550666, 5552555]\n# Create a list of coordinate pairs for the second polygon\ngeometry_polygon_2 = Polygon([(x, y) for x, y in zip(x_polygon_2, y_polygon_2)])\n\n# Construct Shapely polygons using the lists of coordinates\ngeometry_polygon = [geometry_polygon_1, geometry_polygon_2]\n\n\n# Create a GeoDataFrame with the polygons, specifying their attributes, CRS, and additional attributes\nvect_Test = gpd.GeoDataFrame({\n    'ID_region': [1, 2],\n    'Name': ['a', 'b'],\n    'geometry': geometry_polygon,\n}, crs=crs_31468)\n\n# Calculate the region area and add it as a new column\nvect_Test['region_area'] = vect_Test.area\n\n# Visualize the created polygons\nvect_Test.plot()\nplt.show()\n\n\n\n\n\n\n\nplt.close()",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#raster",
    "href": "dataprocess/spatial_data.html#raster",
    "title": "Basic Manipulation",
    "section": "2.2 Raster",
    "text": "2.2 Raster\nFor raster data, the geometry is relatively simple and can be defined by the following components:\n\nCoordinate of Original Point (X0, Y0) plus Resolutions (X and Y)\nBoundaries (Xmin, Xmax, Ymin, Ymax) plus Number of Rows and Columns\n\nOne of the most critical aspects of raster data is the values stored within its cells. You can set or modify these values using the values()&lt;- function in R.\n\nRPython\n\n\n\nrast_Test &lt;- rast(ncol=10, nrow=10, xmin=-150, xmax=-80, ymin=20, ymax=60)\nvalues(rast_Test) &lt;- runif(ncell(rast_Test))\n\nplot(rast_Test)\n\n\n\n\n\n\n\n\n\n\n\nfn_Rast_Test = \"C:\\\\Lei\\\\HS_Web\\\\data_share/raster_Py.tif\"\n\n# Create a new raster with the specified dimensions and extent\nncol, nrow = 10, 10\nxmin, xmax, ymin, ymax = -150, -80, 20, 60\n\n# Create the empty raster with random values\nwith rasterio.open(\n    fn_Rast_Test,\n    \"w\",\n    driver=\"GTiff\",\n    dtype=np.float32,\n    count=1,\n    width=ncol,\n    height=nrow,\n    transform=rasterio.transform.from_origin(xmin, ymax, (xmax - xmin) / ncol, (ymax - ymin) / nrow),\n    crs=\"EPSG:4326\"\n) as dst:\n    # Generate random values and assign them to the raster\n    random_values = np.random.rand(nrow, ncol).astype(np.float32)\n    dst.write(random_values, 1)  # Write the values to band 1\n\n# Now you have an empty raster with random values, and you can read and manipulate it as needed\nwith rasterio.open(fn_Rast_Test) as src:\n    rast_Test = src.read(1)\n\n\nrast_plot(rast_Test)\n\n\n\n\n\n\n\n\n\n\n\nCertainly, you can directly create a data file like an ASC (ASCII) file for raster data.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#assigning-a-crs",
    "href": "dataprocess/spatial_data.html#assigning-a-crs",
    "title": "Basic Manipulation",
    "section": "4.1 Assigning a CRS",
    "text": "4.1 Assigning a CRS\nIn cases where the Coordinate Reference System (CRS) information is not included in the data file’s content, you can assign it manually using the crs() function. This situation often occurs when working with raster data in formats like ASC (Arc/Info ASCII Grid) or other file formats that may not store CRS information.\n\nRPython\n\n\n\ncrs(rast_Test) &lt;- \"EPSG:31468\"\nrast_Test\n\nclass       : SpatRaster \nsize        : 5, 5, 1  (nrow, ncol, nlyr)\nresolution  : 1000, 1000  (x, y)\nextent      : 4480000, 4485000, 5550000, 5555000  (xmin, xmax, ymin, ymax)\ncoord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \nsource      : minibeispiel_raster.asc \nname        : minibeispiel_raster \n\n\nAs the results showed, the CRS information has been filled with the necessary details in line coord. ref..\n\n\n\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\", 'r+')\nrast_Test.crs = CRS.from_epsg(31468)\nprint(rast_Test.crs)\n\nEPSG:31468\n\n\n\n\n\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. You can obtain information about EPSG codes from the EPSG website.\n\n\n\n\n\n\nNOTE\n\n\n\nYou should not use this approach to change the CRS of a data set from what it is to what you want it to be. Assigning a CRS is like labeling something.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#transforming-vector-data",
    "href": "dataprocess/spatial_data.html#transforming-vector-data",
    "title": "Basic Manipulation",
    "section": "4.2 Transforming vector data",
    "text": "4.2 Transforming vector data\nThe transformation of vector data is relatively simple, as it involves applying a mathematical formula to the coordinates of each point to obtain their new coordinates. This transformation can be considered as without loss of precision.\n\nRPython\n\n\nThe project() function can be utilized to reproject both vector and raster data.\n\n# New CRS\ncrs_New &lt;- \"EPSG:4326\"\n# Reproject\nvect_Test_New &lt;- project(vect_Test, crs_New)\n\n# Info of vector layer\nvect_Test_New\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 3  (geometries, attributes)\n extent      : 11.72592, 11.78419, 50.08692, 50.13034  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : ID_region  Name region_area\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;\n values      :         1     a   8.853e+06\n                       2     b   2.208e+06\n\n\n\n\n\ngeopands.to_crs()\n\n\n# New CRS\ncrs_New = \"EPSG:4326\"\n\n# Reproject the vector layer to the new CRS\nvect_Test_New = vect_Test.to_crs(crs=crs_New)\n\n# Info of vector layer\nprint(vect_Test_New)\n\n   ID_region  ...                                           geometry\n0          1  ...  MULTIPOLYGON (((11.78268 50.12711, 11.7737 50....\n1          2  ...  MULTIPOLYGON (((11.74578 50.13033, 11.73611 50...\n\n[2 rows x 4 columns]",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#transforming-raster-data",
    "href": "dataprocess/spatial_data.html#transforming-raster-data",
    "title": "Basic Manipulation",
    "section": "4.3 Transforming raster data",
    "text": "4.3 Transforming raster data\nVector data can be transformed from lon/lat coordinates to planar and back without loss of precision. This is not the case with raster data. A raster consists of rectangular cells of the same size (in terms of the units of the CRS; their actual size may vary). It is not possible to transform cell by cell. For each new cell, values need to be estimated based on the values in the overlapping old cells. If the values are categorical data, the “nearest neighbor” method is commonly used. Otherwise some sort of interpolation is employed (e.g. “bilinear”). (From Spatial Data Science)\n\n\n\n\n\n\nNote\n\n\n\nBecause projection of rasters affects the cell values, in most cases you will want to avoid projecting raster data and rather project vector data.\n\n\n\n4.3.1 With CRS\nThe simplest approach is to provide a new CRS:\n\nRPython\n\n\n\nproject()\n\n\n# New CRS\ncrs_New &lt;- \"EPSG:4326\"\n# Reproject\nrast_Test_New &lt;- project(rast_Test, crs_New, method = 'near')\n\n# Info and Plot of vector layer\nrast_Test_New\n\nclass       : SpatRaster \nsize        : 4, 6, 1  (nrow, ncol, nlyr)\nresolution  : 0.01176853, 0.01176853  (x, y)\nextent      : 11.7188, 11.78941, 50.08395, 50.13102  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\n\nplot(rast_Test)\nplot(rast_Test_New)\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nNew\n\n\n\n\n\n\n\n\nfn_Rast_New = 'C:\\\\Lei\\\\HS_Web\\\\data_share/minibeispiel_raster.tif'\n# Define the new CRS\nnew_crs = {'init': 'EPSG:4326'}\ntransform, width, height = calculate_default_transform(\n        rast_Test.crs, new_crs, rast_Test.width, rast_Test.height, *rast_Test.bounds)\nkwargs = rast_Test.meta.copy()\nkwargs.update({\n        'crs': new_crs,\n        'transform': transform,\n        'width': width,\n        'height': height\n    })        \nrast_Test_New = rasterio.open(fn_Rast_New, 'w', **kwargs)        \nreproject(\n    source=rasterio.band(rast_Test, 1),\n    destination=rasterio.band(rast_Test_New, 1),\n    #src_transform=rast_Test.transform,\n    src_crs=rast_Test.crs,\n    #dst_transform=transform,\n    dst_crs=new_crs,\n    resampling=Resampling.nearest)\n\n(None, None)\n\nrast_Test_New.close()        \n\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\")\nrast_plot(rast_Test)\n\n\n\n\n\n\n\nrast_Test_New = rasterio.open(fn_Rast_New)\nrast_plot(rast_Test_New)\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 With Mask Raster\nA second way is provide an existing SpatRaster with the geometry you desire, with special boundary and resolution, this is a better way.\n\n# New CRS\nrast_Mask &lt;- rast(ncol=10, nrow=10, xmin=265000, xmax=270000, ymin=5553000, ymax=5558000)\ncrs(rast_Mask) &lt;- \"EPSG:25833\"\nvalues(rast_Mask) &lt;- 1\n# Reproject\nrast_Test_New &lt;- project(rast_Test, rast_Mask)\n\n# Info and Plot of vector layer\nrast_Test_New\n\nclass       : SpatRaster \nsize        : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 500, 500  (x, y)\nextent      : 265000, 270000, 5553000, 5558000  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRS89 / UTM zone 33N (EPSG:25833) \nsource(s)   : memory\nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\n\nplot(rast_Test)\nplot(rast_Test_New)\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nNew",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#attributes-manipulation",
    "href": "dataprocess/spatial_data.html#attributes-manipulation",
    "title": "Basic Manipulation",
    "section": "5.1 Attributes manipulation",
    "text": "5.1 Attributes manipulation\n\n5.1.1 Extract all Attributes\n\nas.data.frame()\n\n\ndf_Attr &lt;- as.data.frame(vect_Test)\ndf_Attr\n\n  ID_region Name region_area\n1         1    a     8853404\n2         2    b     2208109\n\n\n\n\n5.1.2 Extract one with attribute name\n\n$name\n[, \"name\"]\n\n\nvect_Test$ID_region\n\n[1] 1 2\n\nvect_Test[,\"ID_region\"]\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 1  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region\n type        :     &lt;int&gt;\n values      :         1\n                       2\n\n\n\n\n5.1.3 Add a new attribute\n\n$name &lt;-\n[, \"name\"] &lt;-\n\n\nvect_Test$New_Attr &lt;- c(\"n1\", \"n2\")\nvect_Test[,\"New_Attr\"] &lt;- c(\"n1\", \"n2\")\n\n\n\n5.1.4 Merge several attributes\n\nsame order\n\ncbind()\n\ncommon (key-)attributes\n\nmerge()\n\n\n\ndf_New_Attr &lt;- data.frame(Name = c(\"a\", \"b\"), new_Attr2 = c(9, 6))\n\ncbind(vect_Test, df_New_Attr)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 6  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region  Name region_area New_Attr  Name new_Attr2\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;num&gt;\n values      :         1     a   8.853e+06       n1     a         9\n                       2     b   2.208e+06       n2     b         6\n\nmerge(vect_Test, df_New_Attr, by = \"Name\")\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 5  (geometries, attributes)\n extent      : 4480500, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       :  Name ID_region region_area New_Attr new_Attr2\n type        : &lt;chr&gt;     &lt;int&gt;       &lt;num&gt;    &lt;chr&gt;     &lt;num&gt;\n values      :     a         1   8.853e+06       n1         9\n                   b         2   2.208e+06       n2         6\n\n\n\n\n5.1.5 Delete a attribute\n\n$name &lt;- NULL\n\n\nvect_Test$New_Attr &lt;- c(\"n1\", \"n2\")\nvect_Test[,\"New_Attr\"] &lt;- c(\"n1\", \"n2\")",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#object-append-and-aggregate",
    "href": "dataprocess/spatial_data.html#object-append-and-aggregate",
    "title": "Basic Manipulation",
    "section": "5.2 Object Append and aggregate",
    "text": "5.2 Object Append and aggregate\n\n5.2.1 Append new Objects\n\nRPython\n\n\n\nrbind()\n\n\n# New Vect\n# Define the coordinate reference system (CRS) with EPSG codes\ncrs_31468 &lt;- \"EPSG:31468\"\n\n# Define coordinates for the first polygon\nx_polygon_3 &lt;- c(4480400, 4481222, 4480500)\ny_polygon_3 &lt;- c(5551000, 5550666, 5552555)\ngeometry_polygon_3 &lt;- cbind(id=3, part=1, x_polygon_3, y_polygon_3)\n\n# Create a vector layer for the polygons, specifying their type, attributes, CRS, and additional attributes\nvect_New &lt;- vect(geometry_polygon_3, type=\"polygons\", atts = data.frame(ID_region = 3, Name = c(\"b\")), crs = crs_31468)\nvect_New$region_area &lt;- expanse(vect_New)\n\n# Append the objects\nvect_Append &lt;- rbind(vect_Test, vect_New)\nvect_Append\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 3, 4  (geometries, attributes)\n extent      : 4480400, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n source      : minibeispiel_polygon.geojson\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       : ID_region  Name region_area New_Attr\n type        :     &lt;int&gt; &lt;chr&gt;       &lt;num&gt;    &lt;chr&gt;\n values      :         1     a   8.853e+06       n1\n                       2     b   2.208e+06       n2\n                       3     b   6.558e+05       NA\n\n\n\n\n\npandas.concat()\n\n\n# Define the coordinate reference system (CRS) with EPSG code\ncrs_31468 = \"EPSG:31468\"\n\n# Define coordinates for the new polygon\nx_polygon_3 = [4480400, 4481222, 4480500]\ny_polygon_3 = [5551000, 5550666, 5552555]\n\n# Create a Polygon geometry\ngeometry_polygon_3 = Polygon(zip(x_polygon_3, y_polygon_3))\n\n# Create a GeoDataFrame for the new polygon\nvect_New = gpd.GeoDataFrame({'ID_region': [3], 'Name': ['b'], 'geometry': [geometry_polygon_3]}, crs=crs_31468)\n\n# Calculate the region area\nvect_New['region_area'] = vect_New['geometry'].area\n\n# Append the new GeoDataFrame to the existing one\nvect_Append = gpd.GeoDataFrame(pd.concat([vect_Test, vect_New], ignore_index=True), crs=crs_31468)\n\n# Now, vect_Append contains the combined data\nprint(vect_Append)\n\n   ID_region  ...                                           geometry\n0          1  ...  MULTIPOLYGON (((4484566 5554566, 4483922 55540...\n1          2  ...  MULTIPOLYGON (((4481929 5554933, 4481222 55506...\n2          3  ...  POLYGON ((4480400 5551000, 4481222 5550666, 44...\n\n[3 rows x 4 columns]\n\n\n\n\n\n\n\n5.2.2 Aggregate / Dissolve\nIt is common to aggregate (“dissolve”) polygons that have the same value for an attribute of interest.\n\nRPython\n\n\n\naggregate()\n\n\n# Aggregate by the \"Name\"\nvect_Aggregated &lt;- terra::aggregate(vect_Append, by = \"Name\")\nvect_Aggregated\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 2, 5  (geometries, attributes)\n extent      : 4480400, 4484666, 5550100, 5554933  (xmin, xmax, ymin, ymax)\n coord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \n names       :  Name mean_ID_region mean_region_area New_Attr agg_n\n type        : &lt;chr&gt;          &lt;num&gt;            &lt;num&gt;    &lt;chr&gt; &lt;int&gt;\n values      :     a              1        8.853e+06       n1     1\n                   b            2.5        1.432e+06       NA     2\n\n\nplot(vect_Append, \"ID_region\")\nplot(vect_Aggregated, \"Name\")\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nAggregated\n\n\n\n\n\n\n\n\ngeopandas.dissolve()\n\n\n# Aggregate by the \"Name\"\nvect_Aggregated = vect_Append.dissolve(by=\"Name\", aggfunc=\"first\")\n\nprint(vect_Aggregated)\n\n                                               geometry  ...   region_area\nName                                                     ...              \na     POLYGON ((4484566 5554566, 4483922 5554001, 44...  ...  8.853404e+06\nb     POLYGON ((4480400 5551000, 4480500 5552555, 44...  ...  2.208109e+06\n\n[2 rows x 3 columns]\n\n\n\nvect_Test.plot()\nplt.show()\n\n\n\n\n\n\n\nplt.close()\nvect_Aggregated.plot()\nplt.show()\n\n\n\n\n\n\n\nplt.close()",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#overlap",
    "href": "dataprocess/spatial_data.html#overlap",
    "title": "Basic Manipulation",
    "section": "5.3 Overlap",
    "text": "5.3 Overlap\nTo perform operations that involve overlap between two vector datasets, we will create a new vector dataset:\n\nRPython\n\n\n\nvect_Overlap &lt;- as.polygons(rast_Test)[1,]\nnames(vect_Overlap) &lt;- \"ID_Rast\"\n\nplot(vect_Overlap, \"ID_Rast\")\n\n\n\n\n\n\n\n\n\n\n\n# Read the raster and get the shapes\nrast_Test = rasterio.open(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/minibeispiel_raster.asc\", 'r+')\nrast_Test.crs = CRS.from_epsg(31468)\ntransform = rast_Test.transform\nshapes = rasterio.features.shapes(rast_Test.read(1), transform=transform)\n\n# Convert the shapes to a GeoDataFrame\ngeometries = [shape(s) for s, v in shapes if v == 1]\nvect_Overlap = gpd.GeoDataFrame({'geometry': geometries})\n\n# Add an \"ID_Rast\" column to the GeoDataFrame\nvect_Overlap['ID_Rast'] = range(1, len(geometries) + 1)\nvect_Overlap.crs =\"EPSG:31468\"\n\n# Plot the polygons with \"ID_Rast\" as the attribute\nvect_Overlap.plot(column='ID_Rast')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n5.3.1 Erase\n\nRPython\n\n\n\nerase()\n\n\nvect_Erase &lt;- erase(vect_Test, vect_Overlap)\nplot(vect_Erase, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='difference')\n\n\nvect_Erase = gpd.overlay(vect_Test, vect_Overlap, how='difference')\nvect_Erase.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.2 Intersect\n\nRPython\n\n\n\nintersect()\n\n\nvect_Intersect &lt;- terra::intersect(vect_Test, vect_Overlap)\nplot(vect_Intersect, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='intersection')\n\n\nvect_Intersect = gpd.overlay(vect_Test, vect_Overlap, how='intersection')\nvect_Intersect.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.3 Union\nAppends the geometries and attributes of the input.\n\nRPython\n\n\n\nunion()\n\n\nvect_Union &lt;- terra::union(vect_Test, vect_Overlap)\nplot(vect_Union, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='union')\n\n\nvect_Union = gpd.overlay(vect_Test, vect_Overlap, how='union')\nvect_Union.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.4 Cover\ncover() is a combination of intersect() and union(). intersect returns new (intersected) geometries with the attributes of both input datasets. union appends the geometries and attributes of the input. cover returns the intersection and appends the other geometries and attributes of both datasets.\n\nRPython\n\n\n\ncover()\n\n\nvect_Cover &lt;- terra::cover(vect_Test, vect_Overlap)\nplot(vect_Cover, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='identity')\n\n\nvect_Cover = gpd.overlay(vect_Test, vect_Overlap, how='identity')\nvect_Cover.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()\n\n\n\n\n\n\n5.3.5 Difference\n\nRPython\n\n\n\nsymdif()\n\n\nvect_Difference &lt;- terra::symdif(vect_Test, vect_Overlap)\nplot(vect_Difference, \"ID_region\")\n\n\n\n\n\n\n\n\n\n\n\ngeopandas.overlay(how='symmetric_difference')\n\n\nvect_Difference = gpd.overlay(vect_Test, vect_Overlap, how='symmetric_difference')\nvect_Difference.plot(column='ID_region', cmap='jet')\nplt.show()\n\n\n\n\n\n\n\nplt.close()",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#raster-algebra",
    "href": "dataprocess/spatial_data.html#raster-algebra",
    "title": "Basic Manipulation",
    "section": "6.1 Raster algebra",
    "text": "6.1 Raster algebra\nMany generic functions that allow for simple and elegant raster algebra have been implemented for Raster objects, including the normal algebraic operators such as +, -, *, /, logical operators such as &gt;, &gt;=, &lt;, ==, !, and functions like abs, round, ceiling, floor, trunc, sqrt, log, log10, exp, cos, sin, atan, tan, max, min, range, prod, sum, any, all. In these functions, you can mix raster objects with numbers, as long as the first argument is a raster object. (Spatial Data Science)\n\nRPython\n\n\n\nrast_Add &lt;- rast_Test + 10\nplot(rast_Add)\n\n\n\n\n\n\n\n\n\n\n\nrast_Add = rast_Test_data + 10\nrast_plot(rast_Add)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#replace-with-condition",
    "href": "dataprocess/spatial_data.html#replace-with-condition",
    "title": "Basic Manipulation",
    "section": "6.2 Replace with Condition",
    "text": "6.2 Replace with Condition\n\nRPython\n\n\n\nrast[condition] &lt;-\n\n\n# Copy to a new raster\nrast_Replace &lt;- rast_Test\n\n# Replace\nrast_Replace[rast_Replace &gt; 1] &lt;- 10\nplot(rast_Replace)\n\n\n\n\n\n\n\n\n\n\n\nrast[condition] =\n\n\nrast_Replace = rast_Test_data\n\n# Replace values greater than 1 with 10\nrast_Replace[rast_Replace &gt; 1] = 10\nrast_plot(rast_Replace)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#summary-of-multi-layers",
    "href": "dataprocess/spatial_data.html#summary-of-multi-layers",
    "title": "Basic Manipulation",
    "section": "6.3 Summary of multi-layers",
    "text": "6.3 Summary of multi-layers\n\nRPython\n\n\n\nrast_Mean &lt;- mean(rast_Test, rast_Replace)\nplot(rast_Mean)\n\n\n\n\n\n\n\n\n\n\n\nrast_Mean = (rast_Test_data + rast_Replace) / 2\nrast_plot(rast_Mean)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#aggregate-and-disaggregate",
    "href": "dataprocess/spatial_data.html#aggregate-and-disaggregate",
    "title": "Basic Manipulation",
    "section": "6.4 Aggregate and disaggregate",
    "text": "6.4 Aggregate and disaggregate\n\nRPython\n\n\n\naggregate()\ndisagg()\n\n\n# Aggregate by factor 2\nrast_Aggregate &lt;- aggregate(rast_Test, 2)\nplot(rast_Aggregate)\n\n\n\n\n\n\n\n# Disaggregate by factor 2\nrast_Disagg &lt;- disagg(rast_Test, 2)\nrast_Disagg\n\nclass       : SpatRaster \nsize        : 10, 10, 1  (nrow, ncol, nlyr)\nresolution  : 500, 500  (x, y)\nextent      : 4480000, 4485000, 5550000, 5555000  (xmin, xmax, ymin, ymax)\ncoord. ref. : DHDN / 3-degree Gauss-Kruger zone 4 (EPSG:31468) \nsource(s)   : memory\nvarname     : minibeispiel_raster \nname        : minibeispiel_raster \nmin value   :                   1 \nmax value   :                   3 \n\nplot(rast_Disagg)\n\n\n\n\n\n\n\n\n\n\n\n# Aggregate by factor 2\nrast_Aggregate = rast_Test_data\nrast_Aggregate = rast_Aggregate[::2, ::2]\nrast_plot(rast_Aggregate)\n\n\n\n\n\n\n\n# Disaggregate by factor 2\nrast_Disagg = rast_Test_data\nrast_Disagg = np.repeat(np.repeat(rast_Disagg, 2, axis=0), 2, axis=1)\nrast_plot(rast_Disagg)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#crop",
    "href": "dataprocess/spatial_data.html#crop",
    "title": "Basic Manipulation",
    "section": "6.5 Crop",
    "text": "6.5 Crop\nThe crop function lets you take a geographic subset of a larger raster object with an extent. But you can also use other spatial object, in them an extent can be extracted.\n\ncrop()\n\nwith extention\nwith rster\nwith vector\n\n\n\nrast_Crop &lt;- crop(rast_Test, vect_Test[1,])\nplot(rast_Crop)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#trim",
    "href": "dataprocess/spatial_data.html#trim",
    "title": "Basic Manipulation",
    "section": "6.6 Trim",
    "text": "6.6 Trim\n\ntrim()\n\nTrim (shrink) a SpatRaster by removing outer rows and columns that are NA or another value.\n\nrast_Trim0 &lt;- rast_Test\nrast_Trim0[21:25] &lt;- NA\nrast_Trim &lt;- trim(rast_Trim0)\n\nplot(rast_Trim0)\nplot(rast_Trim)\n\n\n\n\n\n\nwith NA\n\n\n\n\n\n\n\nTrimed",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/spatial_data.html#mask",
    "href": "dataprocess/spatial_data.html#mask",
    "title": "Basic Manipulation",
    "section": "6.7 Mask",
    "text": "6.7 Mask\n\nRPython\n\n\n\nmask()\ncrop(mask = TRUE) = mask() + trim()\n\nWhen you use mask manipulation in spatial data analysis, it involves setting the cells that are not covered by a mask to NA (Not Available) values. If you apply the crop(mask = TRUE) operation, it means that not only will the cells outside of the mask be set to NA, but the resulting raster will also be cropped to match the extent of the mask.\n\nrast_Mask &lt;- mask(rast_Disagg, vect_Test[1,])\nrast_CropMask &lt;- crop(rast_Disagg, vect_Test[1,], mask = TRUE)\n\nplot(rast_Mask)\nplot(rast_CropMask)\n\n\n\n\n\n\nMask\n\n\n\n\n\n\n\nMask + Crop (Trim)\n\n\n\n\n\n\n\n\nvect_Mask = vect_Test.iloc[0:1].geometry.values[0]\n\n# Create a mask for the vect_Mask on the raster\nrast_Mask = rasterio.features.geometry_mask([vect_Mask], out_shape=rast_Test.shape, transform=rast_Test.transform, invert=True)\n# Apply the mask to the raster\nrast_Crop = rast_Test_data.copy()\nrast_Crop[~rast_Mask] = rast_Test.nodata  # Set values outside the geometry to nodata\n\nrast_plot(rast_Crop)",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Basic Manipulation"
    ]
  },
  {
    "objectID": "dataprocess/index.html",
    "href": "dataprocess/index.html",
    "title": "Data Processing",
    "section": "",
    "text": "Data processing is a fundamental step in transforming raw data into valuable insights. In the domain of hydrology, data often involves spatial and temporal aspects, making it complex and voluminous. Effective processing tools and methods are essential to successfully analyze and utilize this data. This page provides valuable technological skills for processing raw data.",
    "crumbs": [
      "Dataprocess"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html",
    "href": "dataprocess/eda_basic.html",
    "title": "Exploratory data analysis (EDA)",
    "section": "",
    "text": "Before conducting any formal analysis, it is essential to first develop a preliminary understanding of the dataset. This step involves exploring the basic structure, summarizing key characteristics, and evaluating data quality. Therefore, Exploratory Data Analysis (EDA) serves as a crucial first stage in any data analysis process.\nDuring the initial phase of EDA, it is encouraged to freely investigate any hypothesis or question that arises (DataR_Wickham_2023?).\nStatistical summaries provide the most fundamental insights into a dataset and are typically the easiest to obtain. More comprehensive summaries—such as quantiles, distributions, and visual representations—are discussed in the companion sections Statistic Basics and Graphical Statistics.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#dataset-description",
    "href": "dataprocess/eda_basic.html#dataset-description",
    "title": "Exploratory data analysis (EDA)",
    "section": "2.1 Dataset Description",
    "text": "2.1 Dataset Description\nFor demonstration purposes, we will use a synthetic dataset derived from daily temperature data recorded at the Düsseldorf station of the DWD, covering the period from 2005 to 2024.\nIn the correlation section, we will additionally use discharge data from two stations located in the Ruhr River basin.\nThe following R packages are required and will be loaded below:\n\n# Load required libraries for data manipulation, visualization, and time series handling\nlibrary(tidyverse)   # Data wrangling and visualization framework\ntheme_set(theme_bw()) # Set a clean and minimal default ggplot2 theme\nlibrary(plotly)      # Interactive visualization\nlibrary(xts)         # Time series data structure and analysis\nlibrary(patchwork)   # Combine multiple ggplots into one layout\n\nTo visualize the dataset in its basic form, we will define a few helper functions that plot the time series and display key patterns in the data.\n\n\nCode\ncolor_RUB_blue &lt;- \"#17365c\"\ncolor_RUB_green &lt;- \"#8dae10\"\ncolor_TUD_middleblue &lt;- \"#006ab2\"\ncolor_TUD_lightblue &lt;- \"#009de0\"\ncolor_TUD_green &lt;- \"#007d3f\"\ncolor_TUD_lightgreen &lt;- \"#69af22\"\ncolor_TUD_orange &lt;- \"#ee7f00\"\ncolor_TUD_pink &lt;- \"#EC008D\"\ncolor_TUD_purple &lt;- \"#54368a\"\ncolor_TUD_redpurple &lt;- \"#93107d\"\ncolor_SafetyOrange &lt;- \"#ff5e00\"\n\nplotly_xts_single &lt;- function(xts_Data) {\n  # Check input\n  if (!xts::is.xts(xts_Data))\n    stop(\"Input must be an xts object.\")\n  if (NCOL(xts_Data) != 1)\n    stop(\"xts object must contain exactly one column.\")\n  \n  # Extract data frame for ggplot\n  df &lt;- data.frame(\n    Date = zoo::index(xts_Data),\n    Value = as.numeric(xts_Data)\n  )\n  colname &lt;- colnames(xts_Data)[1]\n  \n  # Create ggplot\n  p &lt;- ggplot(df, aes(x = Date, y = Value)) +\n    geom_line(color = \"#17365c\") +\n    labs(\n      xts_Data = \"Date\",\n      y = colname\n    )\n  ggplotly(p)\n}\n\nplot_xts_outlier &lt;- function(xts_Data, idx_Outlier) {\n  # Input checks\n  if (!xts::is.xts(xts_Data))\n    stop(\"Input must be an xts object.\")\n  if (NCOL(xts_Data) != 1)\n    stop(\"xts object must contain exactly one column.\")\n  \n  # Align the two time series to the common time range\n\n  xts_Data_Outlier &lt;- xts_Data\n  xts_Data_Outlier[] &lt;- NA\n  xts_Data_Outlier[idx_Outlier] &lt;- xts_Data[idx_Outlier]\n  xts_Data[idx_Outlier] &lt;- NA\n  # Create ggplot\n  p &lt;- ggplot() +\n    geom_line(aes(index(xts_Data), y = xts_Data, color = \"Normal\")) +\n    geom_line(aes(index(xts_Data_Outlier), y = xts_Data_Outlier, color = \"Outlier\")) +\n    scale_color_manual(\"\", values = c(\"Outlier\" = \"#EC008D\", \"Normal\" = \"#17365c\")) +\n    labs(\n      xts_Data = \"Date\",\n      y = colnames(xts_Data)\n    )\n  \n  # Convert to interactive plotly\n  ggplotly(p)\n}\n\n\nA basic overview of the dataset is presented below using an interactive line plot, which allows us to visually explore the temporal variation of daily temperature values over the observation period.\n\n# Read the synthetic temperature dataset and convert it into an xts time series object\nxts_Outlier &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Outlier.csv\") |&gt; \n  as.xts()\n\n\n# Visualize the daily temperature time series interactively\nplotly_xts_single(xts_Outlier)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#preliminary-global-checks-for-invalid-data",
    "href": "dataprocess/eda_basic.html#preliminary-global-checks-for-invalid-data",
    "title": "Exploratory data analysis (EDA)",
    "section": "2.2 Preliminary Global Checks for Invalid Data",
    "text": "2.2 Preliminary Global Checks for Invalid Data\n\n2.2.1 Marked Values\nIn some datasets, specific “marked” values are used to indicate special meanings, often falling outside the normal measurement range. For example:\n- -999 may represent missing data,\n- -777 may indicate extremely high or low measurements.\nThese coded values must be identified and handled appropriately before analysis. If no additional information or suitable replacements are available and the dataset is sufficiently large, these marked values can be directly treated as unknown. In practice, this means replacing codes like -999 or -777 with proper missing value indicators (e.g., NA in R) so that statistical software correctly interprets them instead of treating them as valid numeric values.\nTypically, marked values are documented in a metadata file, README, or on the data provider’s website.\nIf no documentation exists, they can often be detected through basic visual inspection, appearing as extreme outliers or long constant sequences in a time series plot.\nTo replace these marked values, a simple equality check (==) can locate them, and they can then be replaced with NA:\n\n# Extract a subset of the dataset for 2005–2006\nxts_Outlier_Marked &lt;- xts_Outlier[\"2005-01-01/2006-12-31\"]\n\n# Identify marked values (-999) and replace them with NA\nidx_Outlier_Marked &lt;- which(xts_Outlier_Marked == -999)\nxts_Outlier_Marked[idx_Outlier_Marked] &lt;- NA\n\nThe following plot illustrates this process: the original data (with marked values) is shown in pink, while the cleaned version (after replacing marked values with NA) is shown in dark blue.\n\nplot_xts_outlier(xts_Outlier[\"2005-01-01/2006-12-31\"], idx_Outlier_Marked)\n\n\n\n\n\n\n\n2.2.2 Basic Rational Range\nMost measured variables have a reasonable or “rational” range. For example:\n\nEnvironmental variables like precipitation or discharge should always be positive.\nAir temperature has a wide global range (e.g., −50°C to 50°C), but for a specific location, this range can be narrowed.\n\nTo filter unrealistic values, we can restrict all measurements to fall within a defined interval. For the Düsseldorf station, a reasonable daily temperature range is -15 °C to 50 °C. Values outside this range can be identified using logical operators (&gt;, &lt;) and replaced with NA to indicate invalid data:\n\n# Extract a subset for the year 2012\nxts_Outlier_Range &lt;- xts_Outlier[\"2012-01-01/2012-12-31\"]\n\n# Identify values outside the rational range and set them to NA\nidx_outlier_Range &lt;- which(xts_Outlier_Range &gt; 38 | xts_Outlier_Range &lt; -15)\nxts_Outlier_Range[idx_outlier_Range] &lt;- NA\n\nThe cleaned results after applying this rational range filter are shown below:\n\nplot_xts_outlier(xts_Outlier[\"2012-01-01/2012-12-31\"], idx_outlier_Range)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#outlier-identification-with-statistical-methods",
    "href": "dataprocess/eda_basic.html#outlier-identification-with-statistical-methods",
    "title": "Exploratory data analysis (EDA)",
    "section": "2.3 Outlier Identification with Statistical Methods",
    "text": "2.3 Outlier Identification with Statistical Methods\nIn addition to global range checks, general statistical methods can help identify unusual values more objectively.\nThese approaches are particularly useful for obtaining a first overview of the dataset, especially when we lack deep prior knowledge.\n\n2.3.1 Z-Score Method\nThe Z-score method measures how far each data point is from the mean, in units of standard deviation.\nValues with very high or very low Z-scores are far from the center of the distribution and may be considered outliers.\nA common rule of thumb is that points with an absolute Z-score greater than 3 are potential outliers.\nNote that this threshold assumes approximately normal data — for non-normal distributions, the Z-score may be less reliable.\n\n2.3.1.1 Checking the Normality of Temperature at the Station\n\n# Extract temperature data from 2013–2024\nnum_Hist &lt;- xts_Outlier[\"2013-01-01/2024-12-31\"] |&gt; as.numeric()\n\n# Plot histogram\nggplot() +\n  geom_histogram(aes(num_Hist), color = color_RUB_blue, fill = color_RUB_green) +\n  labs(x = \"Temperature (°C)\", y = \"Frequency\", title = \"Histogram of Daily Temperature\")\n\n\n\n\n\n\n\n\nThe Shapiro-Wilk test evaluates whether a variable follows a normal distribution. It produces two key outputs:\n\nW (Shapiro-Wilk statistic): measures how closely the sample data resemble a normal distribution. Values near 1 indicate approximate normality.\np-value: indicates the statistical significance of the deviation from normality. A small p-value (typically &lt; 0.05) suggests significant deviation from normality.\n\n\nNote: The W statistic reflects the degree of normality, while the p-value also considers sample size. Large datasets can produce very small p-values even when W is close to 1.\n\n\n# Perform Shapiro-Wilk test for normality\nshapiro.test(num_Hist)\n\n\n    Shapiro-Wilk normality test\n\ndata:  num_Hist\nW = 0.9919, p-value = 4.143e-15\n\n\nAlthough the p-value may be smaller than 0.05, combining it with the histogram and W-value shows that the data are sufficiently symmetric for Z-score based outlier detection to remain useful.\n\n\n2.3.1.2 Outlier ckeck with Z-Score\n\n# Subset the data for 2012\nxts_Outlier_Statistic_Z &lt;- xts_Outlier[\"2012-01-01/2012-12-31\"]\n\n# Compute Z-scores and identify outliers\nnum_ZScores &lt;- scale(xts_Outlier_Statistic_Z)\nthreshold_Z &lt;- 3\nidx_Outlier_Z &lt;- abs(num_ZScores) &gt; threshold_Z\n\n# Replace detected outliers with NA\nxts_Outlier_Statistic_Z[idx_Outlier_Z] &lt;- NA\n\n\n# Plot time series after Z-score outlier removal\nplot_xts_outlier(\n  xts_Outlier[\"2012-01-01/2012-12-31\"],\n  idx_Outlier_Z\n)\n\n\n\n\n\n\n\n\n2.3.2 Interquartile Range (IQR) Method\nThe Interquartile Range (IQR) method is another widely used approach for outlier detection. It focuses on the spread of the middle 50% of the data (between the first quartile Q1 and third quartile Q3). The IQR is calculated as:\nThe IQR is calculated as:\n\\[\n\\text{IQR} = Q3 - Q1\n\\]\nOutliers are typically defined as values lying below or above:\n\\[\n\\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR} \\quad , \\quad\n\\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR}\n\\]\nValues outside this range are considered potential outliers and can be handled appropriately.\n\n# Subset the data for the year 2012\nxts_Outlier_Statistic_IQR &lt;- xts_Outlier[\"2012-01-01/2012-12-31\"]\n\n# Calculate the first (Q1) and third (Q3) quartiles\nnum_Q1 &lt;- quantile(xts_Outlier_Statistic_IQR, 0.25, na.rm = TRUE)\nnum_Q3 &lt;- quantile(xts_Outlier_Statistic_IQR, 0.75, na.rm = TRUE)\n\n# Compute the Interquartile Range (IQR)\nnum_IQR &lt;- num_Q3 - num_Q1\n\n# Define lower and upper bounds for outlier detection\nbound_Lower &lt;- num_Q1 - 1.5 * num_IQR\nbound_Upper &lt;- num_Q3 + 1.5 * num_IQR\n\n# Identify outliers outside the IQR bounds\nidx_Outlier_IQR &lt;- (xts_Outlier_Statistic_IQR &lt; bound_Lower) | (xts_Outlier_Statistic_IQR &gt; bound_Upper)\n\n# Replace identified outliers with NA\nxts_Outlier_Statistic_IQR[idx_Outlier_IQR] &lt;- NA\n\n\n# Plot the time series after removing IQR-based outliers\nplot_xts_outlier(\n  xts_Outlier[\"2012-01-01/2012-12-31\"],\n  idx_Outlier_IQR\n)\n\n\n\n\n\n\n\nCode\n# Visualize outliers in a boxplot\nggplot() +\n  geom_boxplot(\n    aes(x = xts_Outlier[\"2012-01-01/2012-12-31\"] |&gt; as.numeric()),  # Convert time series to numeric vector for plotting\n    color = \"#17365c\",                     # Box color\n    outlier.color = \"#EC008D\"              # Highlight outliers in pink\n  ) +\n  labs(\n    x = \"Temperature (°C)\",\n    y = NULL,\n    title = \"Boxplot of Daily Temperature with Outliers Highlighted\"\n  )",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#outlier-identification-with-additional-knowledge",
    "href": "dataprocess/eda_basic.html#outlier-identification-with-additional-knowledge",
    "title": "Exploratory data analysis (EDA)",
    "section": "2.4 Outlier Identification with Additional Knowledge",
    "text": "2.4 Outlier Identification with Additional Knowledge\nIn many cases, outliers are more complex and cannot be detected using simple statistical rules alone.\nTo identify these unusual values effectively, we need to combine domain knowledge, experience, and a deeper understanding of the data.\n\n2.4.1 Constant Sequences over Long Periods in Time Series\nSome unusual situations in time series data appear as constant values over long periods.\nThese may indicate sensor malfunctions, data transmission errors, or missing updates.\nDetecting such patterns is an important part of exploratory data analysis (EDA).\n\n# Subset data for 2010\nxts_Outlier_TS_Const &lt;- xts_Outlier[\"2010-01-01/2010-12-31\"]\n\n# Identify sequences of repeated values\nint_Equal &lt;- rle(as.numeric(xts_Outlier_TS_Const))\nn_Threshold &lt;- 7  # Threshold for long constant sequences\nidx_Long &lt;- which(int_Equal$lengths &gt;= n_Threshold)\n\n# Convert run-length indices to actual positions\nidx_Outlier_Const &lt;- unlist(\n  lapply(idx_Long, \\(i) {\n    start_pos &lt;- sum(int_Equal$lengths[seq_len(i - 1)]) + 1\n    end_pos &lt;- sum(int_Equal$lengths[seq_len(i)])\n    start_pos:end_pos\n  })\n)\n\n# Replace long constant sequences with NA\nxts_Outlier_TS_Const[idx_Outlier_Const] &lt;- NA\n\n\n# Plot time series highlighting constant sequence outliers\nplot_xts_outlier(xts_Outlier[\"2010-01-01/2010-12-31\"], idx_Outlier_Const)\n\n\n\n\n\n\n\n2.4.2 Out of Trend or Seasonality\nTime series data typically consist of trend, seasonality, and residual components. By examining the trend and seasonal patterns, we can detect outliers that appear within global ranges but deviate strongly from expected patterns.\nIn order to detedc the distance between raw values and tresnd, the remeinder vlaues (Raw - Trend) could be used to located the outlier.\n\n# Subset 2008 data and compute residuals after removing trend\nxts_Outlier_TS_Trend &lt;- xts_Outlier[\"2008-01-01/2008-12-31\"]\nxts_Trend &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_TrendSeason.csv\") |&gt; as.xts()\n# Remaeinder\nxts_Remainder &lt;- xts_Outlier_TS_Trend - xts_Trend\n\nFor the continued anayls we will with IQR-Apporch:\n\nnum_Q1 &lt;- quantile(xts_Remainder, 0.25, na.rm = TRUE)\nnum_Q3 &lt;- quantile(xts_Remainder, 0.75, na.rm = TRUE)\n\n# Compute the Interquartile Range (IQR)\nnum_IQR &lt;- num_Q3 - num_Q1\n\n# Define lower and upper bounds for outlier detection\nbound_Lower &lt;- num_Q1 - 1.5 * num_IQR\nbound_Upper &lt;- num_Q3 + 1.5 * num_IQR\n\n# Identify outliers outside the IQR bounds\nidx_Outlier_Trend &lt;- (xts_Remainder &lt; bound_Lower) | (xts_Remainder &gt; bound_Upper)\n\n# Replace identified outliers with NA\nxts_Outlier_TS_Trend[idx_Outlier_Trend] &lt;- NA\n\n\n\nCode\n# Plot raw data and seasonal trend\nxts_Outlier_Trend &lt;- (xts_Outlier[\"2008-01-01/2008-12-31\"])[idx_Outlier_Trend]\n  # Create ggplot\n  gp_Trend &lt;- ggplot() +\n    geom_line(aes(index(xts_Outlier_TS_Trend), y = xts_Outlier_TS_Trend, color = \"Normal\")) +\n    geom_line(aes(index(xts_Outlier_Trend), y = xts_Outlier_Trend, color = \"Outlier\")) +\n    geom_line(aes(index(xts_Trend), y = xts_Trend, color = \"Seasonality\")) +\n    scale_color_manual(\"\", values = c(\"Outlier\" = \"#EC008D\", \"Normal\" = \"#17365c\", \"Seasonality\" = \"#8dae10\")) +\n    labs(\n      xts_Data = \"Date\",\n      y = \"Temperature (°C)\"\n    )\n  \n  # Convert to interactive plotly\n  ggplotly(gp_Trend)\n\n\n\n\n\n\n\n\nCode\n# Prepare data for combined boxplot\ndf_Box_Trend &lt;- data.frame(\n  Value = c(as.numeric(xts_Outlier_TS_Trend), as.numeric(xts_Remainder)),\n  Component = rep(c(\"Original\", \"Remainder (Seasonality Removed)\"),\n                  each = NROW(xts_Outlier_TS_Trend))\n)\n\n# Boxplot of original and residual data\nggplot(df_Box_Trend, aes(y = Value)) +\n  geom_boxplot(color = \"#17365c\", outlier.colour = \"#EC008D\") +\n  facet_wrap(~ Component, ncol = 2, scales = \"free_y\") +\n  labs(y = \"Temperature (°C)\", x = NULL) +\n  theme_bw() +\n  theme(\n    axis.x.title = element_blank(),\n    axis.x.text = element_blank(),\n    axis.x.ticks = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    strip.background = element_rect(fill = \"gray90\", color = NA),\n    strip.text = element_text(face = \"bold\"),\n    plot.title = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nAs the two plots at above showed, in the raw tiem serise, the values between 01.11.2008 and 20.11.2008 will not be rechgnized as outlier, but wenn wen check with the seasonality information, the outlier situaion is scatuall verry obvirous.\n\n\n2.4.3 Relation with Additional Variables\nOutliers can also be identified by examining relationships between variables. A data point far from the expected relationship between two or more correlated variables may indicate unusual behavior.\nFor the Ruhr Basin, two stations—upstream (Bachum) and downstream (Villigst)—usually exhibit a strong correlation (~0.99). However, Bachum occasionally reports unusually high values. Comparing with Villigst allows detection of these outliers.\n\n# Load discharge data for both stations\nxts_Bachum &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Bachum.csv\") |&gt; as.xts()\nxts_Villigst &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Villigst.csv\") |&gt; as.xts()\n\n\n\nCode\n# Prepare time series and scatter plot data\ndf_Bachum &lt;- data.frame(Date = index(xts_Bachum), Value = as.numeric(xts_Bachum[,1]), Station = \"Bachum\")\ndf_Villigst &lt;- data.frame(Date = index(xts_Villigst), Value = as.numeric(xts_Villigst[,1]), Station = \"Villigst\")\ndf_Relation &lt;- bind_rows(df_Bachum, df_Villigst)\n\n# Time series plot\ngp_TSline_Rrelation &lt;- ggplot(df_Relation, aes(x = Date, y = Value, color = Station)) +\n  geom_line(linewidth = 0.5) +\n  facet_wrap(~Station, ncol = 1, scales = \"free_y\") +\n  scale_color_manual(values = c(\"Bachum\" = \"#1f77b4\", \"Villigst\" = \"#17365c\")) +\n  labs(x = NULL, y = \"Discharge (m³/s)\") +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n# Scatter plot of Bachum vs Villigst\ndf_Scatter &lt;- data.frame(Bachum = as.numeric(xts_Bachum), Villigst = as.numeric(xts_Villigst))\ngp_Scatter_Relation &lt;- ggplot(df_Scatter, aes(x = Villigst, y = Bachum)) +\n  geom_point(alpha = 0.4, color = \"#8dae10\") +\n  geom_abline(intercept = 4.1699263, slope = 0.7132356, color = \"#EC008D\") +\n  labs(x = \"Villigst\", y = \"Bachum\")\n\n# Combine both plots using patchwork\n(gp_TSline_Rrelation | gp_Scatter_Relation) +\n  plot_layout(widths = c(3,2))",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#filling-gaps-with-constant-values",
    "href": "dataprocess/eda_basic.html#filling-gaps-with-constant-values",
    "title": "Exploratory data analysis (EDA)",
    "section": "3.1 Filling Gaps with Constant Values",
    "text": "3.1 Filling Gaps with Constant Values\nThe simplest method is to fill missing values with a global statistic such as the mean or median. This approach preserves the overall statistical properties of the dataset and is easy to apply.\nHowever, the disadvantage is also clear: such replacements do not reflect local variations, trends, or relationships with other variables.\n\n# Calculate mean and median of non-missing values\nnum_Mean &lt;- mean(xts_Missing_T, na.rm = TRUE)\nnum_Median &lt;- median(xts_Missing_T, na.rm = TRUE)\n\n# Create copies for mean and median imputation\nxts_Missing_T_Mean &lt;- xts_Missing_T\nxts_Missing_T_Median &lt;- xts_Missing_T\n\n# Identify positions of missing values\nidx_Missing &lt;- is.na(xts_Missing_T)\n\n# Replace missing values with mean or median\nxts_Missing_T_Mean[idx_Missing] &lt;- num_Mean\nxts_Missing_T_Median[idx_Missing] &lt;- num_Median\n\n\n\nCode\n# Extract filled values for plotting\nxts_Filling_Mean &lt;- xts_Missing_T_Mean[idx_Missing]\nxts_Filling_Median &lt;- xts_Missing_T_Median[idx_Missing]\n\n# Plot original series and imputed values\ngp_Fill1 &lt;- ggplot() +\n  geom_line(aes(x = index(xts_Missing_T), y = xts_Missing_T), color = color_RUB_blue) +\n  geom_line(aes(x = index(xts_Filling_Mean), y = xts_Filling_Mean, color = \"Mean\")) +\n  geom_line(aes(x = index(xts_Filling_Median), y = xts_Filling_Median, color = \"Median\")) +\n  scale_color_manual(\"\", values = c(\"Mean\" = color_TUD_pink, \"Median\" = color_RUB_green)) +\n  labs(x = \"Date\", y = \"Temperature (°C)\")\n\n# Convert ggplot to an interactive plotly object\nggplotly(gp_Fill1)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#simple-interpolation-in-the-time-dimension",
    "href": "dataprocess/eda_basic.html#simple-interpolation-in-the-time-dimension",
    "title": "Exploratory data analysis (EDA)",
    "section": "3.2 Simple Interpolation in the Time Dimension",
    "text": "3.2 Simple Interpolation in the Time Dimension\nFor most state variables, which are strongly influenced by their previous states (e.g., temperature, river discharge, or other continuous processes), we can fill gaps by interpolating between neighboring values under the assumption of temporal continuity.\nCommon interpolation approaches include: - Using the previous (forward) value\n- Using the next (backward) value\n- Using the mean of the forward and backward values\n- Linear interpolation between adjacent observations\nAmong these, linear interpolation is the most commonly used method in practice.\n\n# Identify positions of missing values\nidx_Missing &lt;- which(is.na(xts_Missing_T))\n\n# Determine indices of values before and after the missing block\nidx_Forward &lt;- idx_Missing[1] - 1           # Index just before the first missing value\nidx_Backward &lt;- idx_Missing[length(idx_Missing)] + 1  # Index just after the last missing value\n\n# Extract the corresponding temperature values\nnum_Forward &lt;- xts_Missing_T[idx_Forward]   # Value before missing section\nnum_Backward &lt;- xts_Missing_T[idx_Backward] # Value after missing section\n\n# Compute mean of forward and backward values\nnum_MeanForBack &lt;- mean(c(num_Forward, num_Backward))\n\n# Create copies for each filling approach\nxts_Missing_T_MeanForBack &lt;- xts_Missing_T\nxts_Missing_T_Forward &lt;- xts_Missing_T\nxts_Missing_T_Backward &lt;- xts_Missing_T\n\n# Fill missing values using different methods\nxts_Missing_T_Forward[idx_Missing] &lt;- num_Forward          # Fill with overall mean\nxts_Missing_T_Backward[idx_Missing] &lt;- num_Backward      # Fill with overall median\nxts_Missing_T_MeanForBack[idx_Missing] &lt;- num_MeanForBack  # Fill with mean of adjacent values\n\n\n# Create copies for linear interpolation approach\nxts_Missing_T_Linear &lt;- xts_Missing_T\n# Fill missing values by linear interpolation\nxts_Missing_T_Linear &lt;- na.approx(xts_Missing_T)\n\n\n\nCode\n# Extract filled values for plotting\nxts_Filling_Forward &lt;- xts_Missing_T_Forward[idx_Missing]\nxts_Filling_Backward &lt;- xts_Missing_T_Backward[idx_Missing]\nxts_Filling_MeanForBack &lt;- xts_Missing_T_MeanForBack[idx_Missing]\nxts_Filling_Linear &lt;- xts_Missing_T_Linear[idx_Missing]\n\n# Plot original series and filled values\ngp_Fill2 &lt;- ggplot() +\n  geom_line(aes(x = index(xts_Missing_T), y = xts_Missing_T),\n            color = color_RUB_blue) +\n  geom_line(aes(x = index(xts_Filling_Forward), y = xts_Filling_Forward, color = \"Forward\")) +\n  geom_line(aes(x = index(xts_Filling_Backward), y = xts_Filling_Backward, color = \"Backward\")) +\n  geom_line(aes(x = index(xts_Filling_MeanForBack), y = xts_Filling_MeanForBack, color = \"Forward–Backward Mean\")) +\n  geom_line(aes(x = index(xts_Filling_Linear), y = xts_Filling_Linear, color = \"Linear interpolation\")) +\n  scale_color_manual(\n    \"\",\n    values = c(\n      \"Forward\" = color_TUD_pink,\n      \"Backward\" = color_RUB_green,\n      \"Forward–Backward Mean\" = color_SafetyOrange,\n      \"Linear interpolation\" = color_TUD_lightblue\n    )\n  ) +\n  labs(x = \"Date\", y = \"Temperature (°C)\")\n\n# Convert ggplot to interactive plotly\nggplotly(gp_Fill2)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/eda_basic.html#model-based-methods",
    "href": "dataprocess/eda_basic.html#model-based-methods",
    "title": "Exploratory data analysis (EDA)",
    "section": "3.3 Model-Based Methods",
    "text": "3.3 Model-Based Methods\nBy analyzing the time series components—such as trend and seasonality—or by exploring relationships with additional variables, we can construct simple models to estimate missing values. In such cases, the available data serve as model inputs, while the identified trend or relationship acts as a predictive model to fill the unknown values.\n\n3.3.1 Filling with Trend and Seasonality\nIf the time series has a clear trend and seasonal pattern, missing values can be estimated using the fitted trend and seasonality components. This ensures that the filled data remain consistent with the temporal structure of the dataset.\n\n# Load pre-calculated trend and seasonality components\nxts_TrendSeason &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_TrendSeason.csv\") |&gt; as.xts()\n# Create copies for linear interpolation approach\nxts_Missing_T_TrendSeason &lt;- xts_Missing_T\n# Fill missing values using the corresponding trend component\nxts_Missing_T_TrendSeason[idx_Missing] &lt;- xts_TrendSeason[idx_Missing]\n\n\n\nCode\n# Extract filled values for plotting\nxts_Filling_TrendSeason &lt;- xts_Missing_T_TrendSeason[idx_Missing]\nxts_Filling_Linear &lt;- xts_Missing_T_Linear[idx_Missing]\n\n# Plot original series and filled values\ngp_Fill3 &lt;- ggplot() +\n  geom_line(aes(x = index(xts_Missing_T), y = xts_Missing_T),\n            color = color_RUB_blue) +\n  geom_line(aes(x = index(xts_Filling_TrendSeason), y = xts_Filling_TrendSeason, color = \"Trend + Seasonality\")) +\n  geom_line(aes(x = index(xts_Filling_Linear), y = xts_Filling_Linear, color = \"Linear interpolation\")) +\n  scale_color_manual(\n    \"\",\n    values = c(\n      \"Trend + Seasonality\" = color_RUB_green,\n      \"Linear interpolation\" = color_TUD_lightblue\n    )\n  ) +\n  labs(x = \"Date\", y = \"Temperature (°C)\")\n\n# Convert ggplot to interactive plotly\nggplotly(gp_Fill3)\n\n\n\n\n\n\n\n\n3.3.2 Filling with Linear Relationships\nWhen there is a strong linear relationship between variables, we can use regression or correlation-based models to predict missing values. For example, missing discharge values might be estimated based on precipitation or temperature using a simple linear model.\n\n# Load time series data\nxts_Bachum_Missing &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Bachum_Missing.csv\") |&gt; as.xts()\nxts_Villigst &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Villigst.csv\") |&gt; as.xts()\n\n# Identify indices of missing values in Bachum series\nidx_Missing_Q &lt;- which(is.na(xts_Bachum_Missing))\n\n# Compute correlation between Bachum and Villigst, ignoring missing values\ncor(xts_Bachum_Missing, xts_Villigst, use = \"complete.obs\") # 0.9847129\n\n          X2\nV1 0.9847129\n\n# Fit linear regression to predict Bachum from Villigst\nlm_Bachum_Villigst &lt;- lm(xts_Bachum_Missing ~ xts_Villigst)\nlm_Coeffi &lt;- lm_Bachum_Villigst$coefficients # Intercept = b, Slope = a\n\n# Fill missing Bachum values using regression based on Villigst\nxts_Bachum_Missing_Corelation &lt;- xts_Bachum_Missing\nxts_Bachum_Missing_Corelation[idx_Missing_Q] &lt;- \n  lm_Coeffi[1] + lm_Coeffi[2] * xts_Villigst[idx_Missing_Q]  # y = a*x + b\n\n# Fill missing Bachum values using linear interpolation\nxts_Bachum_Missing_LinearInterpolate &lt;- na.approx(xts_Bachum_Missing)\n\n\n\nCode\n# Extract filled values for plotting\nxts_Filling_Corelation &lt;- xts_Bachum_Missing_Corelation[idx_Missing_Q]\nxts_Filling_Linear &lt;- xts_Bachum_Missing_LinearInterpolate[idx_Missing_Q]\n# Plot original series and imputed series\ngp_Bachum_Fill &lt;- ggplot() +\n  geom_line(aes(x = index(xts_Bachum_Missing), y = xts_Bachum_Missing),\n            color = color_RUB_blue) +\n  geom_line(aes(x = index(xts_Filling_Corelation), y = xts_Filling_Corelation,\n                color = \"Regression (Villigst)\")) +\n  geom_line(aes(x = index(xts_Filling_Linear), y = xts_Filling_Linear,\n                color = \"Linear Interpolation\")) +\n  scale_color_manual(\n    \"\",\n    values = c(\n      \"Regression (Villigst)\" = color_RUB_green,\n      \"Linear Interpolation\" = color_TUD_lightblue\n    )\n  ) +\n  labs(x = \"Date\", y = \"Discharge (m³/s)\")\n\n# Convert ggplot to interactive plotly object\nggplotly(gp_Bachum_Fill)",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Exploratory data analysis (EDA)"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html",
    "href": "dataprocess/basic_r_python.html",
    "title": "R & Python Basic",
    "section": "",
    "text": "More Details of R in R for Data Science (2e) and Advanced R\nMore Details of Python in Automate the Boring Stuff with Python and W3 School Python\nThis article serves as a brief introduction to the fundamental coding aspects of both R and Python. It provides a first impression of these scripting languages. For a more comprehensive understanding and in-depth techniques related to both languages, you are encouraged to explore the website mentioned above. The content here is primarily a condensed compilation of information from the provided links, aimed at facilitating a comparison between R and Python.\nData and Functions are the two essential components of every programming language, especially in the context of data science and data processing. They can be likened to nouns and verbs in natural languages. Data describes information, while Functions define actions for manipulating that data.\nThis article is divided into two main sections: Data (Section 1) and Coding (Section 2).\nIn the Data section, we will explore:\nIn the Coding section, we will delve into three key aspects:\nThe above five elements can be considered as the most fundamental elements of every scripting language. Additionally, we will explore object creation and naming in a section called ‘New Objects’ (Section 3). Objects can encompass functions and variables, further enriching our understanding of scripting.\nThis article will provide a solid introduction to the core concepts in programming, laying the groundwork for further exploration in both R and Python.\nOverview:",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#datatypes-structure",
    "href": "dataprocess/basic_r_python.html#datatypes-structure",
    "title": "R & Python Basic",
    "section": "1.1 Datatypes & Structure",
    "text": "1.1 Datatypes & Structure\nIn programming, the concept of datatypes is fundamental. It forms the basis for how we handle and manipulate information in software. The most basic data types, such as integers, numerics, booleans, characters, and bytes, are supported by almost all programming languages. Additionally, there are more complex data types built upon these basics, like strings, which are sequences of characters, and dates, which can be represented as variables of integers and more.\nData structures are equally important, as they determine the organization of data, whether it involves the same data types in multiple dimensions or combinations of different types. Data types and structures are intertwined, serving as the cornerstone for our programming endeavors.\nVariables play a pivotal role in storing data of different types. The choice of data type and structure is critical, as different types and structures enable various operations and functionalities. Therefore, understanding data types and structures is paramount before embarking on data manipulation tasks.\n\n1.1.1 Datatypes\nA data type of a variable specifies the type of data that is stored inside that variable. In this context, we will just discuss Atomic Variables, which represent fundamental data types. There are six basic atomic data types:\n\nLogical (boolean data type)\n\ncan only have two values: TRUE and FALSE\n\nNumeric (double, float, lang)\n\nrepresents all real numbers with or without decimal values.\n\nInteger\n\nspecifies real values without decimal points.\n\nComplex\n\nis used to specify purely imaginary values\n\nCharacter (string)\n\ndata type is used to specify character or string values in a variable\n\nRaw (bytes)\n\nspecifies values as raw bytes\n\n\n\nRPython\n\n\nIn R, variables do not require explicit declaration with a particular data type. Instead, R is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in R:\n\nChecking Data Types: To determine the data type of a variable, you can use the class() function.\nType Conversion: When needed, you can change the data type of a variable using R’s conversion functions, typically prefixed with as..\n\nR’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\nx &lt;- 1000L\nclass(x)\n\n[1] \"integer\"\n\n# Complex\nx &lt;- 9i + 3\nclass(x)\n\n[1] \"complex\"\n\n# Character/String\nx &lt;- \"R is exciting\"\nclass(x)\n\n[1] \"character\"\n\n# Logical/Boolean\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\n# Convert\ny &lt;- as.numeric(x)\nclass(y)\n\n[1] \"numeric\"\n\n# Raw (bytes)\nx &lt;- charToRaw(\"A\")\nx\n\n[1] 41\n\nclass(x)\n\n[1] \"raw\"\n\n\n\n\nIn Python, variables also do not require explicit declaration with a particular data type. Python is dynamically typed, allowing variables to adapt to the data they contain. You can use the following techniques to work with data types in Python:\n\nChecking Data Types: To determine the data type of a variable, you can use the type() function. It allows you to inspect the current data type of a variable.\nType Conversion: When needed, you can change the data type of a variable in Python using various conversion functions, like float().\n\nPython’s flexibility in data type handling simplifies programming tasks and allows for efficient data manipulation without the need for explicit type declarations.\n\n# Numeric\nx = 10.5\nprint(type(x))\n\n&lt;class 'float'&gt;\n\n# Integer\nx = 1000\nprint(type(x))\n\n&lt;class 'int'&gt;\n\n# Complex\nx = 9j + 3\nprint(type(x))\n\n&lt;class 'complex'&gt;\n\n# Character/String\nx = \"Python is exciting\"\nprint(type(x))\n\n&lt;class 'str'&gt;\n\n# Logical/Boolean\nx = True\nprint(type(x))\n\n&lt;class 'bool'&gt;\n\n# Convert to Numeric\ny = float(x)\nprint(type(y))\n\n&lt;class 'float'&gt;\n\n# Raw (bytes)\nx = b'A'\nprint(x)\n\nb'A'\n\nprint(type(x))\n\n&lt;class 'bytes'&gt;\n\n\n\n\n\n\n\n1.1.2 Data Structure\nComparatively, data structures between R and Python tend to exhibit more differences than their data types. However, by incorporating additional libraries like NumPy and pandas, we can access shared data structures which play a vital role in the field of data science.\n\nVector: A set of multiple values (items)\n\nContains items of the same data type or structure\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nArray: A multi-dimensional extension of a vector\n\nMatrix: two dimensions\n\nList: A set of multiple values (items)\n\nContains items of different data types or structures\nIndexed: Allows you to get and change items using indices\nAllows duplicates\nChangeable: You can modify, add, and remove items after creation\n\nTable (Data Frame): Tabular data structure\n\nTwo-dimensional objects with rows and columns\nContains elements of several types\nEach column has the same data type\n\n\n\nRPython\n\n\nThe structure of R variable can be checked with str()ucture:\n\n# Create a vector\nvct_Test &lt;- c(1,5,7)\n# View the structure\nstr(vct_Test)\n\n num [1:3] 1 5 7\n\n# Create a array\nary_Test &lt;- array(1:24, c(2,3,4))\n# View the structure\nstr(ary_Test)\n\n int [1:2, 1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a matrix\nmat_Test &lt;- matrix(1:24, 6, 4)\nmat_Test\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    2    8   14   20\n[3,]    3    9   15   21\n[4,]    4   10   16   22\n[5,]    5   11   17   23\n[6,]    6   12   18   24\n\n# View the structure\nstr(mat_Test)\n\n int [1:6, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Create a list\nlst_Test &lt;- list(c(1,3,5), \"abc\", FALSE)\n# View the structure\nstr(lst_Test)\n\nList of 3\n $ : num [1:3] 1 3 5\n $ : chr \"abc\"\n $ : logi FALSE\n\n# Create a table (data frame)\ndf_Test &lt;- data.frame(name = c(\"Bob\", \"Tom\"), age = c(12, 13))\ndf_Test\n\n  name age\n1  Bob  12\n2  Tom  13\n\n# View the structure\nstr(df_Test)\n\n'data.frame':   2 obs. of  2 variables:\n $ name: chr  \"Bob\" \"Tom\"\n $ age : num  12 13\n\n\n\n\nIn Python, the structure of a variable is treated as the data type, and you can confirm it using the type() function.\nIt’s important to note that some of the most commonly used data structures, such as arrays and data frames (tables), are not part of the core Python language itself. Instead, they are provided by two popular libraries: numpy and pandas.\n\nimport numpy as np\nimport pandas as pd\n\n# Create a vector (list in Python)\nvct_Test = [1, 5, 7]\n# View the structure\nprint(type(vct_Test))\n\n&lt;class 'list'&gt;\n\n# Create a 3D array (NumPy ndarray)\nary_Test = np.arange(1, 25).reshape((2, 3, 4))\n# View the structure\nprint(type(ary_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a matrix (NumPy ndarray)\nmat_Test = np.arange(1, 25).reshape((6, 4))\nprint(type(mat_Test))\n\n&lt;class 'numpy.ndarray'&gt;\n\n# Create a list\nlst_Test = [[1, 3, 5], \"abc\", False]\n# View the structure\nprint(type(lst_Test))\n\n&lt;class 'list'&gt;\n\n# Create a table (pandas DataFrame)\ndf_Test = pd.DataFrame({\"name\": [\"Bob\", \"Tom\"], \"age\": [12, 13]})\nprint(type(df_Test))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nprint(df_Test)\n\n  name  age\n0  Bob   12\n1  Tom   13\n\n\nPython offers several original data structures, including:\n\nTuples: Tuples are ordered collections of elements, similar to lists, but unlike lists, they are immutable, meaning their elements cannot be changed after creation. Tuples are often used to represent fixed collections of items.\nSets: Sets are unordered collections of unique elements. They are valuable for operations that require uniqueness, such as finding unique values in a dataset or performing set-based operations like unions and intersections.\nDictionaries: Dictionaries, also known as dicts, are collections of key-value pairs. They are used to store data in a structured and efficient manner, allowing quick access to values using their associated keys.\n\nWhile these data structures may not be as commonly used in data manipulation and calculations as arrays and data frames, they have unique features and use cases that can be valuable in various programming scenarios.",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#index-subset",
    "href": "dataprocess/basic_r_python.html#index-subset",
    "title": "R & Python Basic",
    "section": "1.2 Index & subset",
    "text": "1.2 Index & subset\nAdditionally, subsetting plays a crucial role in data manipulation. Subsetting allows you to extract specific subsets of data based on conditions, criteria, or filters.\n\nRPython\n\n\nMore Details in Advanced R: 4 Subsetting.\nR’s subsetting operators are fast and powerful. Mastering them allows you to succinctly perform complex operations in a way that few other languages can match. Subsetting in R is easy to learn but hard to master because you need to internalise a number of interrelated concepts:\n\nThere are six ways to subset atomic vectors.\nThere are three subsetting operators, [[, [, and $.\nSubsetting operators interact differently with different vector types (e.g., atomic vectors, lists, factors, matrices, and data frames).\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure).\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn Python, indexing starts from 0, not 1.\n\n\n\n\n\n\n1.2.1 Vector\n\nRPython\n\n\n\nPositive integers return elements at the specified positions:\n\n\nx &lt;- c(2.1, 4.2, 3.3, 5.4)\n\n# One value\nx[1]\n\n[1] 2.1\n\n# More values\nx[c(1:2, 4)]\n\n[1] 2.1 4.2 5.4\n\n# Duplicate indices will duplicate values\nx[c(1, 1)]\n\n[1] 2.1 2.1\n\n# Real numbers are silently truncated to integers\nx[c(2.1, 2.9)]\n\n[1] 4.2 4.2\n\n\n\nNegative integers exclude elements at the specified positions:\n\n\n# Exclude elements\nx[-c(3, 1)]\n\n[1] 4.2 5.4\n\n\n\n\n\n\n\n\nNOTE\n\n\n\nNote that you can’t mix positive and negative integers in a single subset:\n\n\n\nx[c(-1, 2)]\n\nError in x[c(-1, 2)]: nur Nullen dürfen mit negativen Indizes gemischt werden\n\n\n\n\n\nPositive integers return elements at the specified positions:\n\n\nimport numpy as np\nimport pandas as pd\n\n# Create a NumPy array\nx = np.array([2.1, 4.2, 3.3, 5.4])\n\n# One value\nprint(x[0])\n\n2.1\n\n# More values\nprint(x[np.array([0, 1, 3])])\n\n[2.1 4.2 5.4]\n\n# Duplicate indices will duplicate values\nprint(x[np.array([0, 0])])\n\n[2.1 2.1]\n\n\n\negative indexing to access an array from the end:\n\n\n# One value\nprint(x[-1])\n\n5.4\n\n# More values\nprint(x[-np.array([1, 3])])\n\n[5.4 4.2]\n\n\n\n\n\n\n\n1.2.2 Matrices and arrays\n\nRPython\n\n\nThe most common way of subsetting matrices (2D) and arrays (&gt;2D) is a simple generalisation of 1D subsetting: supply a 1D index for each dimension, separated by a comma. Blank subsetting is now useful because it lets you keep all rows or all columns.\n\n# Create a matrix\na2 &lt;- matrix(1:9, nrow = 3)\n# Rename the columns (equivalent to colnames in R)\ncolnames(a2) &lt;- c(\"A\", \"B\", \"C\")\n# Access a specific element using column name\na2[1, \"A\"]\n\nA \n1 \n\n# Select specific rows with all columns\na2[1:2, ]\n\n     A B C\n[1,] 1 4 7\n[2,] 2 5 8\n\n# columns which are excluded \na2[0, -2]\n\n     A C\n\n# Create a 3D array\na3 &lt;- array(1:24, c(2,3,4))\n# Access a specific element(s), in different dimensions\na3[1,2,2]\n\n[1] 9\n\na3[1,2,]\n\n[1]  3  9 15 21\n\na3[1,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    7   13   19\n[2,]    3    9   15   21\n[3,]    5   11   17   23\n\n\n\n\nIn Python, the : symbol is used to indicate all elements of a particular dimension or slice. It allows you to select or reference all items along that dimension in a sequence, array, or data structure.\n\nimport numpy as np\n\n# Create a NumPy matrix\na2 = np.array([[1, 2, 3],\n               [4, 5, 6],\n               [7, 8, 9]])\n\n# Rename the columns (equivalent to colnames in R)\ncolnames = [\"A\", \"B\", \"C\"]\n\n# Access a specific element using column name\nprint(a2[0, colnames.index(\"A\")])\n\n1\n\n# Select the first two rows\nprint(a2[0:2, :])\n\n[[1 2 3]\n [4 5 6]]\n\n# Create a NumPy 3D array\na3 = np.arange(1, 25).reshape((2, 3, 4))\n\n# Access a specific element in the 3D array\nprint(a3[0, 1, 1])\n\n6\n\nprint(a3[0, 1, :])\n\n[5 6 7 8]\n\nprint(a3[0, :, :])\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\n\n\n\n\n\n1.2.3 Data frames\n\nRPython\n\n\nData frames have the characteristics of both lists and matrices:\n\nWhen subsetting with a single index, they behave like lists and index the columns, so df[1:2] selects the first two columns.\nWhen subsetting with two indices, they behave like matrices, so df[1:3, ] selects the first three rows (and all the columns)[^python-dims].\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Select rows\ndf[df$x == 2, ]\n\n  x y z\n2 2 2 b\n\ndf[c(1, 3), ]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# There are two ways to select columns from a data frame\n# Like a list\ndf[c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# Like a matrix\ndf[, c(\"x\", \"z\")]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n# There's an important difference if you select a single \n# column: matrix subsetting simplifies by default, list \n# subsetting does not.\nstr(df[\"x\"])\n\n'data.frame':   3 obs. of  1 variable:\n $ x: int  1 2 3\n\nstr(df[, \"x\"])\n\n int [1:3] 1 2 3\n\n\n\n\nMore detail about Function pandas.Seies.iloc() and pandas.Seies.loc() in pandas document\n\nloc gets rows (and/or columns) with particular labels.\niloc gets rows (and/or columns) at integer locations.\n\n\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Select rows\nprint(df[df['x'] == 2])\n\n   x  y  z\n1  2  2  b\n\nprint(df.iloc[[0, 2]])\n\n   x  y  z\n0  1  3  a\n2  3  1  c\n\n# Select columns\nprint(df[['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select columns like a DataFrame\nprint(df.loc[:, ['x', 'z']])\n\n   x  z\n0  1  a\n1  2  b\n2  3  c\n\n# Select a single column as a Series (simplifies by default)\nprint(df['x'])\n\n0    1\n1    2\n2    3\nName: x, dtype: int64\n\n# Select a single column as a DataFrame (does not simplify)\nprint(df[['x']])\n\n   x\n0  1\n1  2\n2  3\n\n\n\n\n\n\n\n1.2.4 List\n\nRPython\n\n\nThere are two other subsetting operators: [[ and $. [[ is used for extracting single items, while x$y is a useful shorthand for x[[\"y\"]].\n[[ is most important when working with lists because subsetting a list with [ always returns a smaller list. To help make this easier to understand we can use a metaphor:\n[[ can return only a single item, you must use it with either a single positive integer or a single string.\n\nx &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n\n# Get the subset \nx[1]\n\n$a\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\nx[1:2]\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n# Get the element\nx[[1]]\n\n[1] 1 2 3\n\nstr(x[1])\n\nList of 1\n $ a: int [1:3] 1 2 3\n\n# with Label\nx$a\n\n[1] 1 2 3\n\nx[[\"a\"]]\n\n[1] 1 2 3\n\n\n\n\nIn Python there are no effectiv ways to create a items named list. It can always get the element of the list but not a subset of the list.\nIn Python, there are no effective ways to create items with named elements in a list. While you can access individual elements by their positions, there isn’t a straightforward method to create a subset of the list with named elements.\n\n# Create a Python list with nested lists\nx = [list(range(1, 4)), \"a\", list(range(4, 7))]\n\n# Get the subset (Python list slice)\nprint([x[0]])\n\n[[1, 2, 3]]\n\n# Get the element using list indexing\nprint(x[0])\n\n[1, 2, 3]\n\nprint(type(x[0]))\n\n&lt;class 'list'&gt;\n\n\nHowever, dictionaries in Python excel in this regard, as they allow you to assign and access elements using user-defined keys, providing a more efficient way to work with named elements and subsets of data.\n\n# Create a dictionary with labels\nx = {\"a\": list(range(1, 4)), \"b\": \"a\", \"d\": list(range(4, 7))}\n\n\n# Get the element using dictionary indexing\nprint(x[\"a\"])\n\n[1, 2, 3]\n\n# Access an element with a label\nprint(x[\"a\"])\n\n[1, 2, 3]\n\nprint(x.get(\"a\"))\n\n[1, 2, 3]\n\nprint(type(x[\"a\"]))\n\n&lt;class 'list'&gt;",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#data-crud",
    "href": "dataprocess/basic_r_python.html#data-crud",
    "title": "R & Python Basic",
    "section": "1.3 Data CRUD",
    "text": "1.3 Data CRUD\nData manipulation is the art and science of transforming raw data into a more structured and useful format for analysis, interpretation, and decision-making. It’s a fundamental process in data science, analytics, and database management.\nOperations for creating and managing persistent data elements can be summarized as CRUD:\n\nCreate (Add): The creation of new data elements or records.\nRead: The retrieval and access of existing data elements for analysis or presentation.\nUpdate: The modification or editing of data elements to reflect changes or corrections.\nDelete: The removal or elimination of data elements that are no longer needed or relevant.\n\nCombining CRUD operations with subsetting provides a powerful toolkit for working with data, ensuring its accuracy, relevance, and utility in various applications, from database management to data analysis.\n\n1.3.1 Create & Add\nMost of the original data we work with is often loaded from external data sources or files. This process will be discussed in detail in the article titled Data Load.\nIn this section, we will focus on the fundamental aspects of creating and adding data, which may have already been mentioned several times in the preceding text.\n\nRPython\n\n\nCreating new objects in R is commonly done using the assignment operator &lt;-.\nWhen it comes to vectors or list, there are two primary methods to append new elements:\n\nc(): allows you to combine the original vector with a new vector or element, effectively extending the vector.\nappend(): enables you to append a new vector or element at a specific location within the original vector.\n\n\n# Automic value\na &lt;- 1 / 200 * 30\n\n# vector\nx_v &lt;- c(2.1, 4.2, 3.3, 5.4)\n# List\nx_l &lt;- list(a = 1:3, b = \"a\", d = 4:6)\n# add new elements\nc(x_v, c(-1,-5.6))\n\n[1]  2.1  4.2  3.3  5.4 -1.0 -5.6\n\nc(x_l, list(e = c(TRUE, FALSE)))\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\n# append after 2. Element\nappend(x_v, c(-1,-5.6), 2)\n\n[1]  2.1  4.2 -1.0 -5.6  3.3  5.4\n\nappend(x_l, list(e = c(TRUE, FALSE)), 2)\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$e\n[1]  TRUE FALSE\n\n$d\n[1] 4 5 6\n\n\nWhen working with 2D matrices or data frames in R, you can use the following functions to add new elements in the row or column dimensions:\n\ncbind(): to combine data frames or matrices by adding new columns.\nrbind(): to combine data frames or matrices by adding new rows.\n\n\n# Create a matrix\nx_m &lt;- matrix(1:9, nrow = 3)\n# data frame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n# append in colum dimension\ncbind(x_m, -1:-3)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   -1\n[2,]    2    5    8   -2\n[3,]    3    6    9   -3\n\ncbind(df, k = -1:-3)\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\n# append in row dimension\nrbind(x_m, -1:-3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n[4,]   -1   -2   -3\n\nrbind(df, list(-1, -2, \"z\")) # try with rbind(df, c(-1, -2, \"z\"))\n\n   x  y z\n1  1  3 a\n2  2  2 b\n3  3  1 c\n4 -1 -2 z\n\n\nAdditionally, for both lists and data frames in R, you can use the $ &lt;- operator to add new elements:\n\n# Data frame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ncbind(df, k = -1:-3)\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\ndf$k &lt;- -1:-3 # same to df[['k']] &lt;- -1:-3\ndf\n\n  x y z  k\n1 1 3 a -1\n2 2 2 b -2\n3 3 1 c -3\n\n# List\nx_l &lt;- list(a = 1:3, b = \"a\", d = 4:6)\nc(x_l, list(e = c(TRUE, FALSE)))\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\nx_l$e &lt;- c(TRUE, FALSE) # same to x_l[['e']] &lt;- c(TRUE, FALSE)\nx_l\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a\"\n\n$d\n[1] 4 5 6\n\n$e\n[1]  TRUE FALSE\n\n\n\n\nCreating new objects in Python is often accomplished using the assignment operator =. When it comes to adding elements to list, there are three primary functions to consider:\n\nappend(): add a single element to the end of a list.\ninsert(): add an element at a specific position within a list.\nextend() same as +: append elements from an iterable (e.g., another list) to the end of an existing list, allowing for the expansion of the list with multiple elements.\n\n\n# Atomic element\na = 1 / 200 * 30\nb = a + 1\nprint(a)\n\n0.15\n\nprint(b)\n\n1.15\n\n# List\nx = [2.1, 4.2, 3.3, 5.4]\n\n# Append on element\nx.append(-1)\nprint(x)\n\n[2.1, 4.2, 3.3, 5.4, -1]\n\n# Insert on eelement\nx.insert(3, -5.6)\nprint(x)\n\n[2.1, 4.2, 3.3, -5.6, 5.4, -1]\n\n# Extend with new list\nx.extend([6.7, 7.9])\nprint(x)\n\n[2.1, 4.2, 3.3, -5.6, 5.4, -1, 6.7, 7.9]\n\n\nWhen working with numpy.array in Python, you can add elements in two primary ways:\n\nappend(): add element or a new numpy array to the end.\ninsert(): insert element or a new numpy array at specific locations within the original numpy array.\n\n\nimport numpy as np\n\n# Create a NumPy array\nx_a = np.array([2.1, 4.2, 3.3, 5.4])\n\nprint(np.append(x_a, -1))\n\n[ 2.1  4.2  3.3  5.4 -1. ]\n\nprint(np.append(x_a, np.array([6.7, 7.9])))\n\n[2.1 4.2 3.3 5.4 6.7 7.9]\n\nprint(np.insert(x_a, 3, -5.6))\n\n[ 2.1  4.2  3.3 -5.6  5.4]\n\nprint(np.insert(x_a, 3, np.array([6.7, 7.9])))\n\n[2.1 4.2 3.3 6.7 7.9 5.4]\n\n\n\n\n\n\n\n1.3.2 Read\nThe read process is essentially a form of subsetting, where you access specific elements or subsets of data using their indexes. The crucial aspect of this operation is how to obtain and utilize these indexes effectively.\n\nRPython\n\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\n\n# Access using integer index \ndf[1,2]\n\n[1] 3\n\n# Access using names index\ndf[,\"z\"]\n\n[1] \"a\" \"b\" \"c\"\n\ndf$z\n\n[1] \"a\" \"b\" \"c\"\n\n# Access with a value condition\nidx &lt;- which(df$x &gt; 1)\ndf[idx,]\n\n  x y z\n2 2 2 b\n3 3 1 c\n\ndf[idx, \"z\"]\n\n[1] \"b\" \"c\"\n\nidx &lt;- which(df$z == \"a\")\ndf[idx,]\n\n  x y z\n1 1 3 a\n\ndf[idx, 1:2]\n\n  x y\n1 1 3\n\n\n\n\n\nimport pandas as pd\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\n\n# Access using integer index (iloc)\nprint(df.iloc[0, 1])\n\n3\n\n# Access using column label\nprint(df['z'])\n\n0    a\n1    b\n2    c\nName: z, dtype: object\n\nprint(df.z)\n\n0    a\n1    b\n2    c\nName: z, dtype: object\n\n# Access with a value condition\nidx = df['x'] &gt; 1\nprint(df[idx])\n\n   x  y  z\n1  2  2  b\n2  3  1  c\n\nprint(df[df['z'] == 'a'])\n\n   x  y  z\n0  1  3  a\n\nprint(df[df['z'] == 'a'][['x', 'y']])\n\n   x  y\n0  1  3\n\n\n\n\n\n\n\n1.3.3 Update\nThe update operation builds upon the principles of reading. It involves replacing an existing value with a new one, but with certain constraints. The new value must have the same data type, size, and structure as the original value. This ensures data consistency and integrity when modifying data elements. About “data type” it is not so strength, somtimes it is chanable if you replace the whol e.g. colums in data frame.\nIt’s important to note that the concept of ‘data type’ isn’t always rigid. There are cases where data types can change, particularly when replacing entire columns in a data frame, for instance. While data types typically define the expected format and behavior of data, specific operations and transformations may lead to changes in data types to accommodate new values or structures.\n\nRPython\n\n\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf\n\n  x y z\n1 1 3 a\n2 2 2 b\n3 3 1 c\n\n# Update using integer index \ndf[1,2] &lt;- 0\ndf\n\n  x y z\n1 1 0 a\n2 2 2 b\n3 3 1 c\n\n# Update using names index\ndf[2,\"z\"] &lt;- \"lk\"\ndf\n\n  x y  z\n1 1 0  a\n2 2 2 lk\n3 3 1  c\n\n# Update with a value condition\nidx &lt;- which(df$x &gt; 1)\ndf[idx, \"z\"] &lt;- \"bg1\"\ndf\n\n  x y   z\n1 1 0   a\n2 2 2 bg1\n3 3 1 bg1\n\nidx &lt;- which(df$z == \"a\")\ndf[idx,] &lt;- c(-1, -5, \"new_a\")\ndf\n\n   x  y     z\n1 -1 -5 new_a\n2  2  2   bg1\n3  3  1   bg1\n\n\n\n\n\nimport pandas as pd\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\nprint(df)\n\n   x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c\n\n# Update using integer index\ndf.iat[0, 1] = 0\nprint(df)\n\n   x  y  z\n0  1  0  a\n1  2  2  b\n2  3  1  c\n\n# Update using column label and row index\ndf.at[1, 'z'] = \"lk\"\nprint(df)\n\n   x  y   z\n0  1  0   a\n1  2  2  lk\n2  3  1   c\n\n# Update with a value condition\nidx_x_gt_1 = df['x'] &gt; 1\ndf.loc[idx_x_gt_1, 'z'] = \"bg1\"\nprint(df)\n\n   x  y    z\n0  1  0    a\n1  2  2  bg1\n2  3  1  bg1\n\nidx_z_eq_a = df['z'] == 'a'\ndf.loc[idx_z_eq_a] = [-1, -5, \"new_a\"]\nprint(df)\n\n   x  y      z\n0 -1 -5  new_a\n1  2  2    bg1\n2  3  1    bg1\n\n\n\n\n\n\n\n1.3.4 Delete\n\nRPython\n\n\nDeletion in R can be accomplished relatively easily using methods like specifying negative integer indices or setting elements to NULL within a list. However, it’s essential to recognize that there are limitations to deletion operations. For instance, when dealing with multi-dimensional arrays, you cannot delete a single element in the same straightforward manner; instead, you can only delete entire sub-dimensions.\n\n# Create a DataFrame\ndf &lt;- data.frame(x = 1:3, y = 3:1, z = letters[1:3])\ndf\n\n  x y z\n1 1 3 a\n2 2 2 b\n3 3 1 c\n\n# Delete using negative integer index \ndf[,-2]\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\ndf[-2,]\n\n  x y z\n1 1 3 a\n3 3 1 c\n\n# Setting elements to `NULL`\ndf$y &lt;- NULL\ndf\n\n  x z\n1 1 a\n2 2 b\n3 3 c\n\n\n\n\nIn Python is to use the .drop() command to delete the elemnts in datatframe. More details in pandas document\n\ndf = pd.DataFrame({'x': range(1, 4), 'y': range(3, 0, -1), 'z': list('abc')})\nprint(df)\n\n   x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c\n\n# Drop columns\nprint(df.drop(['x', 'z'], axis=1))\n\n   y\n0  3\n1  2\n2  1\n\nprint(df.drop(columns=['x', 'y']))\n\n   z\n0  a\n1  b\n2  c\n\n# Drop a row by index\nprint(df.drop([0, 1]))\n\n   x  y  z\n2  3  1  c",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#math",
    "href": "dataprocess/basic_r_python.html#math",
    "title": "R & Python Basic",
    "section": "2.1 Math",
    "text": "2.1 Math\n\n‘+’ ‘-’ ’*’ ‘/’\nExponent, Logarithm\nTrigonometric functions\nLinear algebra, Matrix multiplication\n\n\nRPython\n\n\n\n1 / 200 * 30\n\n[1] 0.15\n\n(59 + 73 - 2) / 3\n\n[1] 43.33333\n\n3^2\n\n[1] 9\n\nsin(pi / 2) # pi as Const number in R\n\n[1] 1\n\n\n\n\n\nprint(1 / 200 * 30)\n\n0.15\n\nprint((59 + 73 - 2) / 3)\n\n43.333333333333336\n\nprint(3**2)\n\n9\n\nimport math\nprint(math.sin(math.pi/2))\n\n1.0",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#control-flow",
    "href": "dataprocess/basic_r_python.html#control-flow",
    "title": "R & Python Basic",
    "section": "2.2 Control flow",
    "text": "2.2 Control flow\nThere are two primary tools of control flow: choices and loops.\n\nChoices, like if statements calls, allow you to run different code depending on the input.\nLoops, like for and while, allow you to repeatedly run code, typically with changing options.\n\n\n2.2.1 choices\n\n2.2.1.1 Basic If-Else\n\nRPython\n\n\nThe basic form of an if statement in R is as follows:\n\nif (condition) {\n  true_action\n}\nif (condition) {\n  true_action\n} else {\n  false_action\n}\n\nIf condition is TRUE, true_action is evaluated; if condition is FALSE, the optional false_action is evaluated.\nTypically the actions are compound statements contained within {:\nif returns a value so that you can assign the results:\n\na &lt;- 6\nb &lt;- 8\n\nif (b &gt; a) {\n  cat(\"b is greater than a\\n\")\n} else if (a == b) {\n  cat(\"a and b are equal\\n\")\n} else {\n  cat(\"a is greater than b\\n\")\n}\n\nb is greater than a\n\n\n\n\n\n# if statements\nif condition: \n  true_action\n  \n# if-else\nif condition: \n  true_action \nelse: \n  false_action\n\n\n# if-ifel-else\nif condition1: \n  true_action1 \nelif condition2: \n  true_action2 \nelse: \n  false_action\n\n\na = 6\nb = 8\nif b &gt; a:\n  print(\"b is greater than a\")\nelif a == b:\n  print(\"a and b are equal\")\nelse:\n  print(\"a is greater than b\")\n\nb is greater than a\n\n\n\n\n\n\n\n2.2.1.2 switch\n\nRPython\n\n\nClosely related to if is the switch()-statement. It’s a compact, special purpose equivalent that lets you replace code like:\n\nx_option &lt;- function(x) {\n  if (x == \"a\") {\n    \"option 1\"\n  } else if (x == \"b\") {\n    \"option 2\" \n  } else if (x == \"c\") {\n    \"option 3\"\n  } else {\n    stop(\"Invalid `x` value\")\n  }\n}\n\nwith the more succinct:\n\nx_option &lt;- function(x) {\n  switch(x,\n    a = \"option 1\",\n    b = \"option 2\",\n    c = \"option 3\",\n    stop(\"Invalid `x` value\")\n  )\n}\nx_option(\"b\")\n\n[1] \"option 2\"\n\n\nThe last component of a switch() should always throw an error, otherwise unmatched inputs will invisibly return NULL:\n\n\n\nmatch subject:\n    case &lt;pattern_1&gt;:\n        &lt;action_1&gt;\n    case &lt;pattern_2&gt;:\n        &lt;action_2&gt;\n    case &lt;pattern_3&gt;:\n        &lt;action_3&gt;\n    case _:\n        &lt;action_wildcard&gt;\n\n\ndef x_option(x):\n    options = {\n        \"a\": \"option 1\",\n        \"b\": \"option 2\",\n        \"c\": \"option 3\"\n    }\n    return options.get(x, \"Invalid `x` value\")\n\nprint(x_option(\"b\"))\n\noption 2\n\n\n\n\n\n\n\n2.2.1.3 Vectorised if\n\nRPython\n\n\nGiven that if only works with a single TRUE or FALSE, you might wonder what to do if you have a vector of logical values. Handling vectors of values is the job of ifelse(): a vectorised function with test, yes, and no vectors (that will be recycled to the same length):\n\nx &lt;- 1:10\nifelse(x %% 5 == 0, \"XXX\", as.character(x))\n\n [1] \"1\"   \"2\"   \"3\"   \"4\"   \"XXX\" \"6\"   \"7\"   \"8\"   \"9\"   \"XXX\"\n\nifelse(x %% 2 == 0, \"even\", \"odd\")\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nNote that missing values will be propagated into the output.\nI recommend using ifelse() only when the yes and no vectors are the same type as it is otherwise hard to predict the output type. See https://vctrs.r-lib.org/articles/stability.html#ifelse for additional discussion.\n\n\n\n\n\n\n\n\n\n2.2.2 Loops\n\n2.2.2.1 for-Loops\nA for loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string). For each item in vector, perform_action is called once; updating the value of item each time.\n\nRPython\n\n\nIn R, for loops are used to iterate over items in a vector. They have the following basic form:\n\nfor (item in vector) perform_action\n\n\nfor (i in 1:3) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\n\n\n\nfor item in vector \n  perform_action\n\n\nfor i in range(1, 3):\n  print(i)\n\n1\n2\n\n\n\n\n\n\n\n2.2.2.2 while-Loops\nWith the while loop we can execute a set of statements as long as a condition is TRUE:\n\nRPython\n\n\n\ni &lt;- 1\nwhile (i &lt; 6) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\ni = 1\nwhile i &lt; 6:\n  print(i)\n  i += 1\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\n2.2.2.3 terminate\n\nRPython\n\n\nThere are two ways to terminate a for loop early:\n\nnext exits the current iteration.\nbreak exits the entire for loop.\n\n\nfor (i in 1:10) {\n  if (i &lt; 3) \n    next\n\n  print(i)\n  \n  if (i &gt;= 5)\n    break\n}\n\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\n\nfor i in range(1, 10):\n    if i &lt; 3:\n        continue\n    \n    print(i)\n    \n    if i &gt;= 5:\n        break\n\n3\n4\n5",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#function",
    "href": "dataprocess/basic_r_python.html#function",
    "title": "R & Python Basic",
    "section": "2.3 Function",
    "text": "2.3 Function\nMore details of in Advanced R Chapter 6\nA function is a block of code which only runs when it is called. It can be broken down into three components:\n\nThe formals(), the list of arguments that control how you call the function.\nThe body(), the code inside the function.\nThe environment(), the data structure that determines how the function finds the values associated with the names.\n\nWhile the formals and body are specified explicitly when you create a function, the environment is specified implicitly, based on where you defined the function. This location could be within another package or within the workspace (global environment).\n\nRPython\n\n\nThe function environment always exists, but it is only printed when the function isn’t defined in the global environment.\n\nfct_add &lt;- function(x, y) {\n  # A comment\n  x + y\n}\n\n# Get the formal arguments\nformals(fct_add)\n\n$x\n\n\n$y\n\n# Get the function's source code (body)\nbody(fct_add)\n\n{\n    x + y\n}\n\n# Get the function's global environment (module-level namespace)\nenvironment(fct_add)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\n\n\n\ndef fct_add(x, y):\n    # A comment\n    return x + y\n\n# Get the formal arguments\nprint(fct_add.__code__.co_varnames)\n\n('x', 'y')\n\n# Get the function's source code (body)\nprint(fct_add.__code__.co_code)\n\nb'\\x97\\x00|\\x00|\\x01z\\x00\\x00\\x00S\\x00'\n\n# Get the function's global environment (module-level namespace)\nprint(fct_add.__globals__)\n\n{'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__spec__': None, '__annotations__': {}, '__builtins__': &lt;module 'builtins' (built-in)&gt;, 'r': &lt;__main__.R object at 0x0000018FFF41EA10&gt;, 'x': [2.1, 4.2, 3.3, -5.6, 5.4, -1, 6.7, 7.9], 'y': 1.0, 'np': &lt;module 'numpy' from 'C:\\\\Users\\\\lei\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\PYTHON~1\\\\Lib\\\\site-packages\\\\numpy\\\\__init__.py'&gt;, 'pd': &lt;module 'pandas' from 'C:\\\\Users\\\\lei\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\PYTHON~1\\\\Lib\\\\site-packages\\\\pandas\\\\__init__.py'&gt;, 'vct_Test': [1, 5, 7], 'ary_Test': array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]]), 'mat_Test': array([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12],\n       [13, 14, 15, 16],\n       [17, 18, 19, 20],\n       [21, 22, 23, 24]]), 'lst_Test': [[1, 3, 5], 'abc', False], 'df_Test':   name  age\n0  Bob   12\n1  Tom   13, 'a2': array([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]]), 'colnames': ['A', 'B', 'C'], 'a3': array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]]), 'df':    x  y  z\n0  1  3  a\n1  2  2  b\n2  3  1  c, 'a': 6, 'b': 8, 'x_a': array([2.1, 4.2, 3.3, 5.4]), 'idx': 0    False\n1     True\n2     True\nName: x, dtype: bool, 'idx_x_gt_1': 0    False\n1     True\n2     True\nName: x, dtype: bool, 'idx_z_eq_a': 0     True\n1    False\n2    False\nName: z, dtype: bool, 'math': &lt;module 'math' (built-in)&gt;, 'x_option': &lt;function x_option at 0x0000018F924FFCE0&gt;, 'i': 5, 'fct_add': &lt;function fct_add at 0x0000018F924D9DA0&gt;}\n\n\n\n\n\n\n2.3.1 Call\n\nRPython\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2, ...)\n\nTry using seq(), which makes regular sequences of numbers:\n\nseq(from = 1, to = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe often omit the names of the first several arguments in function calls, so we can rewrite this as follows:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can also check the arguments and other information with:\n?seq\nThe “help” windows shows as:\n\n\n\nCalling Syntax:\n\nfunction_name(argument1 = value1, argument2 = value2)\n\n\nsequence = list(range(1, 11))\nprint(sequence)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\n\n\n\n2.3.2 Define\n\nRPython\n\n\nUse the function() keyword:\n\nmy_add1 &lt;- function(x) {\n  x + 1\n}\n\ncalling the function my_add1:\n\nmy_add1(2)\n\n[1] 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn R, the return statement is not essential for a function to yield a value as its result. By default, R will return the result of the last command within the function as its output.\n\n\n\n\nIn Python a function is defined using the def keyword:\n\ndef my_add(x):\n  return x + 1\n\ncalling the function my_add1:\n\nprint(my_add(2))\n\n3\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe return statement is essential for a function to yield a value as its result.",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#naming-rules",
    "href": "dataprocess/basic_r_python.html#naming-rules",
    "title": "R & Python Basic",
    "section": "3.1 Naming rules",
    "text": "3.1 Naming rules\n\nRPython\n\n\n\nmust start with a letter\ncan only contain letters, numbers, underscores _, and dot .\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Reserved Words\n\nTRUE FALSE\nNULL Inf NaN NA NA_real NA_complex_ NA_character_\nif else\nfor while repeat\nnext break\nfunction\nin\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\naFew.People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\n_start_with_underscores\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nmore Reserved Words in:\nhelp(\"reserved\")\n\n\n\nmust start with a letter or the underscore character _\ncan only contain letters, numbers, and underscores _\ncase-sensitive (age, Age and AGE are three different variables)\ncannot be any of the Python keywords (35 keywors in Python 3.8)\n\nTrue False\nNone\nif else elif\nfor while repeat\ntry break continue finally\ndef\nin and or not\nreturn\n\n\n\n\n\n\n\n\n\n\nLegal\n\n\n\ni_use_snake_case\n_start_with_underscores\notherPeopleUseCamelCase\naFew_People_RENOUNCEconvention6\n\n\n\n\n\n\n\n\n\n\n\nIllegal\n\n\n\nwant.contain.dot\n1_start_with_number\nif\ncontain sapce\ncontain-other+charater\n\n\n\n\nMore Keywords in:\n\nhelp(\"keywords\")",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_r_python.html#naming-conventions",
    "href": "dataprocess/basic_r_python.html#naming-conventions",
    "title": "R & Python Basic",
    "section": "3.2 Naming Conventions",
    "text": "3.2 Naming Conventions\n\nCamel Case\n\nEach word, except the first, starts with a capital letter:\nmyVariableName\n\nPascal Case\n\nEach word starts with a capital letter:\nMyVariableName\n\nSnake Case\n\nEach word is separated by an underscore character:\nmy_variable_name",
    "crumbs": [
      "Dataprocess",
      "R & Python Basic"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html",
    "href": "dataprocess/basic_format.html",
    "title": "Basic Data & File Format",
    "section": "",
    "text": "In the realm of hydrological modeling, various data structures and file formats are employed to effectively manage, analyze, and simulate hydrological processes. These structures and formats facilitate the representation of hydrological data, making it accessible for researchers, modelers, and decision-makers. Below are some of the common data structures and file formats used in hydrological modeling, along with their key features.\nOverview:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#array",
    "href": "dataprocess/basic_format.html#array",
    "title": "Basic Data & File Format",
    "section": "Array",
    "text": "Array\nArrays are collections of elements, typically of the SAME data type, organized in a linear or multi-dimensional fashion. They provide efficient data storage and manipulation, making them essential for numerical computations.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#table-dataframe",
    "href": "dataprocess/basic_format.html#table-dataframe",
    "title": "Basic Data & File Format",
    "section": "Table (Dataframe)",
    "text": "Table (Dataframe)\nTabular data structures, often referred to as tables, are a fundamental way of organizing and representing data in a structured format. They consist of rows and columns, where each row typically represents a single observation or record, and each column represents a specific attribute or variable associated with those observations. Tabular structures are highly versatile and are widely used for storing and analyzing various types of data, ranging from simple lists to complex datasets. This characteristic of tables enables them to represent and manage a wide range of information efficiently.\nCompare to array, in a table, columns are allowed to have different data types, but all values within a specific column must share the same data type.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#sec-spatialData",
    "href": "dataprocess/basic_format.html#sec-spatialData",
    "title": "Basic Data & File Format",
    "section": "Spatial Data",
    "text": "Spatial Data\nMore Details in Spatial Data Science\nSpatial data refers to data that has a geographic or spatial component, representing the locations and shapes of physical objects on the Earth’s surface. This type of data is essential in various fields, including geography, environmental science, urban planning, and more. One of the key elements in spatial data is its association with coordinate systems, which allow precise location referencing.\n\nSpatial Vector\nSpatial vector data structures represent geometric shapes like points, lines, and polygons in space. They are widely used in geographic information systems (GIS) for mapping and analyzing spatial data, such as landuse boundaries or river networks.\n\nThe term “Vector” is used because spatial vector data is essentially stored as a vector of points, lines, or polygons (which are composed of lines). The data structure for geographic shapes is divided into two key components:\n\nGeometry: Geometry represents the spatial shape or location of the geographic feature. It defines the boundaries, points, lines, or polygons that make up the feature. These geometric elements are used to precisely describe the geometric feature.\nAttributes: Attributes are associated with the geographic feature and provide additional information about it. These attributes can include data such as the feature’s name, population, temperature, or any other relevant details. Attributes are typically organized and stored in a tabular format, making it easy to perform data analysis and visualization.\n\nThe data structure of points in geospatial data is relatively simple. The geometry of one point is described by its coordinates, typically represented as X (or longitude) and Y (or latitude) values.\n\nOn the other hand, lines and polygons are more complex geometric shapes. The geometry of a line or polygon is defined by a sequence of multiple points. These points are connected in a specific order to form the shape of the line or polygon. In other words, the geometry of every line (or polygon) is composed of a series of coordinates of points.\n\n\n\nSpatial Raster\nSpatial raster data structures are grid-based representations of spatial data, where each cell holds a value. They are commonly used for storing continuous data, like satellite imagery or elevation models.\nThe datastructure of raster data is quite simple. In a raster, each row shares the same X value, and each column shares the same Y value. Additionally, in most situations, the resolution in each dimension remains constant. This means that specifying the starting point and the resolutions is usually sufficient to describe the coordinates of every grid cell. A single raster layer indeed resembles a 2D matrix.\n\n\n\nCoordinate Reference System (CRS)\nIn addition to Geometry (of Vector) and Koordinate (of aster), another essential component of spatial data is the Coordinate Reference System (CRS). The CRS plays a crucial role in geospatial data by providing a framework for translating the Earth’s 3D surface into a 2D coordinate system.\nKey points about the Coordinate Reference System (CRS) include:\n\nAngular coordinates: The earth has an irregular spheroid-like shape. The natural coordinate reference system for geographic data is longitude/latitude.\nProjection: The CRS defines how the Earth’s curved surface is projected onto a 2D plane, enabling the representation of geographic features on maps and in geographic information systems (GIS). Different projection methods exist, each with its own strengths and weaknesses depending on the region and purpose of the map.\nUnits: CRS specifies the units of measurement for coordinates. Common units include degrees (for latitude and longitude), meters, and feet, among others.\nReference Point: It establishes a reference point (usually the origin) and orientation for the coordinate system.\nEPSG Code: Many CRS are identified by an EPSG (European Petroleum Survey Group) code, which is a unique numeric identifier that facilitates data sharing and standardization across GIS systems.\n\nThe CRS is fundamental for correctly interpreting and analyzing spatial data, as it ensures that geographic features are accurately represented in maps and GIS applications. Different CRSs are used for different regions and applications to minimize distortion and provide precise geospatial information.\nThe use of EPSG (European Petroleum Survey Group) codes is highly recommended for defining Coordinate Reference Systems (CRS) in spatial data. These codes consist of a string of numbers that uniquely identify a specific CRS. By using EPSG codes, you can easily access comprehensive definitions of different CRSs, which include details about their coordinate systems, datums, projections, and other parameters. Many software applications and libraries support EPSG codes, making it a standardized and convenient way to specify CRS information in spatial data.\nYou can obtain information about EPSG codes from the EPSG website. This website serves as a valuable resource for accessing detailed information associated with EPSG codes, including coordinate reference system (CRS) definitions and specifications.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#time-series",
    "href": "dataprocess/basic_format.html#time-series",
    "title": "Basic Data & File Format",
    "section": "Time Series",
    "text": "Time Series\nTime series data structures are specifically designed to capture and represent information recorded over a period of time. They play a crucial role in analyzing trends, patterns, and dependencies within sequences of data. Time series data, by definition, have a temporal dimension, making time an essential component of these structures.\nIn comparison to spatial information, time information is relatively straightforward. When the time dimension progresses in uniform steps, it can be efficiently described using the start time and step intervals. However, when the time intervals are irregular or non-uniform, additional time-related details are necessary. This can include specifying the year, month, and day for date-based time data or the hour, minute, and second for time-based information.\nIt’s worth noting that while most time series data adheres to the standard calendar system, some datasets may use alternative calendar systems such as the Julian calendar. Additionally, time zone information is crucial when working with time data, as it ensures accurate temporal references across different geographical regions.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#plain-text-ascii",
    "href": "dataprocess/basic_format.html#plain-text-ascii",
    "title": "Basic Data & File Format",
    "section": "Plain text (ASCII)",
    "text": "Plain text (ASCII)\nASCII (American Standard Code for Information Interchange) is a plain text format, making it human-readable.\n\nAdvantages\n\nHuman-Readable: Users can easily view, understand, and edit the data directly in a text editor.\nWidespread Support, Ease of Import/Export: ASCII is universally supported. Most programming languages, data analysis tools, and software applications can read and write ASCII files, ensuring high compatibility.\nLightweight: ASCII files are typically lightweight and do not consume excessive storage space, making them suitable for large datasets.\nSimple Structure: ASCII files have a straightforward structure, often using lines of text with fields separated by delimiters. This simplicity aids in data extraction and manipulation.\n\n\n\nDisadvantages\n\nLimited Data Types: ASCII primarily handles text-based data and is not suitable for complex data types such as images, multimedia, or hierarchical data.\nNo Inherent Data Validation: ASCII files lack built-in mechanisms for data validation or integrity checks, requiring users to ensure data conformity.\nLack of Compression: ASCII files do not inherently support data compression, potentially resulting in larger file sizes compared to binary formats.\nSlower Reading/Writing: Reading and writing data in ASCII format may be slower, especially for large datasets, due to additional parsing required to interpret text-based data.\n\n\n\nFile format for ASCII data\nWhen it comes to plain text formats, there is no universal standard, and it’s highly adaptable to specific needs. The initial step in loading a plain text table is to analyze the structure of the file.\nTypically, a text table can store 2D data, comprising columns and rows or a matrix. However, above the data body, there’s often metadata that describes the data. Metadata can vary widely between data body.\nDividing rows is usually straightforward and can be achieved by identifying row-end characters. However, dividing columns within each row presents multiple possibilities, such as spaces, tabs, commas, or semicolons.\n\n.txt: This is the most generic and widely used file extension for plain text files. It doesn’t imply any specific format or structure; it’s just a simple text file.\n.csv (Comma-Separated Values): While CSV files contain data separated by commas, they are still considered ASCII files because they use plain text characters to represent data values. Each line in a CSV file typically represents a record, with values separated by commas.\n\nIn .txt files, any of these separators can be used, but in .csv files, commas or semicolons are commonly employed as separator characters.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#excel-files",
    "href": "dataprocess/basic_format.html#excel-files",
    "title": "Basic Data & File Format",
    "section": "Excel Files",
    "text": "Excel Files\nExcel files, often denoted with the extensions .xls or .xlsx, are a common file format used for storing structured data in tabular form. These files are not to be confused with the Microsoft Excel software itself but are the data containers created and manipulated using spreadsheet software like Excel.\nExcel files are widely used in various applications, including data storage, analysis, reporting, and sharing. They consist of rows and columns, where each cell can contain text, numbers, formulas, or dates. These files are versatile and can hold different types of data, making them a popular choice for managing information.\n\nAdvantages:\n\nUser-Friendly Interface: Excel’s user-friendly interface makes it accessible to users with varying levels of expertise. Its familiar grid layout simplifies data input and manipulation.\nVersatility: Excel can handle various types of data, from simple lists to complex calculations.\nFormulas and Functions: Excel provides an extensive library of built-in formulas and functions, allowing users to automate calculations and streamline data processing.\nData Visualization: Creating charts and graphs in Excel is straightforward. It helps in visualizing data trends and patterns, making complex information more accessible.\nData Validation: Excel allows you to set rules and validation criteria for data entry, reducing errors and ensuring data accuracy.\n\n\n\nDisadvantages:\n\nLimited Data Handling: Excel has limitations in handling very large datasets. Performance may degrade, and it’s not suitable for big data analytics.\nLack of Version Control: Excel lacks robust version control features, making it challenging to track changes and manage document versions in collaborative environments.\n\nIn conclusion, Excel is a valuable tool for various data-related tasks but comes with limitations in terms of scalability, data integrity, and security. Careful consideration of its strengths and weaknesses is essential when deciding whether it’s the right choice for your data management needs.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#binary",
    "href": "dataprocess/basic_format.html#binary",
    "title": "Basic Data & File Format",
    "section": "Binary",
    "text": "Binary\nUnlike text-based files, binary files store data in a way that is optimized for computer processing and can represent a wide range of data types, from simple numbers to complex structures. These files are used in various applications, including programming, scientific research, and data storage, due to their efficiency in handling data.\n\nAdvantages of Binary Formats\n\nEfficiency: Binary formats are highly efficient for data storage and transmission because they represent data in a compact binary form. This can significantly reduce storage space and data transfer times, making them ideal for large datasets.\nData Integrity: Binary formats often include built-in mechanisms for data integrity and error checking. This helps ensure that data remains intact and accurate during storage and transmission.\nComplex Data: Binary formats can represent complex data structures, which makes them suitable for a wide range of data types.\nFaster I/O: Reading and writing data in binary format is generally faster than text-based formats like ASCII. This efficiency is particularly important for applications that require high-speed data processing.\nSecurity: Binary formats can provide a level of data security because they are not easily human-readable. This can be advantageous when dealing with sensitive information.\n\n\n\nDisadvantages of Binary Formats\n\nLack of Human-Readability: Binary formats are not human-readable, making it difficult to view or edit the data directly. This can be a disadvantage when data inspection or manual editing is required.\nCompatibility: Binary formats may not be universally compatible across different software platforms and programming languages. This can lead to issues when sharing or accessing data in various environments.\nLimited Metadata: Binary formats may not include comprehensive metadata structures, making it challenging to document and describe the data effectively.\nVersion Compatibility: Changes in the binary format’s structure or encoding can lead to compatibility issues when working with data created using different versions of software or hardware.\nPlatform Dependence: Binary formats can be platform-dependent, meaning they may not be easily transferable between different operating systems or hardware architectures.\n\nBinary formats are a valuable choice for certain applications, particularly when efficiency, data integrity, and complex data types are crucial. However, they may not be suitable for all scenarios, especially when human readability, compatibility, or ease of data inspection is essential.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#nectcdf",
    "href": "dataprocess/basic_format.html#nectcdf",
    "title": "Basic Data & File Format",
    "section": "NectCDF",
    "text": "NectCDF\n\nNetCDF (Network Common Data Form) is a versatile data format widely used in scientific and environmental applications. It is primarily a binary data format, but it includes structured elements for efficient data storage and management. Here are some key characteristics of NetCDF:\n\nBinary Representation: NetCDF data files are primarily stored in binary format, which enables efficient storage and handling of numerical data, particularly floating-point numbers.\nSelf-Describing: NetCDF files are self-describing, meaning they include metadata alongside the data. This metadata provides essential information about the data’s structure, dimensions, units, and other attributes.\nHierarchical Structure: NetCDF supports a hierarchical structure capable of representing complex data types, including multi-dimensional arrays and groups of data variables.\nData Compression: NetCDF allows for data compression, which can reduce the storage space required for large datasets while maintaining data integrity.\nLanguage Support: NetCDF libraries and tools are available for multiple programming languages, making it accessible to a wide range of scientific and data analysis applications.\n\nNetCDF’s combination of binary efficiency and structured metadata makes it an invaluable choice for storing and sharing scientific data, particularly in fields such as meteorology, oceanography, and environmental science.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#database-systems",
    "href": "dataprocess/basic_format.html#database-systems",
    "title": "Basic Data & File Format",
    "section": "Database Systems",
    "text": "Database Systems\nDatabase Systems, such as SQL and NoSQL databases, are crucial for efficiently managing and querying large, structured datasets. They provide structured data storage, ensuring data integrity and consistency. SQL databases like MySQL and PostgreSQL are well-suited for relational data, while NoSQL databases like MongoDB excel in handling semi-structured or unstructured data. These systems are commonly used for storing long-term observational data, model outputs, and sensor data in scientific research and various enterprise applications.\n\nAdvantages\n\nEfficient Data Retrieval: Databases are optimized for querying and retrieving data, making it quick and efficient to access information.\nData Integrity: Databases enforce data integrity rules, ensuring that data remains consistent and reliable over time.\nStructured Storage: They provide a structured way to store data, making it easier to organize and manage large datasets.\nConcurrent Access: Multiple users or applications can access the database simultaneously, enabling collaboration and scalability.\nSecurity: Database systems offer security features like user authentication and authorization to protect sensitive data.\nBackup and Recovery: They often include mechanisms for automated data backup and recovery, reducing the risk of data loss.\n\n\n\nDisadvantages\n\nComplexity: Setting up and maintaining a database can be complex and requires specialized knowledge.\nCost: Licensing, hardware, and maintenance costs can be significant, especially for enterprise-grade database systems.\nScalability Challenges: Some database systems may face scalability limitations as data volume grows.\nLearning Curve: Users and administrators need to learn query languages (e.g., SQL) and database management tools.\nOverhead: Databases can introduce overhead due to indexing, data normalization, and transaction management.\nVendor Lock-In: Depending on the chosen database system, there may be vendor lock-in, making it challenging to switch to another system.\nResource Intensive: Databases consume computing resources, such as CPU and RAM, which can affect system performance.\n\nThe choice of using a database system depends on specific requirements, such as data volume, complexity, security, and scalability needs. It’s essential to carefully evaluate the advantages and disadvantages in the context of your project.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/basic_format.html#spatial-data-file-formats",
    "href": "dataprocess/basic_format.html#spatial-data-file-formats",
    "title": "Basic Data & File Format",
    "section": "Spatial Data File Formats",
    "text": "Spatial Data File Formats\nSpatial data files are a specialized type of data format designed for storing geographic or location-based information. Unlike standard data files that store text, numbers, or other types of data, spatial data files are tailored for representing the geographical features of our world.\nSpatial data comes in various file formats, each tailored for specific types of geographic data and applications. Here are some commonly used formats and their key differences:\n\nRaster Data Formats\nMore Raster file-formats in GDAL\n\nTIFF (Tagged Image File Format):\n\nA widely used raster format for storing high-quality images and raster datasets.\nSupports georeferencing and metadata, making it suitable for spatial applications.\n\nASC (Arc/Info ASCII Grid):\n\nA plain text format used to represent raster data in a grid format.\nContains elevation or other continuous data with rows and columns of values.\n\nJPEG (Joint Photographic Experts Group), PNG (Portable Network Graphics):\n\nCommonly used for photographs and images, but not ideal for spatial analysis due to lossy compression.\n\n\n\n\nVector Data Formats\nMore Vector file-formats in GDAL\n\nShapefile (SHP):\n\nOne of the most common vector formats used in GIS applications.\nConsists of multiple files (.shp, .shx, .dbf, etc.) to store point, line, or polygon geometries and associated attributes.\n\n\n\n\n\nFile extension\nContent\n\n\n\n\n.dbf\nAttribute information\n\n\n.shp\nFeature geometry\n\n\n.shx\nFeature geometry index\n\n\n.aih\nAttribute index\n\n\n.ain\nAttribute index\n\n\n.prj\nCoordinate system information\n\n\n.sbn\nSpatial index file\n\n\n.sbx\nSpatial index file\n\n\n\n\nGeoPackage (GPKG):\n\nAn open, standards-based platform-independent format for spatial data.\nCan store multiple layers, attributes, and geometries in a single file.\n\nKML (Keyhole Markup Language):\n\nXML-based format used for geographic visualization in Earth browsers like Google Earth.\nSuitable for storing points, lines, polygons, and related attributes.\n\nGeoJSON:\n\nA lightweight format for encoding geographic data structures using JSON (JavaScript Object Notation).\nIdeal for web applications due to its simplicity and ease of use.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Basic Data & File Format"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html",
    "href": "dataprocess/data_load.html",
    "title": "Data Loading",
    "section": "",
    "text": "This Aritcl will show the process to load data from other files. I t will divide into four paties: plain text (read able ASCII), Excel, NetCDF and spatial data.\nOverview:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#example-file",
    "href": "dataprocess/data_load.html#example-file",
    "title": "Data Loading",
    "section": "1.1 Example File",
    "text": "1.1 Example File\nLet’s start with an example CSV file named Bachum_2763190000100.csv. This file contains pegel discharge data and is sourced from open data available at ELWAS-WEB NRW. You can also access it directly from the internet via Github, just like you would access a local file.\nTake a look:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#library-and-functions",
    "href": "dataprocess/data_load.html#library-and-functions",
    "title": "Data Loading",
    "section": "1.2 Library and functions",
    "text": "1.2 Library and functions\n\nRPython\n\n\nFirst, we need to load the necessary library tidyverse. This library collection includes readr for reading files and dplyr for data manipulation, among others.\nAnd, we set the URL address as the file path (including the file name).\n\n# load the library\nlibrary(tidyverse)\nfn_Bachum &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n\nThe documentation for the readr library is available online and can be accessed at https://readr.tidyverse.org.\nOf particular interest are the following functions:\n\nreadr::read_csv()\nreadr::read_table()\n\nWe can observe that the CSV file is divided by semicolons. Therefore, it’s more appropriate to use read_csv2() rather than read_csv().\nThe difference between read_*() functions in the readr package is determined by the delimiter character used in the files:\n\n\n\nCHEAT SHEET from Rstudio\n\n\n\n\n\n# load the library\nimport pandas as pd\nfn_Bachum = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/Bachum_2763190000100.csv\"\nfn_Datatype = \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/load_Datatype.txt\"\n\nThe documentation for the pandas library is available online and can be accessed at https://pandas.pydata.org/docs/index.html.\nOf particular interest are the following functions:\n\npandas.read_csv()\npandas.read_table()",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#metadata-handel",
    "href": "dataprocess/data_load.html#metadata-handel",
    "title": "Data Loading",
    "section": "1.3 Metadata Handel",
    "text": "1.3 Metadata Handel\nMetadata can vary widely between datasets, so it’s handled separately from the data body.\nThere are three ways to deal with metadata:\n\nDirectly Ignore: This approach involves ignoring metadata when it’s redundant or readily available from other data sources, such as file names or external references.\nExtract from Text: When metadata is crucial but not in table form, you can extract information from text strings. For more information, refer to the section on string manipulation Section 3.\nRead as a Second Table: If metadata is well-organized in a tabular format, it can be read as a separate table to facilitate its use.\n\nIn the Bachum_2763190000100.csv file, you will find that there are 10 lines of metadata, which are well-organized in a tabular format. However, it’s important to note that the consistency in values column varies.\n\n1.3.1 Directly Ignore use grguments skip\n\n# skip = 10\nread_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\n\n# A tibble: 10 × 2\n   X1            X2\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n\n\n\n\n1.3.2 Read metadata as table\nWhen directly reading all metadata into one table, you may encounter mixed data types. In the metadata, there are three data types:\n\nNumeric: Examples include Pegelnullpunkt and Einzugsgebiet.\nString: This category covers fields like Name, Pegelnummer, and others.\nDate: Date values are present in columns like Datum von and Datum bis.\n\nIn a data frame (tibble), columns must have the same data type. Consequently, R will automatically convert them to a single data type, which is typically string.\nTo address this situation, you should specify the data type you want to read. For example, to read the date values in lines 4 and 5, you can use the following settings: 1. skip = 3 to skip the first three lines of metadata. 2. n_max = 2 to read the next two lines (lines 4 and 5) as date values.\n\nRPython\n\n\n\n# skip = 3\nread_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\n\n# A tibble: 2 × 2\n  X1        X2        \n  &lt;chr&gt;     &lt;chr&gt;     \n1 Datum von 01.01.1990\n2 Datum bis 31.12.2022\n\n\n\n\n\ndf_bach = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\nprint(df_bach)\n\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n\n\n\n\n\nUnfortunately, R may not always recognize date values correctly, so you may need to perform additional steps for conversion:\n\nAfter Reading: This involves transforming the data from its initial format to the desired date format within your R environment.\nSet the Data Type by Reading: Another approach is to set the data type while reading the data.\n\nMore details in the next section:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#load-tabular-data",
    "href": "dataprocess/data_load.html#load-tabular-data",
    "title": "Data Loading",
    "section": "1.4 Load tabular data",
    "text": "1.4 Load tabular data\n\nRPython\n\n\nTo read the first 10 lines of metadata, you can use the n_max setting with a value of n_max = 10 in the read_csv2() function.\n\nread_csv2(fn_Bachum, n_max = 10, col_names = FALSE)\n\n# A tibble: 10 × 2\n   X1                          X2             \n   &lt;chr&gt;                       &lt;chr&gt;          \n 1 \"Name\"                      \"Bachum\"       \n 2 \"Pegelnummer\"               \"2763190000100\"\n 3 \"Gew\\xe4sser\"               \"Ruhr\"         \n 4 \"Datum von\"                 \"01.01.1990\"   \n 5 \"Datum bis\"                 \"31.12.2022\"   \n 6 \"Parameter\"                 \"Abfluss\"      \n 7 \"Q Einheit\"                 \"m\\xb3/s\"      \n 8 \"Tagesmittelwerte\"           &lt;NA&gt;          \n 9 \"Pegelnullpunkt [m\\xfcNHN]\" \"146,83\"       \n10 \"Einzugsgebiet [km\\xb2]\"    \"1.532,02\"     \n\n\nAfter dealing with the metadata, we can proceed to load the data body using the readr::read_*() function cluster. Plain text files typically store data in a tabular or matrix format, both of which have at most two dimensions. When using the readr::read_() function, it automatically returns a tibble. If your data in the text file is in matrix format, you can use conversion functions like as.matrix() to transform it into other data structures.\n\n# 1. load\ntb_Read &lt;- read_csv2(fn_Bachum, skip = 10, n_max = 10, col_names = FALSE)\ntb_Read\n\n# A tibble: 10 × 2\n   X1            X2\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 01.01.1990  20.6\n 2 02.01.1990  19.0\n 3 03.01.1990  17.9\n 4 04.01.1990  16.8\n 5 05.01.1990  16.0\n 6 06.01.1990  14.8\n 7 07.01.1990  14.3\n 8 08.01.1990  14.0\n 9 09.01.1990  14.4\n10 10.01.1990  14.5\n\n# 2. convert\ndf_Read &lt;- as.data.frame(tb_Read)\nmat_Read &lt;- as.matrix(tb_Read)\n\ndf_Read\n\n           X1     X2\n1  01.01.1990 20.640\n2  02.01.1990 18.994\n3  03.01.1990 17.949\n4  04.01.1990 16.779\n5  05.01.1990 16.019\n6  06.01.1990 14.817\n7  07.01.1990 14.296\n8  08.01.1990 13.952\n9  09.01.1990 14.403\n10 10.01.1990 14.500\n\nmat_Read\n\n      X1           X2      \n [1,] \"01.01.1990\" \"20.640\"\n [2,] \"02.01.1990\" \"18.994\"\n [3,] \"03.01.1990\" \"17.949\"\n [4,] \"04.01.1990\" \"16.779\"\n [5,] \"05.01.1990\" \"16.019\"\n [6,] \"06.01.1990\" \"14.817\"\n [7,] \"07.01.1990\" \"14.296\"\n [8,] \"08.01.1990\" \"13.952\"\n [9,] \"09.01.1990\" \"14.403\"\n[10,] \"10.01.1990\" \"14.500\"\n\n\n\n\n\ntb_Read = pd.read_csv(fn_Bachum, skiprows=10, nrows=10, header=None, delimiter=';', decimal=',', encoding='latin-1')\nprint(tb_Read)\n\n            0       1\n0  01.01.1990  20.640\n1  02.01.1990  18.994\n2  03.01.1990  17.949\n3  04.01.1990  16.779\n4  05.01.1990  16.019\n5  06.01.1990  14.817\n6  07.01.1990  14.296\n7  08.01.1990  13.952\n8  09.01.1990  14.403\n9  10.01.1990  14.500",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#sec-datatype",
    "href": "dataprocess/data_load.html#sec-datatype",
    "title": "Data Loading",
    "section": "1.5 Data type",
    "text": "1.5 Data type\nIn this section, we will work with a custom-made text file that contains various data types and formats. The file consists of three rows, with one of them serving as the header containing column names, and six columns in total.\nLet’s take a look:\n\nActually the function will always guse the dattype for each column, when the data really normally format the function will return the right datatype for the data:\n\nRPython\n\n\n\nread_table(fn_Datatype)\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de    str  \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;      &lt;chr&gt;\n1     1      0.1 0,1      2023-09-15 15.09.2023 en   \n2     9      9.6 9,6      2023-09-16 16.09.2023 de   \n\n\n\n\n\ndf = pd.read_table(fn_Datatype)\nprint(df)\n\n   int  float_en float_de     date_en     date_de str\n0    1       0.1      0,1  2023-09-15  15.09.2023  en\n1    9       9.6      9,6  2023-09-16  16.09.2023  de\n\nprint(df.dtypes)\n\nint           int64\nfloat_en    float64\nfloat_de     object\ndate_en      object\ndate_de      object\nstr          object\ndtype: object\n\n\n\n\n\nBy default, functions like readr::read_table() in R and pandas.read_table() in Python will attempt to guess data types automatically when reading data. Here’s how this guessing typically works:\n\nIf a column contains only numbers and decimal dots (periods), it will be recognized as numeric (double in R and int or float in Python).\nIf a date is formatted in “Y-M-D” (e.g., “2023-08-27”) or “h:m:s” (e.g., “15:30:00”) formats, it may be recognized as a date or time type. Nur in R\nIf the data type cannot be confidently determined, it is often treated as a string (str in R and object in Python).\n\nThis automatic guessing is convenient, but it’s essential to verify the inferred data types, especially when working with diverse datasets.\n\n1.5.1 Set the Data Type by Reading\nExplicitly setting data types using the col_types (in R) or dtype (in Python) argument can help ensure correct data handling.\n\nRPython\n\n\nTo address the issue of date recognition, you can set the col_types argument, you can use a compact string representation where each character represents one column:\n\nc: Character\ni: Integer\nn: Number\nd: Double\nl: Logical\nf: Factor\nD: Date\nT: Date Time\nt: Time\n?: Guess\n_ or -: Skip\n\nto \"cD\" when reading the data. This informs the function that the first column contains characters (c) and the second column contains Dates (D).\n\nread_table(fn_Datatype, col_types = \"iddDDc\")\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;  &lt;chr&gt;\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n\n\n\nread_table(fn_Datatype, col_types = \"idd?Dc\")\n\n# A tibble: 2 × 6\n    int float_en float_de date_en    date_de str  \n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;  &lt;chr&gt;\n1     1      0.1       NA 2023-09-15 NA      en   \n2     9      9.6       NA 2023-09-16 NA      de   \n\n\n\n\nTo set data types when reading data using functions pandas.read_*, you have three main choices by using the dtype parameter:\n\nstr: Specify the data type as a string.\nint: Specify the data type as an integer.\nfloat: Specify the data type as a floating-point number.\n\nHowever, you can also use the dtype parameter with a callable function to perform more advanced type conversions. Some commonly used functions include:\n\npd.to_datetime: Converts a column to datetime format.\npd.to_numeric: Converts a column to numeric (integer or float) format.\npd.to_timedelta: Converts a column to timedelta format.\n\n\n# Define column names and types as a dictionary\ncol_types = {\"X1\": str, \"X2\": pd.to_datetime}\n# Read the CSV file, skip 3 rows, read 2 rows, and specify column names and types\ndf = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', names=[\"X1\", \"X2\"], dtype=col_types, encoding='latin-1')\n\n# Display the loaded data\nprint(df)\n\n\nDON’T RUN Error, because data doesn’t match the default format of ‘Y-m-d’.\n\n\n\n\n\nUnfortunately, the default date format in R and Python may not work for German-style dates like “d.m.Y” as R and Python primarily recognizes the “Y-m-d” format.\n\n\n\n1.5.2 After Reading\nTo address this issue, you can perform date conversions after reading the data:\n\nRPython\n\n\nUsing function as.Date() and specify the date format using the format argument, such as format = \"%d.%m.%Y\".\n\ndf_Date &lt;- read_csv2(fn_Bachum, skip = 3, n_max = 2, col_names = FALSE)\ndf_Date$X2 &lt;- df_Date$X2 |&gt; as.Date(format = \"%d.%m.%Y\")\ndf_Date\n\n# A tibble: 2 × 2\n  X1        X2        \n  &lt;chr&gt;     &lt;date&gt;    \n1 Datum von 1990-01-01\n2 Datum bis 2022-12-31\n\n\n\n\n\ndf_Date = pd.read_csv(fn_Bachum, skiprows=3, nrows=2, header=None, delimiter=';', encoding='latin-1')\n\n# Display the loaded data\nprint(df_Date)\n\n           0           1\n0  Datum von  01.01.1990\n1  Datum bis  31.12.2022\n\n# 2. Convert the second column (X2) to a date format\ndf_Date[1] = pd.to_datetime(df_Date[1], format='%d.%m.%Y')\n\n# Display the DataFrame with the second column converted to date format\nprint(df_Date)\n\n           0          1\n0  Datum von 1990-01-01\n1  Datum bis 2022-12-31\n\nprint(df_Date.dtypes)\n\n0            object\n1    datetime64[ns]\ndtype: object",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#example-file-1",
    "href": "dataprocess/data_load.html#example-file-1",
    "title": "Data Loading",
    "section": "2.1 Example File",
    "text": "2.1 Example File\nLet’s begin with an example Excel file named Pegeln_NRW.xlsx. This file contains information about measurement stations in NRW (Nordrhein-Westfalen, Germany) and is sourced from open data available at ELWAS-WEB NRW. You can also access it directly from Github.\nTake a look:",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#library-and-functions-1",
    "href": "dataprocess/data_load.html#library-and-functions-1",
    "title": "Data Loading",
    "section": "2.2 Library and functions",
    "text": "2.2 Library and functions\n\nRPython\n\n\nTo load the necessary library, readxl, and access its help documentation, you can visit this link. The readxl::read_excel() function is versatile, as it can read both .xls and .xlsx files and automatically detects the format based on the file extension. Additionally, you have the options of using read_xls() for .xls files and read_xlsx() for .xlsx files. More details in the Page.\n\n# load the library\nlibrary(readxl)\n# The Excel file cannot be read directly from GitHub. You will need to download it to your local machine first\nfn_Pegeln &lt;- \"C:\\\\Lei\\\\HS_Web\\\\data_share/Pegeln_NRW.xlsx\"\n\n\n\nThe pandas.read_excel() function is versatile, as it can read both .xls and .xlsx files and automatically detects the format based on the file extension. More details in the Page.\n\nimport pandas as pd\n\n# Specify the path to the Excel file\nfn_Pegeln = \"C:\\\\Lei\\\\HS_Web\\\\data_share/Pegeln_NRW.xlsx\"",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#load-tabular-data-1",
    "href": "dataprocess/data_load.html#load-tabular-data-1",
    "title": "Data Loading",
    "section": "2.3 Load tabular data",
    "text": "2.3 Load tabular data\nSimilar to plain text files, metadata is often provided before the data body in Excel files. In Excel, each cell can be assigned a specific data type, while in R tables (data.frame or tibble), every column must have the same data type. This necessitates separate handling of metadata and data body to ensure that the correct data types are maintained.\nUnlike plain text files where we can only select lines to load, Excel allows us to define coordinates to access a specific celles-box wherever they are located.\n\n2.3.1 First try without any setting\n\nRPython\n\n\n\n# try without setting\ntb_Pegeln &lt;- read_excel(fn_Pegeln)\ntb_Pegeln\n\n# A tibble: 277 × 16\n   Suchergebnisse Pegel.…¹ ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt;                   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 \"Suchkriterien:\\n -- \\… &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 2  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 3  &lt;NA&gt;                   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n 4 \"Name\"                  Pege… Gewä… Betr… Pege… Einz… Q von Q bis NQ    MNQ  \n 5 \"Ahlen\"                 3211… Werse LANU… 73,47 46,62 1975  2013  0     0,07 \n 6 \"Ahmsen\"                4639… Werre LANU… 64,28 593   1963  2022  1,21  2,22 \n 7 \"Ahrhütte-Neuhof\"       2718… Ahr   LANU… 340,… 124   1986  2011  0,22  0,36 \n 8 \"Albersloh\"             3259… Werse LANU… 48,68 321,… 1973  2020  0,12  0,24 \n 9 \"Altena\"                2766… Lenne LANU… 154,… 1.190 1950  2021  1,36  6,48 \n10 \"Altena_Rahmedestraße\"  2766… Rahm… LANU… 157,… 29,6  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n# ℹ 267 more rows\n# ℹ abbreviated name: ¹​`Suchergebnisse Pegel.xlsx 14.09.2023 10:01`\n# ℹ 6 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;, ...13 &lt;chr&gt;, ...14 &lt;chr&gt;,\n#   ...15 &lt;chr&gt;, ...16 &lt;chr&gt;\n\n\n\n\n\n# Read the Excel file into a pandas DataFrame\ntb_Pegeln = pd.read_excel(fn_Pegeln)\n\n# Display the DataFrame\nprint(tb_Pegeln)\n\n            Suchergebnisse Pegel.xlsx 14.09.2023 10:01  ...      Unnamed: 15\n0    Suchkriterien:\\n -- \\n\\n273 von 273 Datensätze...  ...              NaN\n1                                                  NaN  ...              NaN\n2                                                  NaN  ...              NaN\n3                                                 Name  ...  Nordwert in UTM\n4                                                Ahlen  ...       5733198,52\n..                                                 ...  ...              ...\n272                                            Wolbeck  ...      5750912,504\n273                                    Wt-Kluserbrücke  ...          5679856\n274                                         Zeppenfeld  ...      5626270,319\n275                                          Zerkall 1  ...      5618750,896\n276                                          Zerkall 2  ...       5617784,98\n\n[277 rows x 16 columns]\n\n\n\n\n\nWhen we provide only the file name to the function, we will always retrieve all the content from the first sheet. However, due to the limitations in R (and Python) tables, every column will be recognized as the same data type, typically character.\n\n\n2.3.2 Give a range\n\nRPython\n\n\n\n# using the range argument\ntb_Pegeln_Range &lt;- read_excel(fn_Pegeln, range = \"Suchergebnisse Pegel!A5:P10\")\ntb_Pegeln_Range\n\n# A tibble: 5 × 16\n  Name            Pegelnummer   Gewässername Betreiber  `Pegelnullpunkt [müNHN]`\n  &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                   \n1 Ahlen           3211000000300 Werse        LANUV, NRW 73,47                   \n2 Ahmsen          4639000000100 Werre        LANUV, NRW 64,28                   \n3 Ahrhütte-Neuhof 2718193000100 Ahr          LANUV, NRW 340,58                  \n4 Albersloh       3259000000100 Werse        LANUV, NRW 48,68                   \n5 Altena          2766930000100 Lenne        LANUV, NRW 154,22                  \n# ℹ 11 more variables: `Einzugsgebiet [km²]` &lt;chr&gt;, `Q von` &lt;chr&gt;,\n#   `Q bis` &lt;chr&gt;, NQ &lt;chr&gt;, MNQ &lt;chr&gt;, MQ &lt;chr&gt;, MHQ &lt;chr&gt;, HQ &lt;chr&gt;,\n#   `Q Einheit` &lt;chr&gt;, `Ostwert in UTM` &lt;chr&gt;, `Nordwert in UTM` &lt;chr&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe data type of “Pegelnullpunkt [müNHN]” appears to be incorrect due to improper settings in Excel.\n\n\n\n\nCompared to R, Python doesn’t have a direct equivalent to the “Range”. Instead, you can achieve a similar method like plain text with skiprows. Additionally, you can use usecols to specify the columns you want to include.\n\n# Read the specified range from the Excel file into a pandas DataFrame\ntb_Pegeln_Range = pd.read_excel(fn_Pegeln, sheet_name=\"Suchergebnisse Pegel\", skiprows = 4, usecols=\"A:P\")\n\n# Display the DataFrame\nprint(tb_Pegeln_Range)\n\n                Name    Pegelnummer  ... Ostwert in UTM Nordwert in UTM\n0              Ahlen  3211000000300  ...      425366,05      5733198,52\n1             Ahmsen  4639000000100  ...     479549,678     5771201,838\n2    Ahrhütte-Neuhof  2718193000100  ...     339937,139     5583651,051\n3          Albersloh  3259000000100  ...     412463,351     5748891,345\n4             Altena  2766930000100  ...     407683,712     5682846,836\n..               ...            ...  ...            ...             ...\n268          Wolbeck  3289100000100  ...     416214,865     5750912,504\n269  Wt-Kluserbrücke  2736510000100  ...         371494         5679856\n270       Zeppenfeld  2722590000100  ...     430354,152     5626270,319\n271        Zerkall 1  2823500000100  ...     320063,421     5618750,896\n272        Zerkall 2  2823490000100  ...     319449,788      5617784,98\n\n[273 rows x 16 columns]",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/data_load.html#data-type",
    "href": "dataprocess/data_load.html#data-type",
    "title": "Data Loading",
    "section": "2.4 Data type",
    "text": "2.4 Data type\nCompared to plain text files, Excel data already contains data type information for each cell. Therefore, the data type will be directly determined by the data type specified in Excel.\nHowever, there are instances where the data type in Excel is not correctly set, so manual data type conversion may be necessary. For more details, refer to Section 1.5.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "Data Loading"
    ]
  },
  {
    "objectID": "dataprocess/extract_spatial.html",
    "href": "dataprocess/extract_spatial.html",
    "title": "Values Extract",
    "section": "",
    "text": "Raster Data is actually a kind of Sample data of a area, the area will be divided in regular grids (equally sized rectangles). The typical raster data like elevation is already the most important data for many spatial-based research fields. The raster form is also the important form for meteorological data, in order the area-value (e.g. Temperature and Presentation) to represent. But for the application e.g. in the Hydrology need we some statistical values for specific research regions, especially the Average.\nTherefore we need the operate EXTRACT.\n\n\nThe basic Data are like:\n\n\n\nThe example Data\n\n\n\ntwo Raster data\ntwo Regions shape files\n\nFor the operate EXTRACT, we need basically the raster data and the regions (shape files). Before this we must confirm that, the both data are in the same CRS (coordinate reference system). In this Blog we discuss only the theories and ideas about EXTRACT. There will show total four methods:\n\n\nThe first method we just use the original Raster with original resolution. But when the resolution not so fine, it will occur to that the selected grids have the big difference than the region. This is one very typical Problem in meteorological data, they have not so gut space resolution because the time resolution is always finer than the common geological data. In order to balance the data size, we must reduce the space resolution.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint”\n\n\nFor the SELECT, there are two familiar methods Touch and Center-point:\n\nTouch: all the grids, who touched by the region, will be selected\nCenter-point: only the grids, who’s Center-point is with in the region, will be selected\n\nFor the both SELECT methods there some implausible cases:\n\nwhen we use Touch method, it will select some grids, who has only a little area within the region, like Cell 4\nCell 5: only an eighth of the area within the region, but it counts as a “whole cell” just because its center is in the region\nCell 18: with three quarters of the area in the region, but is not selected, just because the center is not in the region\n\nSummary we can say: the original resolution can be used, only when the deviation between the grids and region is not so big and\n\nTouch includes all grid cells that are touched, so can be used for some extreme statistical value (e.g. Max or Min)\nCenter-point can be used for the average value and actually the deviation maybe reduced, due to the surplus of selected grids and deficit of not selected grids in the boundary.\n\n\n\n\nThe second method is one simplest method, we need only refine our data in higher resolution, like resolution in 10 times finer and the grids will in 100 times more.\nEssentially there is no difference as 1. method, but the problem will be solved. This method is pointed, just because I must use Matlab processing the data, but there is no spatial Analyse Toolbox in Matlab. Therefore this is fast the only Answer, just because the Refine needs no supply from spatial Analyse Toolbox, we can easily repeat the data 10 more in row and 10 more in column.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint” after Disaggregation\n\n\nLike the figure shows: the accuracy is gut improvement, the deviation should lay under the 1%.\n\n\n\nThe weighted mean is always exacter than the numerical average. The important Point for the weighted mean is the weights, in the spatial analyse it’s the portion of the area. So, the main task in third method is calculate the area of every value, that within one region.\nIn order to calculate the area, we need actually convert the raster grids into shape, for the convert we have also two methods:\n\nthe same value as one Polygon (this method should more convenient for the categories data with only several value)\nevery grid as a rectangle polygon, then calculate the portion of the area, where is located within the region (this method is use in R::terra package, but there is also a small deviation, when the CRS lie in lon-lat. The portion of one grid will be not equal to the portion of the region, because the grid area one to one is already different.)\n\n\n\n\nIllustration von Extract mit Wert-Polygon\n\n\nIn the Illustration is every value as the same polygon converted.\n\n\n\nThis method is designed only for the meteorological data, those have the big mange on time scalar. It’s also the most effective method in the practice.\nThe theory and the formal is just like:\n\\[\n\\vec{\\Omega}_{[time,region]} = \\vec{A}_{[time,grid]} \\cdot \\vec{W}_{[grid,region]}\n\\]\n\\(\\vec{\\Omega}_{[time,region]}\\) = Region-value of every region\n\\(\\vec{A}_{[time,grid]}\\) = all Value in the matrix [time, grid]\n\\(\\vec{W}_{[grid,region]}\\) = Weights of every grid to every region in the matrix [grid,region]\n\n\nFor the Weight-Matrix calculate we just the portion of the grid area only that within the region to the whole region area (but not the whole grid), then divide the area of the region.\nOne example weight_grid:\n            [R1]      [R2]\n [G1]      0.000      0.00\n [G2] 134364.119 189431.77\n [G3] 212464.416      0.00\n [G4]   2747.413      0.00\n [G5] 150176.618      0.00\n [G6]      0.000  45011.22\nG for Grid and R for Region\n\n\n\nOne example mat_value:\n     [G1] [G2] [G3] [G4] [G5] [G6] \n[T1]    2    1    3    4    1    1  \n[T2]    3    1    2    4    1    1  \nT for Time\nThe end.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/extract_spatial.html#extract-from-raster",
    "href": "dataprocess/extract_spatial.html#extract-from-raster",
    "title": "Values Extract",
    "section": "",
    "text": "The basic Data are like:\n\n\n\nThe example Data\n\n\n\ntwo Raster data\ntwo Regions shape files\n\nFor the operate EXTRACT, we need basically the raster data and the regions (shape files). Before this we must confirm that, the both data are in the same CRS (coordinate reference system). In this Blog we discuss only the theories and ideas about EXTRACT. There will show total four methods:\n\n\nThe first method we just use the original Raster with original resolution. But when the resolution not so fine, it will occur to that the selected grids have the big difference than the region. This is one very typical Problem in meteorological data, they have not so gut space resolution because the time resolution is always finer than the common geological data. In order to balance the data size, we must reduce the space resolution.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint”\n\n\nFor the SELECT, there are two familiar methods Touch and Center-point:\n\nTouch: all the grids, who touched by the region, will be selected\nCenter-point: only the grids, who’s Center-point is with in the region, will be selected\n\nFor the both SELECT methods there some implausible cases:\n\nwhen we use Touch method, it will select some grids, who has only a little area within the region, like Cell 4\nCell 5: only an eighth of the area within the region, but it counts as a “whole cell” just because its center is in the region\nCell 18: with three quarters of the area in the region, but is not selected, just because the center is not in the region\n\nSummary we can say: the original resolution can be used, only when the deviation between the grids and region is not so big and\n\nTouch includes all grid cells that are touched, so can be used for some extreme statistical value (e.g. Max or Min)\nCenter-point can be used for the average value and actually the deviation maybe reduced, due to the surplus of selected grids and deficit of not selected grids in the boundary.\n\n\n\n\nThe second method is one simplest method, we need only refine our data in higher resolution, like resolution in 10 times finer and the grids will in 100 times more.\nEssentially there is no difference as 1. method, but the problem will be solved. This method is pointed, just because I must use Matlab processing the data, but there is no spatial Analyse Toolbox in Matlab. Therefore this is fast the only Answer, just because the Refine needs no supply from spatial Analyse Toolbox, we can easily repeat the data 10 more in row and 10 more in column.\n\n\n\nIllustration of Mask with “Touch” oder “Centerpoint” after Disaggregation\n\n\nLike the figure shows: the accuracy is gut improvement, the deviation should lay under the 1%.\n\n\n\nThe weighted mean is always exacter than the numerical average. The important Point for the weighted mean is the weights, in the spatial analyse it’s the portion of the area. So, the main task in third method is calculate the area of every value, that within one region.\nIn order to calculate the area, we need actually convert the raster grids into shape, for the convert we have also two methods:\n\nthe same value as one Polygon (this method should more convenient for the categories data with only several value)\nevery grid as a rectangle polygon, then calculate the portion of the area, where is located within the region (this method is use in R::terra package, but there is also a small deviation, when the CRS lie in lon-lat. The portion of one grid will be not equal to the portion of the region, because the grid area one to one is already different.)\n\n\n\n\nIllustration von Extract mit Wert-Polygon\n\n\nIn the Illustration is every value as the same polygon converted.\n\n\n\nThis method is designed only for the meteorological data, those have the big mange on time scalar. It’s also the most effective method in the practice.\nThe theory and the formal is just like:\n\\[\n\\vec{\\Omega}_{[time,region]} = \\vec{A}_{[time,grid]} \\cdot \\vec{W}_{[grid,region]}\n\\]\n\\(\\vec{\\Omega}_{[time,region]}\\) = Region-value of every region\n\\(\\vec{A}_{[time,grid]}\\) = all Value in the matrix [time, grid]\n\\(\\vec{W}_{[grid,region]}\\) = Weights of every grid to every region in the matrix [grid,region]\n\n\nFor the Weight-Matrix calculate we just the portion of the grid area only that within the region to the whole region area (but not the whole grid), then divide the area of the region.\nOne example weight_grid:\n            [R1]      [R2]\n [G1]      0.000      0.00\n [G2] 134364.119 189431.77\n [G3] 212464.416      0.00\n [G4]   2747.413      0.00\n [G5] 150176.618      0.00\n [G6]      0.000  45011.22\nG for Grid and R for Region\n\n\n\nOne example mat_value:\n     [G1] [G2] [G3] [G4] [G5] [G6] \n[T1]    2    1    3    4    1    1  \n[T2]    3    1    2    4    1    1  \nT for Time\nThe end.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/extract_spatial.html#extract-from-polygons",
    "href": "dataprocess/extract_spatial.html#extract-from-polygons",
    "title": "Values Extract",
    "section": "2.1 Extract from Polygons",
    "text": "2.1 Extract from Polygons\nWhen we just EXTRACT one value (one attribute) we can straight use the function intersect(), to intersect the data-polygons and regions-polygons then calculate the statistic values for every regions.\n\n\n\nIllustration of Extract with Polygon-value\n\n\nBut when we need to extract more values (attributes), again the idea from last Blog Exact with scale product will be used:\n\\[\n\\vec{\\Omega}_{[attribute,region]} = \\vec{A}_{[attribute,polygon]} \\cdot \\vec{W}_{[polygon,region]}\n\\]\n\\(\\vec{\\Omega}_{[attribute,region]}\\) = Region-value of every region\n\\(\\vec{A}_{[attribute,polygon]}\\) = attributes list in the matrix [attribute,polygon]\n\\(\\vec{W}_{[polygon,region]}\\) = Weights of every polygon to every region in the matrix [polygon,region]\n\nWeight-Matrix create: just use intersect() then statistic the portion of the value-area to the region-area in matrix [value-polygon, regions]\nValue-Matrix create: connect the attribute list\nscale product with both Value- and Weight-Matrix",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/extract_spatial.html#extract-from-points",
    "href": "dataprocess/extract_spatial.html#extract-from-points",
    "title": "Values Extract",
    "section": "2.2 Extract from Points",
    "text": "2.2 Extract from Points\n\n2.2.1 Numerical Mean\nThe moooost simple and direct method is the Numerical Mean of points in the region:\n\n\n\nIllustration of Extract with Point-Value: Numerical Mean of points in the region\n\n\n\nInterset with regions, then select points which in the region\nCalculate mean value of points\n\nThe weakness are also obviously, many points, who lay just near the boundary of region, will be ignored. It’s also familiar that in some regions there are no points laying in.\nSo, we need maybe convert the point-data to polygon- or raster-data\n\n\n2.2.2 Tiessen (Dirichlet) Polygon\nActually the convert to the polygon is the most popular and typical method specially with the Tiessen (Dirichlet) Polygon in meteorological fields.\n\n\n\nIllustration of Extract mit Punkt-Wert\n\n\n\nConvert point data to Tiessen polygon data\nuse the method of polygon like above\n\n\n\n2.2.3 Interpolate as Raster\nThe second convert idea is convert to the raster: Interpolation\n\n\n\nIllustration der Interpolation mit Punkt-Werten\n\n\nThe Interpolation is also one important issue, and it will be discussed in th near future. Here will just show the three most impotent methods: Nearest neighbor, IDW (Inverse distance weighted) and Kringing\nThe three methods are also very easy processed in R::terra, that will be showed in the next Blog.\nThe end.",
    "crumbs": [
      "Dataprocess",
      "Spatial Data",
      "Values Extract"
    ]
  },
  {
    "objectID": "dataprocess/NetCDF.html",
    "href": "dataprocess/NetCDF.html",
    "title": "NetCDF",
    "section": "",
    "text": "NetCDF stands for “Network Common Data Form.” It is a file format that is designed to store large arrays of data, primarily used in scientific and engineering applications. NetCDF files are self-describing, meaning they contain metadata along with the data, which makes it easier to understand the contents. NetCDF is particularly well-suited for storing multi-dimensional data, such as time series, spatial data, and climate model outputs. It can handle data with complex structures like grids, which are common in environmental and geospatial datasets.\nIn simple terms, NetCDF is a file format for storing multi-dimensional arrays of data along with metadata.\nMore Details in unidata.\nNetCDF files have a hierarchical structure, consisting of dimensions, variables, and attributes. Dimensions define the size of arrays, variables hold the data, and attributes provide additional information about the data.\nWith these three components, you can efficiently handle the import, creation, and export of data in the NetCDF format.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "NetCDF"
    ]
  },
  {
    "objectID": "dataprocess/NetCDF.html#library",
    "href": "dataprocess/NetCDF.html#library",
    "title": "NetCDF",
    "section": "1 Library",
    "text": "1 Library\n\nRPython\n\n\nThe ncdf4 R package is a powerful tool for working with NetCDF data in R, allowing you to read, write, and manipulate datasets in this format with ease and efficiency.\n\nlibrary(ncdf4)\nlibrary(tidyverse)\n\n# Define the NetCDF file path\nfn_NetCDF &lt;- \"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\"\n\n\n\nThe netCDF4 Python Library is a powerful tool for working with NetCDF data in R, allowing you to read, write, and manipulate datasets in this format with ease and efficiency.\n\nimport netCDF4 as nc\nimport numpy as np\n\n# Define the NetCDF file path\nfn_NetCDF = \"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\"\n\n\n\n\nThe Test data minibeispiel_NetCDF.nc is avable from Github data_share, but it can not be direcly read from Git hub so you need download to local.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "NetCDF"
    ]
  },
  {
    "objectID": "dataprocess/NetCDF.html#import",
    "href": "dataprocess/NetCDF.html#import",
    "title": "NetCDF",
    "section": "2 Import",
    "text": "2 Import\n\n2.1 Open\nThe first step in working with NetCDF files is to open the file using the nc_open() function. However, it’s important to note that opening the file doesn’t directly load its contents into the R environment. Instead, it establishes a connection between the file and the R session and effectively locks the file for reading or writing operations.\n\nRPython\n\n\n\n# Open the NetCDF file\nnc_Test &lt;- nc_open(fn_NetCDF)\n\n\n\n\n# Open the NetCDF file\nnc_Test = nc.Dataset(fn_NetCDF, \"r\")\n\n\n\n\n\n\n2.2 Basic Information\nAfter opening a NetCDF file in R, you can access the basic information about the dataset, which is contained in a list. This information typically includes details about three components: dimensions, variables, and attributes of the NetCDF file.\n\nRPython\n\n\n\n# Access the dimensions\n# nc_Test$dim\nnc_Test$dim |&gt; names()\n\n[1] \"latitude\"  \"longitude\" \"time\"     \n\n# Access the variables\n# nc_Test$var\nnc_Test$var |&gt; names()\n\n[1] \"T0\"  \"crs\"\n\nnc_Test$var$T0$size\n\n[1] 6 8 3\n\n# Access attributes\nncatt_get(nc_Test, 0)\n\n$title\n[1] \"Multidimensional data example\"\n\n$author\n[1] \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\"\n\n\n\n\n\n# Access the dimensions\nprint(nc_Test.dimensions)\n\n{'latitude': \"&lt;class 'netCDF4.Dimension'&gt;\": name = 'latitude', size = 6, 'longitude': \"&lt;class 'netCDF4.Dimension'&gt;\": name = 'longitude', size = 8, 'time': \"&lt;class 'netCDF4.Dimension'&gt;\" (unlimited): name = 'time', size = 3}\n\n# Access the variables\nprint(nc_Test.variables)\n\n{'latitude': &lt;class 'netCDF4.Variable'&gt;\nfloat64 latitude(latitude)\n    units: degrees_north\n    long_name: latitude\nunlimited dimensions: \ncurrent shape = (6,)\nfilling on, default _FillValue of 9.969209968386869e+36 used, 'longitude': &lt;class 'netCDF4.Variable'&gt;\nfloat64 longitude(longitude)\n    units: degrees_east\n    long_name: longitude\nunlimited dimensions: \ncurrent shape = (8,)\nfilling on, default _FillValue of 9.969209968386869e+36 used, 'time': &lt;class 'netCDF4.Variable'&gt;\nint32 time(time)\n    units: day since 1961-01-01 00:00:00 +00\n    long_name: time\nunlimited dimensions: time\ncurrent shape = (3,)\nfilling on, default _FillValue of -2147483647 used, 'T0': &lt;class 'netCDF4.Variable'&gt;\nfloat32 T0(time, longitude, latitude)\n    units: cel\n    _FillValue: -9999.0\nunlimited dimensions: time\ncurrent shape = (3, 8, 6)\nfilling on, 'crs': &lt;class 'netCDF4.Variable'&gt;\nfloat32 crs()\n    long_name: coordinate reference system\n    EPSG: EPSG:4236\nunlimited dimensions: \ncurrent shape = ()\nfilling on, default _FillValue of 9.969209968386869e+36 used}\n\n# Get the size of the \"T0\" variable\nprint(nc_Test.variables[\"T0\"].size)\n\n144\n\n# Access attributes associated with the NetCDF file\nprint(nc_Test.__dict__)\n\n{'title': 'Multidimensional data example', 'author': 'Kan, Lei, kan.lei@ruhr-uni-bochum.de'}\n\n\n\n\n\n\n\n2.3 Values and Attributes\nWith the basic information about variables obtained, you can access the values and attributes of each variable as needed. You can also obtain specific subsets of variables using start points and counts for each dimension.\nAdditionally, dimensions are treated as variables in the NetCDF structure, making it easier to work with them.\n\nRPython\n\n\n\nncvar_get()\n\nstart: The starting point of every dimension to load variable values\ncount: The length of every dimension to read\n\nncatt_get()\n\n\n# Retrieve the variable \"T0\" WHOLE\nncvar_get(nc_Test, \"T0\")\n\n, , 1\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]       [,7]\n[1,] 0.6815190 0.6685974 0.3355391 0.8300888 0.4126872 0.4213721 0.40900978\n[2,] 0.5834032 0.1514063 0.3042631 0.2520407 0.5793471 0.1833323 0.32067558\n[3,] 0.4924265 0.7376668 0.3327484 0.9493681 0.2489835 0.5988685 0.07783964\n[4,] 0.8509517 0.6454237 0.5295522 0.8479783 0.4104529 0.4381394 0.19551247\n[5,] 0.9409876 0.8425627 0.8565235 0.4752189 0.2917338 0.4781619 0.81465298\n[6,] 0.2088355 0.6121973 0.3734793 0.7684925 0.9713812 0.5124385 0.34575224\n          [,8]\n[1,] 0.5420827\n[2,] 0.8335595\n[3,] 0.9659727\n[4,] 0.6240343\n[5,] 0.7685761\n[6,] 0.3649738\n\n, , 2\n\n          [,1]      [,2]       [,3]      [,4]       [,5]      [,6]      [,7]\n[1,] 0.2292564 0.5033694 0.48205003 0.8213873 0.21405630 0.8678198 0.7539087\n[2,] 0.6349170 0.4978186 0.56034225 0.4690158 0.95863396 0.1501258 0.2760119\n[3,] 0.2507486 0.8313300 0.04853146 0.1445253 0.08056273 0.3183620 0.7758245\n[4,] 0.5768890 0.2531487 0.45174122 0.1941784 0.02582907 0.4415914 0.9977322\n[5,] 0.7781205 0.6768693 0.71639782 0.8491389 0.13584627 0.2038828 0.1653473\n[6,] 0.3596051 0.2155040 0.62368399 0.5900931 0.57847399 0.6779157 0.4215007\n           [,8]\n[1,] 0.80507427\n[2,] 0.36874512\n[3,] 0.21089411\n[4,] 0.32393828\n[5,] 0.49121958\n[6,] 0.05253027\n\n, , 3\n\n           [,1]       [,2]       [,3]       [,4]      [,5]      [,6]      [,7]\n[1,] 0.44514486 0.51928353 0.57821035 0.79330933 0.4629536 0.5375589 0.9546921\n[2,] 0.43322235 0.92014349 0.52168131 0.18247831 0.9246678 0.3676251 0.8033844\n[3,] 0.46280038 0.08913017 0.76012933 0.46169522 0.8953038 0.1482120 0.5176442\n[4,] 0.38045707 0.69357723 0.12975638 0.23547187 0.4842421 0.6838360 0.6942847\n[5,] 0.09263945 0.19688506 0.02503374 0.49694207 0.6021117 0.3664415 0.3513815\n[6,] 0.52355707 0.57813245 0.61153209 0.03659012 0.9769987 0.9546434 0.5306273\n          [,8]\n[1,] 0.3938288\n[2,] 0.4538694\n[3,] 0.9583837\n[4,] 0.5619988\n[5,] 0.4093841\n[6,] 0.6018101\n\n# Retrieve a subset of the variable \"T0\"\n# This subset starts at position (1, 1, 1) and has a count of (2, 3, 1) along each dimension\nncvar_get(nc_Test, \"T0\", start = c(1, 1, 1), count = c(2, 3, 1))\n\n          [,1]      [,2]      [,3]\n[1,] 0.6815190 0.6685974 0.3355391\n[2,] 0.5834032 0.1514063 0.3042631\n\n# Retrieve attributes associated with the variable \"T0\"\nncatt_get(nc_Test, \"T0\")\n\n$units\n[1] \"cel\"\n\n$`_FillValue`\n[1] -9999\n\n\n\n\n\nnc.variables[\"var_Name\"]\nnc.variables[\"var_Name\"].__dict__\n\n\n# Retrieve the entire \"T0\" variable\nt0_variable = nc_Test.variables[\"T0\"][:]\nprint(\"T0 variable (whole):\", t0_variable)\n\nT0 variable (whole): [[[0.68151903 0.58340317 0.4924265  0.85095173 0.9409876  0.20883553]\n  [0.6685974  0.15140632 0.7376668  0.64542365 0.84256274 0.61219734]\n  [0.33553913 0.30426314 0.33274835 0.52955216 0.8565235  0.3734793 ]\n  [0.8300888  0.2520407  0.9493681  0.8479783  0.47521892 0.7684925 ]\n  [0.4126872  0.5793471  0.24898352 0.41045293 0.29173383 0.97138125]\n  [0.42137206 0.18333228 0.59886855 0.43813944 0.47816187 0.5124385 ]\n  [0.40900978 0.32067558 0.07783964 0.19551247 0.814653   0.34575224]\n  [0.54208267 0.83355945 0.96597266 0.62403435 0.76857615 0.3649738 ]]\n\n [[0.22925638 0.63491696 0.2507486  0.576889   0.77812046 0.35960513]\n  [0.5033694  0.4978186  0.83133    0.25314873 0.6768693  0.21550402]\n  [0.48205003 0.56034225 0.04853146 0.45174122 0.7163978  0.623684  ]\n  [0.8213873  0.4690158  0.14452533 0.1941784  0.8491389  0.5900931 ]\n  [0.2140563  0.95863396 0.08056273 0.02582907 0.13584627 0.578474  ]\n  [0.8678198  0.15012585 0.31836203 0.44159144 0.20388278 0.6779157 ]\n  [0.7539087  0.2760119  0.7758245  0.9977322  0.16534728 0.4215007 ]\n  [0.8050743  0.36874512 0.21089411 0.32393828 0.49121958 0.05253027]]\n\n [[0.44514486 0.43322235 0.46280038 0.38045707 0.09263945 0.52355707]\n  [0.51928353 0.9201435  0.08913017 0.69357723 0.19688506 0.57813245]\n  [0.57821035 0.5216813  0.76012933 0.12975638 0.02503374 0.6115321 ]\n  [0.79330933 0.18247831 0.46169522 0.23547187 0.49694207 0.03659012]\n  [0.46295357 0.9246678  0.8953038  0.48424208 0.6021117  0.97699875]\n  [0.53755885 0.36762506 0.14821199 0.68383604 0.36644155 0.95464337]\n  [0.9546921  0.8033844  0.5176442  0.69428474 0.35138154 0.53062725]\n  [0.3938288  0.45386937 0.95838374 0.56199884 0.4093841  0.6018101 ]]]\n\n# Retrieve a subset of the \"T0\" variable\n# This subset starts at position (0, 0, 0) and has a count of (2, 3, 1) along each dimension\nprint(t0_variable[0:2, 0:3, 0:1])\n\n[[[0.68151903]\n  [0.6685974 ]\n  [0.33553913]]\n\n [[0.22925638]\n  [0.5033694 ]\n  [0.48205003]]]\n\n# Access attributes associated with the \"T0\" variable\nprint(nc_Test.variables[\"T0\"].__dict__)\n\n{'units': 'cel', '_FillValue': np.float32(-9999.0)}\n\n\n\n\n\n\n\n2.4 Close\nWhen working with NetCDF files in R using the ncdf4 package, it’s crucial to remember that opening a file establishes a connection. This prevents data corruption and conflicts. To finish, always close the file using nc_close() once you’ve completed your operations.\n\nRPython\n\n\n\n# Close the NetCDF file\nnc_close(nc_Test)\n\n\n\n\n# Close the NetCDF file\nnc_Test.close()",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "NetCDF"
    ]
  },
  {
    "objectID": "dataprocess/NetCDF.html#create-and-export",
    "href": "dataprocess/NetCDF.html#create-and-export",
    "title": "NetCDF",
    "section": "3 Create and Export",
    "text": "3 Create and Export\nIn this section, we will walk you through the steps to create a NetCDF file with your data. By following these steps, you’ll be able to prepare your data and save it in the NetCDF format for further analysis or sharing.\n\n3.1 Create new empty NetCDF file (Python)\n\nRPython\n\n\nIn R, you need after defining the dimensions and variables to create the file in the system. See Section 3.4.\n\n\nIn Python, you first need to create (connect) a new empty file in the system and an object in Python.\n\n# Create a NetCDF file\nnc_Create = nc.Dataset(\"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF_Py.nc\", \"w\")\n\n\n\n\n\n\n3.2 Define the Dimensions\nThe initial step in creating a NetCDF dataset is dimension definition:\n\nRPython\n\n\n\nncdim_def()\n\n\n# Define dimension metadata\nnum_Dim_Lon &lt;- seq(11.72, 11.79, 0.01)\nnum_Dim_Lat &lt;- seq(50.08, 50.13, 0.01)\nnum_Dim_Time &lt;- 1:3\n\ndim_lon &lt;- ncdim_def(\"longitude\", \"degrees_east\",\n                     num_Dim_Lon,\n                     longname = \"longitude\")\ndim_lat &lt;- ncdim_def(\"latitude\", \"degrees_north\",\n                     num_Dim_Lat,\n                     longname = \"latitude\")\ndim_time &lt;- ncdim_def(\"time\", \"day since 1961-01-01 00:00:00 +00\",\n                      num_Dim_Time, unlim=TRUE,\n                      longname = \"time\")\n\n\n\n\nnc.createDimension()\n\n\n# Define dimension metadata\nnum_Dim_Lon = np.arange(11.72, 11.8, 0.01)\nnum_Dim_Lat = np.arange(50.08, 50.14, 0.01)\nnum_Dim_Time = np.arange(1, 4)\n\n\n# Define dimensions\nnc_Create.createDimension(\"longitude\", len(num_Dim_Lon))\n\n\"&lt;class 'netCDF4.Dimension'&gt;\": name = 'longitude', size = 9\n\nnc_Create.createDimension(\"latitude\", len(num_Dim_Lat))\n\n\"&lt;class 'netCDF4.Dimension'&gt;\": name = 'latitude', size = 7\n\nnc_Create.createDimension(\"time\", len(num_Dim_Time))  # Use None for unlimited dimension\n\n\"&lt;class 'netCDF4.Dimension'&gt;\": name = 'time', size = 3\n\n\ndim_lon = nc_Create.createVariable(\"longitude\", \"f4\", \"longitude\")\ndim_lat = nc_Create.createVariable(\"latitude\", \"f4\", \"latitude\")\ndim_time = nc_Create.createVariable(\"time\", \"i\", \"time\") \n\ndim_lon[:] = num_Dim_Lon\ndim_lat[:] = num_Dim_Lat\ndim_time[:] = num_Dim_Time\n\nCompared to R, in Python, you need to create a variable with the same name to store the values of the dimension. In Python, a pure dimension will only consider the dimension’s size and name.\n\n\n\nIn this example, we will create a 3D array with latitude, longitude, and time dimensions.\n\n\n3.3 Define the Variales\nThe next step is to define a variable, but you don’t need to assign values to it at this stage. There are three common attributes (name, units and dimensions) that are essential for every variable and should always be defined. Other user-defined attributes can be added later as needed.\n\nRPython\n\n\n\nncvar_def()\n\nname\nunits\ndim\n\n\nYou also have the option to create a dimension with no data values, effectively making it a null dimension. However, you can still set attributes for this dimension to store non-array information.\nAfter defining all the variables, it’s necessary to gather them into a list.\n\n# Define a variable named \"T0\" with the units \"cel\" and dimensions dim_lat, dim_lon, and dim_time.\n# The missing value for this variable is set to -9999.\nvar_T0 &lt;- ncvar_def(\"T0\", \"cel\", list(dim_lat, dim_lon, dim_time), -9999)\n\n# Define a variable named \"crs\" with no units and no dimensions (empty list).\n# This variable is defined as NULL initially.\nvar_crs &lt;- ncvar_def(\"crs\", \"\", list(), NULL)\n\n# Combine variables into a list\nvars &lt;- list(var_T0, var_crs)\n\n\n\n\nnc.createVariable()\n\nname\nunits\ndim\n\n\n\n# Define variables\nvar_T0 = nc_Create.createVariable(\"T0\", \"f4\", (\"latitude\", \"longitude\", \"time\"))\nvar_T0.units = \"cel\"\nvar_T0.missing_value = -9999\n\nvar_crs = nc_Create.createVariable(\"crs\", \"S1\")  # Create an empty variable\n\n\n\n\n\n\n3.4 Create new empty NetCDF file (R)\n\nR\n\n\nYou can now create a NetCDF file with the (list of) variables you have:\n\nnc_create(filename, vars)\n\n\nnc_Create &lt;- nc_create(\"C:\\\\Lei\\\\HS_Web\\\\data_share\\\\minibeispiel_NetCDF.nc\", vars)\n\n\n\n\n\n\n3.5 Put the Data\nAfter creating the NetCDF file, it will be an empty file in your local folder. The next step is to populate the file with data for each of the variables. This involves specifying the values for each variable and writing them to the file.\n\nRPython\n\n\n\nncvar_put()\n\n\nncvar_put(nc_Create, var_T0, runif(length(num_Dim_Lat) * length(num_Dim_Lon) * length(num_Dim_Time)))\n\n\n\n\n# Add data to the \"T0\" variable (random data)\nvar_T0[:] = np.random.rand(len(num_Dim_Lat), len(num_Dim_Lon), len(num_Dim_Time))\n\n\n\n\n\n\n3.6 Put Attributes\nWhen populating a NetCDF file, it’s essential to not only specify the variable data values but also the attributes associated with those variables. Attributes provide crucial metadata that describes the data, such as units, long names, and other relevant information.\n\nRPython\n\n\n\nncatt_put()\n\nAbsolutely, you can set attributes not only for individual variables.\n\n# Add the \"long_name\" and \"EPSG\" attributes to the variable \"var_crs\"\nncatt_put(nc_Create, var_crs, \"long_name\", \"coordinate reference system\")\nncatt_put(nc_Create, var_crs, \"EPSG\", \"EPSG:4236\")\n\n\n\n\nvar_crs.long_name = \"coordinate reference system\"\nvar_crs.EPSG = \"EPSG:4236\"\n\n\n\n\nBut also for the entire NetCDF file as global attributes. Global attributes provide overarching information about the dataset, such as its title, source, creation date, and any other relevant details.\n\nRPython\n\n\n\n# Add the \"title\" and \"author\" global attributes to the NetCDF file\nncatt_put(nc_Create, 0, \"title\", \"Multidimensional data example\")\nncatt_put(nc_Create, 0, \"author\", \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\")\n\n\n\n\n# Add global attributes\nnc_Create.title = \"Multidimensional data example\"\nnc_Create.author = \"Kan, Lei, kan.lei@ruhr-uni-bochum.de\"\n\n\n\n\n\n\n3.7 Close\nAt the end, make sure to close the connections to your NetCDF files.\n\nRPython\n\n\n\nnc_close(nc_Create)\n\n\n\n\n# Close the NetCDF file\nnc_Create.close()\n\n\n\n\nOnce you’ve gone through these steps, you’ll have a well-maintained NetCDF file that can be easily used for any further processing, transformations, or visualization.",
    "crumbs": [
      "Dataprocess",
      "Data-Processing",
      "NetCDF"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html",
    "href": "dataprocess/statistic_basic.html",
    "title": "Statistic Basic",
    "section": "",
    "text": "One of the most important tasks while analyzing any time series is to describe and summarize the time series data in forms, which easily convey their important characteristics.\nKey statistical characteristics often described include: a measure of the central tendency of the data, a measure of spread or variability, a measure of the symmetry of the data distribution, and perhaps estimates of extremes such as some large or small percentile (Snedecor and Cochran 1980).\n\n\nAccording to Helsel and Hirsch (2020), the data about which a statement or summary is to be made are called ‘population’ or sometimes ‘target population’. It may be impossible both physically and economically to collect all data of interest. Alternatively, a subset of the entire data called ‘sample’ is selected and measured in such a way that conclusions about the sample may be extended to the entire population.\n\n\n\nIn statistics, measures of location or central tendency are used to summarize and describe the central or typical value in a dataset. Here are the six common measures of location (Machiwal and Jha 2012):\n\nMean: The mean, often referred to as the average, is calculated by summing all the values in a dataset and dividing by the number of values. It represents the balance point of the data.\nMedian: The median is the middle value when the data is sorted in ascending order. It’s less sensitive to extreme values (outliers) than the mean and is a good measure of the central value when the data is skewed.\nMode: The mode is the value that appears most frequently in the dataset. There can be multiple modes in a dataset, and it’s useful for categorical or discrete data.\nGeometric Mean: The geometric mean is used for data that is not normally distributed, such as financial returns or growth rates. It’s calculated by taking the nth root of the product of n values.\nTrimmed Mean: The trimmed mean is a variation of the mean that removes a certain percentage of extreme values (usually a specified percentage from both tails of the distribution) before calculating the mean. This makes it more robust to outliers.\n\nAmong these measures, the mean and median are the most widely used for summarizing data.\n\n\nThe arithmetic mean (\\(\\overline{{x}}\\)) is calculated by summing up of all data values, \\(x_{\\mathrm{i}}\\) and dividing the sum by the sample size \\(n\\):\n\\[\n{\\overline{{x}}}=\\sum_{i=1}^{n}{\\frac{x_{\\mathrm{i}}}{n}}\n\\]\n\n\n\nThe median is the middle value in a dataset when the data is ordered from smallest to largest. It’s a robust measure of central tendency that is not influenced by extreme values (outliers).\nFor an ordered dataset with ‘n’ values:\n\nIf ‘n’ is odd, the median is the middle value: \\[\n\\text{M} = x_{\\frac{n+1}{2}}\n\\]\nIf ‘n’ is even, the median is the average of the two middle values: \\[\n\\text{M} = \\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}\n\\]\n\n\n\n\nThe geometric mean (GM) is often used to compute summary statistic for positively skewed datasets (Machiwal and Jha 2012).\n\\[\n{\\mathrm{GM}}={\\mathrm{exp}}\\left[\\sum_{i=1}^{n}{\\frac{\\ln\\left(x_{\\mathrm{i}}\\right)}{n}}\\right]\n\\]\nFor the positively skewed data series, the GM is usually fairly close to the median of the series. In fact, the GM is an unbiased estimate of the median when the logarithms of the datasets are symmetric (Helsel et al. 2020).\n\n\n\n\n\n\nThe ‘sample variance’ and ‘sample standard deviation’ (square root of sample variance) are classical measures of spread (dispersion), which are the most common measures of dispersion (Machiwal and Jha 2012).\n\\[\ns^{2}=\\sum_{i=1}^{n}\\frac{\\left(x_{\\mathrm{i}}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}\n\\]\n\\[\ns={\\sqrt{\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}}}}\n\\]\n\n\n\nRobust measures of spreading about the mean include ‘range’, ‘interquartile range’, ‘coefficient of variation’ and ‘median absolute deviation’ (Machiwal and Jha 2012).\n\n\nQuantiles are values that divide a dataset into equally sized subsets. Common quantiles include quartiles (dividing data into four parts), quintiles (dividing into five parts), deciles (dividing into ten parts), and percentiles (dividing into one hundred parts).\n\nSort the dataset in ascending order.\nCompute the index ‘i’ as\n\n\\[\ni = \\text{round}((n+1) \\cdot q)\n\\]\n\nIf ‘i’ is an integer, the quantile is\n\n\\[\n\\text{Q}(q) = x_i\n\\] - If ‘i’ is not an integer, the quantile is interpolated as\n\\[\n\\text{Q}(q) = x_{\\lfloor i \\rfloor} + (i - \\lfloor i \\rfloor) \\cdot (x_{\\lfloor i \\rfloor + 1} - x_{\\lfloor i \\rfloor})\n\\]\nQuantiles are used to understand the spread and distribution of data and are often used in box plots and histograms to visualize data distribution.\n\n\n\nThe coefficient of variation (CV) gives a normalized measure of spreading about the mean, and is estimated as (Machiwal and Jha 2012):\n\\[\n\\mathbf{C}\\mathbf{V}(\\vartheta_{0})={\\frac{s}{\\bar{x}}}\\times100\n\\]\nHydrologic variables with larger CV values are more variable than those with smaller values. Wilding (in (SoilSpatialVariability_nielsen_1985?)) suggested a classification scheme for identifying the extent of variability for soil properties based on their CV values, where CV values of 0-15, 16-35 and &gt;36 indicate little, moderate and high variability, respectively.\n\n\n\nQuartile coefficient (QC) of dispersion is another descriptive statistic which measures dispersion and is used to make comparison within and between datasets. The test-statistic is computed using the first (P25) and third (P75) quartiles for each data set. The quartile coefficient of dispersion (QC) is given as (Machiwal and Jha 2012):\n\\[\n\\text{QC}={\\frac{P_{75}-P_{25}}{P_{75}+P_{25}}}\n\\]\n\n\n\n\n\nHydrologic time series data are usually skewed, which means that data in the time series are not symmetric around the mean or median, with extreme values extending out longer in one direction (Machiwal and Jha 2012).\n\n\nIt is defined as the adjusted third moment about the mean divided by the cube of the standard deviation (s), and is mathematically expressed as follows:\n\\[\ng={\\frac{n}{\\left(n-1\\right)\\,\\left(n-2\\right)}}\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{3}}{s^{3}}}\n\\]\nA positively skewed distribution of hydrologic time series with right extended tail has a positive coefficient of skewness, whereas a time series with negative-skewed distribution with left extended tail has a negative coefficient of skewness (Machiwal and Jha 2012).\n\n\n\nA robust measure of skewness is the ‘quartile skew coefficient (QS)’, which is defined as the difference in distances of the upper and lower quartiles from the median, divided by the IQR (Kenney John F 1939). Mathematically, it is expressed as:\n\\[\n\\text{QS}=\\frac{\\left(P_{75}-P_{50}\\,\\right)-\\left(P_{50}-P_{25}\\,\\right)}{P_{75}-P_{25}}\n\\]",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html#population-and-sample",
    "href": "dataprocess/statistic_basic.html#population-and-sample",
    "title": "Statistic Basic",
    "section": "",
    "text": "According to Helsel and Hirsch (2020), the data about which a statement or summary is to be made are called ‘population’ or sometimes ‘target population’. It may be impossible both physically and economically to collect all data of interest. Alternatively, a subset of the entire data called ‘sample’ is selected and measured in such a way that conclusions about the sample may be extended to the entire population.",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-location",
    "href": "dataprocess/statistic_basic.html#measures-of-location",
    "title": "Statistic Basic",
    "section": "",
    "text": "In statistics, measures of location or central tendency are used to summarize and describe the central or typical value in a dataset. Here are the six common measures of location (Machiwal and Jha 2012):\n\nMean: The mean, often referred to as the average, is calculated by summing all the values in a dataset and dividing by the number of values. It represents the balance point of the data.\nMedian: The median is the middle value when the data is sorted in ascending order. It’s less sensitive to extreme values (outliers) than the mean and is a good measure of the central value when the data is skewed.\nMode: The mode is the value that appears most frequently in the dataset. There can be multiple modes in a dataset, and it’s useful for categorical or discrete data.\nGeometric Mean: The geometric mean is used for data that is not normally distributed, such as financial returns or growth rates. It’s calculated by taking the nth root of the product of n values.\nTrimmed Mean: The trimmed mean is a variation of the mean that removes a certain percentage of extreme values (usually a specified percentage from both tails of the distribution) before calculating the mean. This makes it more robust to outliers.\n\nAmong these measures, the mean and median are the most widely used for summarizing data.\n\n\nThe arithmetic mean (\\(\\overline{{x}}\\)) is calculated by summing up of all data values, \\(x_{\\mathrm{i}}\\) and dividing the sum by the sample size \\(n\\):\n\\[\n{\\overline{{x}}}=\\sum_{i=1}^{n}{\\frac{x_{\\mathrm{i}}}{n}}\n\\]\n\n\n\nThe median is the middle value in a dataset when the data is ordered from smallest to largest. It’s a robust measure of central tendency that is not influenced by extreme values (outliers).\nFor an ordered dataset with ‘n’ values:\n\nIf ‘n’ is odd, the median is the middle value: \\[\n\\text{M} = x_{\\frac{n+1}{2}}\n\\]\nIf ‘n’ is even, the median is the average of the two middle values: \\[\n\\text{M} = \\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}\n\\]\n\n\n\n\nThe geometric mean (GM) is often used to compute summary statistic for positively skewed datasets (Machiwal and Jha 2012).\n\\[\n{\\mathrm{GM}}={\\mathrm{exp}}\\left[\\sum_{i=1}^{n}{\\frac{\\ln\\left(x_{\\mathrm{i}}\\right)}{n}}\\right]\n\\]\nFor the positively skewed data series, the GM is usually fairly close to the median of the series. In fact, the GM is an unbiased estimate of the median when the logarithms of the datasets are symmetric (Helsel et al. 2020).",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-spreaddispersion",
    "href": "dataprocess/statistic_basic.html#measures-of-spreaddispersion",
    "title": "Statistic Basic",
    "section": "",
    "text": "The ‘sample variance’ and ‘sample standard deviation’ (square root of sample variance) are classical measures of spread (dispersion), which are the most common measures of dispersion (Machiwal and Jha 2012).\n\\[\ns^{2}=\\sum_{i=1}^{n}\\frac{\\left(x_{\\mathrm{i}}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}\n\\]\n\\[\ns={\\sqrt{\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{2}}{\\left(n-1\\right)}}}}\n\\]\n\n\n\nRobust measures of spreading about the mean include ‘range’, ‘interquartile range’, ‘coefficient of variation’ and ‘median absolute deviation’ (Machiwal and Jha 2012).\n\n\nQuantiles are values that divide a dataset into equally sized subsets. Common quantiles include quartiles (dividing data into four parts), quintiles (dividing into five parts), deciles (dividing into ten parts), and percentiles (dividing into one hundred parts).\n\nSort the dataset in ascending order.\nCompute the index ‘i’ as\n\n\\[\ni = \\text{round}((n+1) \\cdot q)\n\\]\n\nIf ‘i’ is an integer, the quantile is\n\n\\[\n\\text{Q}(q) = x_i\n\\] - If ‘i’ is not an integer, the quantile is interpolated as\n\\[\n\\text{Q}(q) = x_{\\lfloor i \\rfloor} + (i - \\lfloor i \\rfloor) \\cdot (x_{\\lfloor i \\rfloor + 1} - x_{\\lfloor i \\rfloor})\n\\]\nQuantiles are used to understand the spread and distribution of data and are often used in box plots and histograms to visualize data distribution.\n\n\n\nThe coefficient of variation (CV) gives a normalized measure of spreading about the mean, and is estimated as (Machiwal and Jha 2012):\n\\[\n\\mathbf{C}\\mathbf{V}(\\vartheta_{0})={\\frac{s}{\\bar{x}}}\\times100\n\\]\nHydrologic variables with larger CV values are more variable than those with smaller values. Wilding (in (SoilSpatialVariability_nielsen_1985?)) suggested a classification scheme for identifying the extent of variability for soil properties based on their CV values, where CV values of 0-15, 16-35 and &gt;36 indicate little, moderate and high variability, respectively.\n\n\n\nQuartile coefficient (QC) of dispersion is another descriptive statistic which measures dispersion and is used to make comparison within and between datasets. The test-statistic is computed using the first (P25) and third (P75) quartiles for each data set. The quartile coefficient of dispersion (QC) is given as (Machiwal and Jha 2012):\n\\[\n\\text{QC}={\\frac{P_{75}-P_{25}}{P_{75}+P_{25}}}\n\\]",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/statistic_basic.html#measures-of-skewness",
    "href": "dataprocess/statistic_basic.html#measures-of-skewness",
    "title": "Statistic Basic",
    "section": "",
    "text": "Hydrologic time series data are usually skewed, which means that data in the time series are not symmetric around the mean or median, with extreme values extending out longer in one direction (Machiwal and Jha 2012).\n\n\nIt is defined as the adjusted third moment about the mean divided by the cube of the standard deviation (s), and is mathematically expressed as follows:\n\\[\ng={\\frac{n}{\\left(n-1\\right)\\,\\left(n-2\\right)}}\\sum_{i=1}^{n}{\\frac{\\left(x_{i}-{\\overline{{x}}}\\,\\right)^{3}}{s^{3}}}\n\\]\nA positively skewed distribution of hydrologic time series with right extended tail has a positive coefficient of skewness, whereas a time series with negative-skewed distribution with left extended tail has a negative coefficient of skewness (Machiwal and Jha 2012).\n\n\n\nA robust measure of skewness is the ‘quartile skew coefficient (QS)’, which is defined as the difference in distances of the upper and lower quartiles from the median, divided by the IQR (Kenney John F 1939). Mathematically, it is expressed as:\n\\[\n\\text{QS}=\\frac{\\left(P_{75}-P_{50}\\,\\right)-\\left(P_{50}-P_{25}\\,\\right)}{P_{75}-P_{25}}\n\\]",
    "crumbs": [
      "Dataprocess",
      "Statistic",
      "Statistic Basic"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html",
    "href": "dataprocess/timeserises_analyse.html",
    "title": "Time Series Analyse",
    "section": "",
    "text": "A time series is often adequately described as a function of four components: trend, seasonality, dependent stochastic component and independent residual component (Machiwal and Jha 2012). It can be mathematically expressed as (Shahin, Oorschot, and Lange 1993):\n\\[\nx_{\\mathrm{t}}=T_{\\mathrm{t}}+S_{\\mathrm{t}}+\\varepsilon_{\\mathrm{t}}+\\eta_{\\mathrm{t}}\n\\]\nwhere\n\n\\(T_{\\mathrm{t}}\\) = trend component,\n\\(S_{\\mathrm{t}}\\) = seasonality,\n\\(\\varepsilon_{\\mathrm{t}}\\) = dependent stochastic component, and\n\\(\\eta_{\\mathrm{t}}\\) = independent residual component.\n\nThe first two components can be treat as systematic pattern, which are deterministic in nature, whereas the stochastic component accounts for the random error.\n\n\nFor demonstration purposes, we will use a synthetic dataset derived from daily temperature data recorded at the Düsseldorf station of the DWD, covering the period from 2005 to 2024.\nThe following R packages are required and will be loaded below:\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\nlibrary(xts)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(car)\n\nxts_Component &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Component.csv\") |&gt; as.xts()\n\nThe components of the dataset from 2020 to 2024 are shown as follows:\n\n# --- 1. Fit a loess trend to the time series\n# Use a span of 1 (full smoothing window) to estimate the trend\ntrend_Fit100 &lt;- stats::loess(coredata(xts_Component) ~ as.numeric(index(xts_Component)), span = 1)\n\n# Extract the fitted trend values\ntrend_Loess100 &lt;- trend_Fit100$fitted\nxts_Trend &lt;- xts(trend_Loess100, order.by = index(xts_Component))\n# Detrend the original series by subtracting the fitted trend\nxts_Detrend &lt;- xts_Component - trend_Loess100\n\n# Extract the time index from the xts object\ntime_index &lt;- index(xts_Component)\n\n# --- 2. Compute seasonal component based on day of the year\n# Convert dates to day-of-year (1–365/366)\nseason_Day_Index &lt;- as.numeric(format(time_index, \"%j\"))\n\n# Compute the mean of the detrended series for each day-of-year\n# This gives the seasonal effect for each day\nseason_Day &lt;- ave(xts_Detrend, season_Day_Index, FUN = mean, na.rm = TRUE)\n\n# Compute the residual component (remainder) after removing trend and seasonal effects\nremainder_Day &lt;- xts_Detrend - season_Day\n\n# --- 3. Combine results into a tidy data frame for plotting\ndf_TS_Plot &lt;- data.frame(\n  date = index(xts_Component[\"2020-01-01/2024-12-31\"]),             # time index\n  Original = xts_Component[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),# original time series\n  Trend = xts_Trend[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),  # extracted trend\n  Seasonal = season_Day[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),   # extracted seasonal component\n  Residual = remainder_Day[\"2020-01-01/2024-12-31\"] |&gt; as.numeric() # residual component\n) |&gt;\n  pivot_longer(\n    cols = c(Original, Trend, Seasonal, Residual), # pivot components into long format\n    names_to = \"Component\",                        # column indicating component type\n    values_to = \"Value\"                            # column containing values\n  )\n\n# --- 4. Plot the time series decomposition using ggplot2\ngp_TS &lt;- ggplot(df_TS_Plot, aes(x = date, y = Value)) +\n  geom_line(aes(color = Component)) +                # plot lines colored by component\n  facet_wrap(~Component, ncol = 1, scales = \"free_y\") + # separate facet for each component, free y-scale\n  labs(\n    x = NULL, \n    y = \"Value\", \n    title = \"Time Series Decomposition\"\n  ) +\n  theme(\n    strip.text = element_text(face = \"bold\", size = 12), # bold facet labels\n    panel.grid.minor = element_blank()                  # remove minor grid lines\n  )\n\n# Convert ggplot object to interactive plotly plot\nggplotly(gp_TS)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#stationarity-tests",
    "href": "dataprocess/timeserises_analyse.html#stationarity-tests",
    "title": "Time Series Analyse",
    "section": "2.1 Stationarity Tests",
    "text": "2.1 Stationarity Tests\n\n2.1.1 Augmented Dickey-Fuller (ADF) Test\nThe Augmented Dickey-Fuller (ADF) test checks whether a time series has a unit root, i.e., whether it is non-stationary.\n\nNull hypothesis (H₀): The series has a unit root → non-stationary\n\nAlternative hypothesis (H₁): The series is stationary\n\nInterpretation:\n\np-value &lt; 0.05 → reject H₀ → series is stationary\n\np-value ≥ 0.05 → fail to reject H₀ → series is likely non-stationary\n\n\nadf.test(coredata(xts_Component))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  coredata(xts_Component)\nDickey-Fuller = -5.7176, Lag order = 19, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n2.1.2 KPSS Test\nThe KPSS (Kwiatkowski–Phillips–Schmidt–Shin) test is complementary to the ADF test.\nIt tests whether a series is stationary around a level or trend.\n\nNull hypothesis (H₀): The series is stationary\n\nAlternative hypothesis (H₁): The series is non-stationary\n\nInterpretation:\n\np-value ≥ 0.05 → fail to reject H₀ → series is stationary\n\np-value &lt; 0.05 → reject H₀ → series is non-stationary\n\n\nkpss.test(coredata(xts_Component), null = \"Level\")\n\n\n    KPSS Test for Level Stationarity\n\ndata:  coredata(xts_Component)\nKPSS Level = 0.4444, Truncation lag parameter = 11, p-value = 0.05802",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#trend",
    "href": "dataprocess/timeserises_analyse.html#trend",
    "title": "Time Series Analyse",
    "section": "2.2 Trend",
    "text": "2.2 Trend\nTrend analysis helps reveal long-term changes in a time series by smoothing out short-term fluctuations.\nDifferent methods can be used depending on the data characteristics and analysis goals — from flexible non-parametric smoothing to simple linear regression or aggregation-based trends.\n\n2.2.1 LOESS Fit\nThe LOESS (Locally Estimated Scatterplot Smoothing) method provides a flexible, non-linear fit that adapts to local patterns in the data.\nIt is especially useful when the trend is not strictly linear but varies over time.\nThe span parameter controls the smoothness — higher values produce smoother curves.\n\n# LOESS with a wide smoothing window (span = 1)\ntrend_Fit100 &lt;- stats::loess(coredata(xts_Component) ~ as.numeric(index(xts_Component)), span = 1)\ntrend_Loess100 &lt;- trend_Fit100$fitted\n\n# LOESS with a narrower smoothing window (span = 0.5)\ntrend_Fit50 &lt;- stats::loess(coredata(xts_Component) ~ as.numeric(index(xts_Component)), span = 0.5)\ntrend_Loess50 &lt;- trend_Fit50$fitted\n\n\n\n2.2.2 Linear Regression Fit\nA linear regression trend assumes that the time series follows a steady, linear increase or decrease over time. This simple model is often used as a baseline or to capture overall tendencies.\nThis approach assumes linearity — it may not capture seasonal or cyclical behavior, but it provides a clear long-term direction.\n\n# Fit a linear regression model on time (index) and data values\ntrend_LM &lt;- stats::lm(coredata(xts_Component) ~ as.numeric(index(xts_Component)))\n\n# Extract fitted (predicted) trend values\ntrend_Lin &lt;- trend_LM$fitted.values\n\n\n\n2.2.3 Period Means\nFor datasets with strong seasonality or noise, calculating period averages (e.g., yearly means) can be an effective way to reveal large-scale trends. A linear fit on these aggregated means reduces variability and highlights gradual changes over longer timescales.\nYearly-mean trends are particularly useful when short-term fluctuations (like daily or monthly variability) obscure the long-term signal.\n\n# Aggregate the data to yearly means\nxts_Year &lt;- apply.yearly(xts_Component, colMeans)\n\n# Set the date to mid-year for better visual alignment\nindex(xts_Year) &lt;- as.Date(paste0(format(index(xts_Year), \"%Y\"), \"-07-01\"))\n\n# Fit a linear model to the yearly means\ntrend_YearLM &lt;- stats::lm(coredata(xts_Year) ~ as.numeric(index(xts_Year)))\n\n# Extract the fitted trend line for yearly data\ntrend_YearLin &lt;- trend_YearLM$fitted.values\n\n\n\n2.2.4 Trend Visualization\nThe following plot compares different trend estimation approaches — LOESS fits, linear regression, and yearly mean trends.\n\n\nCode\n# Convert daily xts data to a data frame for plotting\ndf_TS_Trend_Daily &lt;- data.frame(\n  Date = index(xts_Component) |&gt; as_date(),\n  Loess100 = trend_Loess100,     # LOESS with span = 1.0\n  Loess50  = trend_Loess50,      # LOESS with span = 0.5\n  Lin      = trend_Lin           # Linear regression (full dataset)\n)\n\n# Convert yearly aggregated data\ndf_TS_Trend_Yearly &lt;- data.frame(\n  Date = index(xts_Year) |&gt; as_date(),\n  Value = coredata(xts_Year) |&gt; as.numeric(),  # Yearly means\n  YearLin = trend_YearLin                      # Linear regression on yearly means\n)\n\n# Combine and plot all trend lines for comparison\ngp_trend &lt;- ggplot() +\n  geom_line(data = df_TS_Trend_Yearly, aes(x = Date, y = Value, color = \"Yearly mean\"), linewidth = 0.6) +\n  geom_line(data = df_TS_Trend_Daily, aes(x = Date, y = Loess100, color = \"LOESS (span = 1.0)\"), linewidth = 1) +\n  geom_line(data = df_TS_Trend_Daily, aes(x = Date, y = Loess50,  color = \"LOESS (span = 0.5)\"), linewidth = 1) +\n  geom_line(data = df_TS_Trend_Daily, aes(x = Date, y = Lin,      color = \"Linear (full)\"), linewidth = 1) +\n  geom_line(data = df_TS_Trend_Yearly, aes(x = Date, y = YearLin, color = \"Linear (yearly)\"), linewidth = 1) +\n  scale_color_manual(\n    name = \"Trend Type\",\n    values = c(\n      \"Yearly mean\"       = \"gray80\",\n      \"LOESS (span = 1.0)\" = \"#1b9e77\",\n      \"LOESS (span = 0.5)\" = \"#7570b3\",\n      \"Linear (full)\"      = \"#d95f02\",\n      \"Linear (yearly)\"    = \"#e7298a\"\n    )\n  ) +\n  labs(\n    x = \"Date\",\n    y = \"Temperature (°C)\",\n    title = \"Comparison of Trend Estimation Methods\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    legend.title = element_blank()\n  )\n\n# Convert static ggplot to interactive plotly visualization\nggplotly(gp_trend)\n\n\n\n\n\n\nThe smoother LOESS curves (with different spans) show local variations, while the linear fits highlight long-term directional changes. The yearly mean trend provides an aggregated perspective, reducing short-term variability.\nHere’s an improved and polished version of your text and code section with clearer explanations, smoother academic phrasing, and well-structured code comments suitable for a Quarto/R Markdown document:",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#t-test-and-anova",
    "href": "dataprocess/timeserises_analyse.html#t-test-and-anova",
    "title": "Time Series Analyse",
    "section": "3.1 t-test and ANOVA",
    "text": "3.1 t-test and ANOVA\nTo investigate differences between groups, we can apply t-tests and Analysis of Variance (ANOVA):\n\nThe t-test is used when comparing the means of two groups, such as upstream vs. downstream stations.\nThe ANOVA test generalizes this to three or more groups, for example when comparing data across multiple years or months.\n\nBoth tests assess whether the group means differ significantly from each other. To interpret their results, we consider the hypotheses and the resulting p-value:\n\nNull hypothesis (H₀): All group means are equal (no significant difference).\nAlternative hypothesis (H₁): At least one group mean is different.\n\nInterpretation:\n\nA large p-value (typically &gt; 0.05) → Fail to reject H₀ → No significant difference between groups.\nA small p-value (&lt; 0.05) → Reject H₀ → Significant difference exists between groups, suggesting that group characteristics (e.g., mean temperature) vary meaningfully.\n\nWhen using these tests, it is also good practice to check homogeneity of variances (e.g., with the Levene test), since standard ANOVA assumes that all groups have similar variance.",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#levenes-test-for-equal-variances",
    "href": "dataprocess/timeserises_analyse.html#levenes-test-for-equal-variances",
    "title": "Time Series Analyse",
    "section": "3.2 Levene’s Test for Equal Variances",
    "text": "3.2 Levene’s Test for Equal Variances\nThe Levene Test is a robust and widely used method to assess the homogeneity of variance among groups.\n\nNull hypothesis (H₀): All group variances are equal.\nAlternative hypothesis (H₁): At least one group has a different variance.\n\nInterpretation:\n\nA large p-value (typically &gt; 0.05) → Fail to reject H₀ → Variances are not significantly different.\nA small p-value (&lt; 0.05) → Reject H₀ → Variances are significantly different, and equal-variance methods (like standard ANOVA) should be applied cautiously.",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#example-with-different-grouping",
    "href": "dataprocess/timeserises_analyse.html#example-with-different-grouping",
    "title": "Time Series Analyse",
    "section": "3.3 Example with different Grouping",
    "text": "3.3 Example with different Grouping\n\n3.3.1 Example: Annual Grouping\nWe first test whether yearly data show significant mean or variance differences.\n\n# Group data by year\ngroup_Homo_Year &lt;- factor(format(index(xts_Component), \"%Y\"))\n\n# One-way ANOVA: check for mean differences between years\nsummary(aov(xts_Component ~ group_Homo_Year))\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ngroup_Homo_Year   19   3815   200.8   4.472 2.99e-10 ***\nResiduals       7285 327107    44.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Levene’s test: check for homogeneity of variances across years\nleveneTest(y = xts_Component |&gt; as.numeric(), group = group_Homo_Year)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup   19  7.2229 &lt; 2.2e-16 ***\n      7285                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.3.2 Example: Two Time Periods (Decades)\nWe can also split the dataset into two large periods (e.g., first vs. second half) and test for differences.\n\n# Divide dataset into two halves\nn_Data &lt;- nrow(xts_Component)\ngroup_Homo_Decade &lt;- factor(ifelse(1:n_Data &lt;= n_Data/2, \"FirstHalf\", \"SecondHalf\"))\n\n# t-test: compare mean values between the two halves\nt.test(xts_Component ~ group_Homo_Decade)\n\n\n    Welch Two Sample t-test\n\ndata:  xts_Component by group_Homo_Decade\nt = -5.258, df = 7298.9, p-value = 1.498e-07\nalternative hypothesis: true difference in means between group FirstHalf and group SecondHalf is not equal to 0\n95 percent confidence interval:\n -1.1348668 -0.5184657\nsample estimates:\n mean in group FirstHalf mean in group SecondHalf \n                10.98817                 11.81484 \n\n# Levene’s test: compare variances between the two halves\nleveneTest(y = xts_Component |&gt; as.numeric(), group = group_Homo_Decade)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value Pr(&gt;F)\ngroup    1  0.6557 0.4181\n      7303               \n\n\n\n\n3.3.3 Example: Random Two-Group Test\nTo check how the tests behave under random grouping (without actual structure), we can randomly assign each observation to one of two groups.\n\nset.seed(666)\n\n# Randomly assign data to two groups\ngroup_Homo_Random &lt;- sample(1:2, size = n_Data, replace = TRUE, prob = rep(1/2, 2)) |&gt; factor()\n\n# Apply t-test and Levene’s test\nt.test(xts_Component ~ group_Homo_Random)\n\n\n    Welch Two Sample t-test\n\ndata:  xts_Component by group_Homo_Random\nt = -2.0798, df = 7302.5, p-value = 0.03758\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.63620769 -0.01881909\nsample estimates:\nmean in group 1 mean in group 2 \n       11.23827        11.56579 \n\nleveneTest(y = xts_Component |&gt; as.numeric(), group = group_Homo_Random)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value Pr(&gt;F)\ngroup    1       0 0.9974\n      7303               \n\n\n\n\n3.3.4 Example: Random Multi-Group Test (ANOVA)\nFinally, we create five random groups to illustrate multi-group testing.\n\nset.seed(2025)\n\n# Randomly assign data to five groups\ngroup_Homo_Random &lt;- sample(1:5, size = n_Data, replace = TRUE, prob = rep(1/5, 5)) |&gt; factor()\n\n# ANOVA: compare means among five random groups\nsummary(aov(xts_Component ~ group_Homo_Random))\n\n                    Df Sum Sq Mean Sq F value Pr(&gt;F)\ngroup_Homo_Random    4     89   22.34   0.493  0.741\nResiduals         7300 330833   45.32               \n\n# Levene’s test: check variance homogeneity among groups\nleveneTest(y = xts_Component |&gt; as.numeric(), group = group_Homo_Random)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value Pr(&gt;F)\ngroup    4  0.0849 0.9871\n      7300               \n\n\nWhile random groupings typically show no meaningful differences, structured groupings (e.g., by year or region) can highlight systematic trends or changes in variance, which may indicate shifts in climate, instrumentation, or data quality.\nIn practical applications, especially when preparing data for machine learning or statistical modeling, it is often necessary to divide the dataset into separate parts (e.g., training and testing). Before doing so, we can check whether these parts are homogeneous in their statistical properties. The following example divides the dataset into two halves and tests whether their means and variances differ significantly:\nWhen dividing a dataset into training and testing subsets (as commonly done in machine learning), it is important to ensure that both subsets are statistically homogeneous. If the two parts of the dataset differ significantly in their mean or variance, the trained model may not generalize well to unseen data — a problem known as data leakage or sampling bias.\nBy performing homogeneity tests (such as the t-test or Levene’s test) before splitting or after sampling, we can verify that both parts of the dataset come from similar distributions. This ensures that the model learns general patterns rather than artifacts caused by unequal group characteristics.",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#seasonality-based-on-daily-data-with-different-resolutions",
    "href": "dataprocess/timeserises_analyse.html#seasonality-based-on-daily-data-with-different-resolutions",
    "title": "Time Series Analyse",
    "section": "4.1 Seasonality based on daily data with different resolutions",
    "text": "4.1 Seasonality based on daily data with different resolutions\nIn this part, we remove the long-term trend and extract the repeating seasonal components at different temporal resolutions (daily, weekly, and monthly).\n\n# Remove the long-term LOESS trend to isolate seasonal fluctuations\nxts_Detrend &lt;- xts_Component - trend_Loess100\n\n# Aggregate to monthly and weekly mean values for coarser seasonal patterns\nxts_Detrend_Month &lt;- apply.monthly(xts_Detrend, colMeans)\nxts_Detrend_Week &lt;- apply.weekly(xts_Detrend, colMeans)\n\n# Extract numeric and temporal indices\nnum_Detrend &lt;- as.numeric(xts_Detrend)\ntime_index &lt;- index(xts_Component)\nseason_Day_Index &lt;- as.numeric(format(time_index, \"%j\"))   # day of year (1–365)\nseason_Week_Index &lt;- as.numeric(format(time_index, \"%V\"))  # week number (1–53)\nseason_Month_Index &lt;- as.numeric(format(time_index, \"%m\")) # month (1–12)\n\n# Compute mean seasonal cycles for each resolution\nseason_Day &lt;- ave(num_Detrend, season_Day_Index, FUN = mean, na.rm = TRUE)\nseason_Week &lt;- ave(num_Detrend, season_Week_Index, FUN = mean, na.rm = TRUE)\nseason_Month &lt;- ave(num_Detrend, season_Month_Index, FUN = mean, na.rm = TRUE)\n\nThe resulting seasonal cycles help visualize how the detrended temperature data fluctuate across different time scales.\n\n\nCode\n# Combine daily, weekly, and monthly seasonal signals into one data frame\n\ndf_Season &lt;- data.frame(\nDate = time_index,\nDay = season_Day,\nWeek = season_Week,\nMonth = season_Month\n) |&gt; filter(Date &gt;= as.Date(\"2023-01-01\"), Date &lt;= as.Date(\"2024-12-31\"))\n\n# Reshape to long format for ggplot visualization\n\ndf_Season_Long &lt;- df_Season |&gt;\nselect(Date, Day, Week, Month) |&gt;\npivot_longer(cols = c(Day, Week, Month), names_to = \"Seasonality\", values_to = \"Value\")\n\n# Plot different seasonal resolutions over time\n\ngp_Season1 &lt;- ggplot(df_Season_Long, aes(x = Date, y = Value, color = Seasonality)) +\ngeom_line() +\nlabs(\ntitle = \"Seasonality Component (2023–2024)\",\nx = \"Date\",\ny = \"Temperature (°C)\",\ncolor = \"\"\n) +\ntheme(\nlegend.position = \"top\",\nlegend.title = element_blank()\n)\n\nggplotly(gp_Season1)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#seasonality-with-smoothing",
    "href": "dataprocess/timeserises_analyse.html#seasonality-with-smoothing",
    "title": "Time Series Analyse",
    "section": "4.2 Seasonality with smoothing",
    "text": "4.2 Seasonality with smoothing\nShort-term noise can obscure seasonal patterns. To clarify them, a rolling mean (7-day window) is applied to smooth the daily seasonality.\n\n# Convert to xts and apply 7-day rolling mean to smooth short-term variability\nxts_Season &lt;- as.xts(df_Season)\nxts_Season_Roll &lt;- rollmean(xts_Season, 7)\ndf_Season_Roll &lt;- as.data.frame(xts_Season_Roll)\ndf_Season_Roll$Date &lt;- index(xts_Season_Roll)\n\n\n\nCode\n# Reshape smoothed seasonal data for plotting\n\ndf_Season_Roll_Long &lt;- df_Season_Roll |&gt;\nselect(Date, Day, Week, Month) |&gt;\npivot_longer(cols = c(Day, Week, Month), names_to = \"Season_Rollality\", values_to = \"Value\")\n\n# Plot smoothed (rolling mean) seasonal components\n\ngp_Season2 &lt;- ggplot(df_Season_Roll_Long, aes(x = Date, y = Value, color = Season_Rollality)) +\ngeom_line() +\nlabs(\ntitle = \"Season_Rollality Component (2023–2024)\",\nx = \"Date\",\ny = \"Temperature (°C)\",\ncolor = \"\"\n) +\ntheme(\nlegend.position = \"top\",\nlegend.title = element_blank()\n)\nggplotly(gp_Season2)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#seasonality-based-on-different-resolutions",
    "href": "dataprocess/timeserises_analyse.html#seasonality-based-on-different-resolutions",
    "title": "Time Series Analyse",
    "section": "4.3 Seasonality based on different resolutions",
    "text": "4.3 Seasonality based on different resolutions\nFinally, to compare the influence of aggregation levels, we compute and visualize seasonal cycles derived from monthly and weekly means. Aggregating data changes the apparent smoothness and can affect the detectability of seasonal components.\n\n# Aggregate to monthly and weekly means\nxts_Detrend_Month &lt;- apply.monthly(xts_Detrend, colMeans, na.rm = TRUE)\nxts_Detrend_Week  &lt;- apply.weekly(xts_Detrend, colMeans, na.rm = TRUE)\n\n# Adjust indices to represent middle of month or week\nindex(xts_Detrend_Month) &lt;- as.Date(format(index(xts_Detrend_Month), \"%Y-%m-15\"))\nindex(xts_Detrend_Week) &lt;- index(xts_Detrend_Week) - 3  # approximate mid-week\n\n# Compute mean seasonality at each resolution\nidx_Month_Middle &lt;- as.numeric(format(index(xts_Detrend_Month), \"%m\"))\nidx_Week_Middle  &lt;- as.numeric(format(index(xts_Detrend_Week), \"%V\"))\n\nseason_MonthMean &lt;- ave(as.numeric(coredata(xts_Detrend_Month)), idx_Month_Middle, FUN = mean, na.rm = TRUE)\nseason_WeekMean  &lt;- ave(as.numeric(coredata(xts_Detrend_Week)), idx_Week_Middle, FUN = mean, na.rm = TRUE)\n\nThese comparisons illustrate how temporal resolution affects the perception of periodic patterns. Monthly averages emphasize long-term seasonal cycles, while weekly data preserve more short-term variations within the broader annual trend.\n\n\nCode\n# Combine and visualize monthly vs weekly seasonality\n\ndf_PeriodMean &lt;- rbind(\ndata.frame(Date = index(xts_Detrend_Month), Seasonality = season_MonthMean, Type = \"Monthly\"),\ndata.frame(Date = index(xts_Detrend_Week),  Seasonality = season_WeekMean,  Type = \"Weekly\")\n) |&gt; filter(Date &gt;= as.Date(\"2023-01-01\"), Date &lt;= as.Date(\"2024-12-31\"))\n\ngp_Season3 &lt;- ggplot(df_PeriodMean, aes(x = Date, y = Seasonality, color = Type)) +\ngeom_line() +\nlabs(\ntitle = \"Weekly and Monthly Seasonality (2023–2024)\",\nx = \"Date\",\ny = \"Temperature (°C)\",\ncolor = \"\"\n) +\ntheme(\nlegend.position = \"top\",\nlegend.title = element_blank()\n)\nggplotly(gp_Season3)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#autocorrelation-function-acf",
    "href": "dataprocess/timeserises_analyse.html#autocorrelation-function-acf",
    "title": "Time Series Analyse",
    "section": "5.1 Autocorrelation Function (ACF)",
    "text": "5.1 Autocorrelation Function (ACF)\nThe Autocorrelation Function (ACF) shows the correlation between a time series and its lagged values.\nMathematically, for a time series \\(x_t\\) of length \\(n\\), the autocorrelation at lag \\(k\\) is defined as:\n\\[\n\\rho_k = \\frac{\\text{Cov}(x_t, x_{t+k})}{\\text{Var}(x_t)} = \\frac{\\sum_{t=1}^{n-k} (x_t - \\bar{x})(x_{t+k} - \\bar{x})}{\\sum_{t=1}^{n} (x_t - \\bar{x})^2}\n\\]\nwhere \\(\\bar{x}\\) is the mean of the series.\nHow to read the ACF plot:\n\nThe x-axis shows the lag (number of time steps).\n\nThe y-axis shows the correlation at each lag.\n\nThe horizontal dashed lines indicate the 95% confidence interval (\\(\\frac{1.96}{\\sqrt{n}}\\)). Values outside these lines are usually considered statistically significant autocorrelations.\n\nA slowly decaying ACF suggests a strong trend or persistent correlation.\n\nA spike at a specific lag may indicate seasonality or periodic patterns.\n\nIf the ACF drops quickly to zero, the series is likely random or has weak temporal dependence.\n\n\nnum_ACF &lt;- acf(coredata(xts_Component), 400, plot = FALSE)\ndf_ACF &lt;- with(num_ACF, data.frame(lag = lag, acf = acf))\ngp_ACF &lt;- ggplot(df_ACF, aes(x = lag, y = acf)) +\n  geom_segment(aes(x = lag, xend = lag, y = 0, yend = acf), color = \"#17365c\") +\n  geom_hline(yintercept = 0, color = \"black\") +\n  geom_hline(yintercept = c(2, -2) / sqrt(length(xts_Component)), \n             linetype = \"dashed\", color = \"#EC008D\") +\n  labs(title = \"Autocorrelation Function (ACF)\",\n       x = \"Lag\",\n       y = \"ACF\")\nggplotly(gp_ACF)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#partial-autocorrelation-function-pacf",
    "href": "dataprocess/timeserises_analyse.html#partial-autocorrelation-function-pacf",
    "title": "Time Series Analyse",
    "section": "5.2 Partial Autocorrelation Function (PACF)",
    "text": "5.2 Partial Autocorrelation Function (PACF)\nThe Partial Autocorrelation Function (PACF) measures the correlation between \\(x_t\\) and \\(x_{t+k}\\) after removing the effects of all intermediate lags \\(1, 2, ..., k-1\\).\nThe Partial Autocorrelation Function (PACF) is similar to the ACF but shows the correlation between the series and its lagged values after removing the effect of shorter lags.\nIn other words, the PACF tells us the direct influence of a specific lag on the current value.\nThe PACF plot is very useful to identify the order of autoregressive (AR) processes in time series models.\nFor example, if the PACF cuts off after lag 1, it indicates that an AR(1) model might describe the data well.\nTogether with the ACF, the PACF gives a complete picture of the temporal dependence structure of the dataset.\nHow to read the PACF plot:\n\nThe x-axis shows the lag, and the y-axis shows the partial correlation.\n\nThe horizontal dashed lines indicate the 95% confidence interval. Values outside are statistically significant.\n\nA sharp cutoff after lag \\(p\\) indicates an AR(\\(p\\)) process.\n\nSignificant spikes at multiple lags may suggest the series requires a more complex model, such as ARMA.\n\nPACF helps distinguish the direct influence of a lag from indirect effects through shorter lags.\n\nBy examining both the ACF and PACF plots together, we can identify trends, seasonality, and the appropriate order of autoregressive and moving average models for time series analysis.\n\nnum_PACF &lt;- pacf(coredata(xts_Component), 400, plot = FALSE)\ndf_PACF &lt;- with(num_PACF, data.frame(lag = lag, acf = acf))\ngp_PACF &lt;- ggplot(df_PACF, aes(x = lag, y = acf)) +\n  geom_segment(aes(x = lag, xend = lag, y = 0, yend = acf), color = \"#17365c\") +\n  geom_hline(yintercept = 0, color = \"black\") +\n  geom_hline(yintercept = c(2, -2) / sqrt(length(xts_Component)), \n             linetype = \"dashed\", color = \"#EC008D\") +\n  labs(title = \"Autocorrelation Function (ACF)\",\n       x = \"Lag\",\n       y = \"PACF\")\nggplotly(gp_PACF)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html",
    "href": "dataprocess/timeserises_visual.html",
    "title": "Visualization",
    "section": "",
    "text": "Visualizing time series is crucial for identifying patterns, trends, and anomalies in data over time. Here are some key considerations and methods for visualizing time series data.\nIn this Artikel we will use the R package ggplot (tidyverse) for plotig and the results data fro HBV Light as the data:\n\n# Library\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(ggh4x) # difference area\nlibrary(reshape2)\n\n# File name\nfn_ResultsHBV &lt;- \"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/tbl_HBV_Results.txt\"\n# Load Data\ndf_ResultHBV &lt;- read_table(fn_ResultsHBV)\n\n# Convert Date column to a Date type\ndf_ResultHBV$Date &lt;- as_date(df_ResultHBV$Date |&gt; as.character(), format = \"%Y%m%d\")\n\nidx_1979 &lt;- which(df_ResultHBV$Date &gt;= as_date(\"1979-01-01\") & df_ResultHBV$Date &lt;= as_date(\"1979-12-31\"))\ndf_Plot &lt;- df_ResultHBV[idx_1979, ]",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#basic-line",
    "href": "dataprocess/timeserises_visual.html#basic-line",
    "title": "Visualization",
    "section": "2.1 Basic Line",
    "text": "2.1 Basic Line\n\nggplot(df_Plot, aes(x = Date)) +\n  geom_line(aes(y = Qobs, color = \"Obs.\")) +\n  geom_line(aes(y = Qsim, color = \"Sim.\"))",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#line-plot-with-shaded-difference-area",
    "href": "dataprocess/timeserises_visual.html#line-plot-with-shaded-difference-area",
    "title": "Visualization",
    "section": "2.2 Line Plot with Shaded Difference Area",
    "text": "2.2 Line Plot with Shaded Difference Area\n\nggplot(df_Plot, aes(x = Date)) +\n  geom_line(aes(y = Qobs, color = \"Obs.\")) +\n  geom_line(aes(y = Qsim, color = \"Sim.\")) +\n  stat_difference(aes(ymin = Qsim, ymax = Qobs), alpha = .5)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#line-cluster",
    "href": "dataprocess/timeserises_visual.html#line-cluster",
    "title": "Visualization",
    "section": "2.3 Line Cluster",
    "text": "2.3 Line Cluster\n\n# Melting the data for ggplot\ndf_Plot_Melt &lt;- reshape2::melt(df_Plot[,c(\"Date\", \"Qsim\", \"Precipitation\", \"AET\")], id = \"Date\")\n# Plot\nggplot(df_Plot_Melt, aes(x = Date, y = value, color = variable, group = variable)) +\n  geom_line()",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#basic-bar",
    "href": "dataprocess/timeserises_visual.html#basic-bar",
    "title": "Visualization",
    "section": "3.1 Basic Bar",
    "text": "3.1 Basic Bar\n\nggplot(df_Plot, aes(x = Date)) +\n  geom_col(aes(y = Precipitation))",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#stacked-bar",
    "href": "dataprocess/timeserises_visual.html#stacked-bar",
    "title": "Visualization",
    "section": "3.2 Stacked Bar",
    "text": "3.2 Stacked Bar\n\n# Snowfall and rain fall caculate\ndf_Plot &lt;- df_Plot |&gt; mutate(Snowfall = pmax(0, c(0, diff(Snow))), \n                             Rainfall = Precipitation - Snowfall) \ndf_Plot_Melt2 &lt;- reshape2::melt(df_Plot[1:120, c(\"Date\", \"Snowfall\", \"Rainfall\")], id = \"Date\")\n# Plot\nggplot(df_Plot_Melt2, aes(x = Date, y = value, fill = variable, group = variable)) +\n  geom_col(position=\"stack\")",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#dodge",
    "href": "dataprocess/timeserises_visual.html#dodge",
    "title": "Visualization",
    "section": "3.3 Dodge",
    "text": "3.3 Dodge\n\n# Plot\nggplot(df_Plot_Melt2, aes(x = Date, y = value, fill = variable, group = variable)) +\n  geom_col(position=\"dodge\")",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#stacked-area",
    "href": "dataprocess/timeserises_visual.html#stacked-area",
    "title": "Visualization",
    "section": "6.1 Stacked Area",
    "text": "6.1 Stacked Area\nA stacked area plot is a type of data visualization that displays the cumulative contribution of different groups to a total over time or another continuous variable. Each group’s contribution is represented as a colored area, and these areas are stacked on top of each other.\n\n# Data\nmelt_Balance_Q &lt;- df_Plot[,c(\"Date\", \"AET\", \"Q0\", \"Q1\", \"Q2\" )] |&gt; melt(id = \"Date\")\n# Plot\nggplot(melt_Balance_Q, aes(Date, value, fill = variable)) +\n  geom_area()",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_visual.html#percent-area",
    "href": "dataprocess/timeserises_visual.html#percent-area",
    "title": "Visualization",
    "section": "6.2 Percent Area",
    "text": "6.2 Percent Area\nA percent stacked area plot is a variation of the stacked area plot where the y-axis represents percentages, showcasing the proportion of each group relative to the total at each point in time. This type of plot is particularly useful when you want to emphasize the relative distribution of different groups over time.\n\n# Data\nmelt_Balance_Q$perc &lt;- melt_Balance_Q$value / rowSums(df_Plot[,c(\"AET\", \"Q0\", \"Q1\", \"Q2\" )])\n# PLot\nggplot(melt_Balance_Q, aes(Date, perc, fill = variable)) +\n  geom_area()",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Visualization"
    ]
  },
  {
    "objectID": "dataprocess/visual_plotElements.html",
    "href": "dataprocess/visual_plotElements.html",
    "title": "The elements of a plot",
    "section": "",
    "text": "At the beginning of plotting, we can first examine the elements of a plot that provide an overview. These include the background, panel, title, and axes. While they may not be as visually prominent as the main plot elements (points or lines), they are crucial for extracting meaningful information from a plot.\nIn this section, we will leverage concepts from ggplot2. These concepts are fundamental not only to ggplot2 but also to other plotting engines. This article will primarily focus on the section on Theme Elements in the book “ggplot2: Elegant Graphics for Data Analysis (3e)” by Hadley Wickham (2009).\nUnder ggplot2, these elements are divided into five groups: plot, axis, legend, panel, and facet. However, for a single plot, we will not delve into the fifth facet group.\n\nTo illustrate these elements, we will use a random dataset to create a scatter plot as an example.\n\n\n\n\n\n\nImportant\n\n\n\nThis article will focus on illustrating the elements, while the techniques for manipulating and changing the content of these elements will be covered in a separate article. With this article, you can get a basic impression of the plot.\n\n\n\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Create a sample dataset\ndata &lt;- data.frame(\n  x = rnorm(50),\n  y = rnorm(50),\n  group = rep(c(\"A\", \"B\"), each = 25)\n)\n\nThe original plot appears as follows:\n\ngp_Test &lt;- ggplot(data, aes(x, y, color = group)) +\n  geom_point()\n\ngp_Test\n\n\n\n\nOriginal\n\n\n\n\n\n1 Plot elements\nThe “plot” represents the entire plot, basically, defining the background on which all other elements are drawn. There are three main elements related to the plot:\n\nplot.background: background rectangle area\nplot.title: title for the whole plot\nplot.margin: margins around the plot\n\n\nplot.backgroundplot.titleplot.margin\n\n\n\ngp_Test + \n  theme(plot.background = element_rect(fill = \"red\"))\n\n\n\n\nBackground in red\n\n\n\n\n\n\n\ngp_Test + \n  ggtitle(\"Title in red\") +\n  theme(plot.title = element_text(color = \"red\"))\n\n\n\n\nTitle in red\n\n\n\n\n\n\n\ngp_Test + \n  theme(plot.background = element_rect(fill = \"red\"),\n        plot.margin = margin(10,10,10,10, \"mm\"))\n\n\n\n\nBackground in red and margin in 10 mm\n\n\n\n\n\n\n\n\n\n2 Axis elements\nThe “axis” in a plot provides a crucial reference for interpreting the data or a scale for measurement. It consists of tick marks, labels, and a title. The axis allows viewers to understand the quantitative values represented in the plot, aiding in data analysis and visualization.\nThere are four main elements related to the axis:\n\naxis.line: line parallel to axis\naxis.text: tick labels (axis.text.x, axis.text.y)\naxis.title: axis titles (axis.title.x, axis.title.y)\naxis.ticks: axis tick marks\n\naxis.ticks.length: length of tick marks\n\n\n\naxis.lineaxis.textaxis.titleaxis.ticks\n\n\n\ngp_Test + \n  theme(axis.line = element_line(color = \"red\", linewidth = 2))\n\n\n\n\nAxis line in red, line width in 2\n\n\n\n\n\n\n\ngp_Test + \n  theme(axis.text = element_text(color = \"red\", size = 15))\n\n\n\n\nTick labels in red and font size in 15\n\n\n\n\n\n\n\ngp_Test + \n  theme(axis.title = element_text(color = \"red\", size = 15))\n\n\n\n\nAxis titles in red and font size in 15\n\n\n\n\n\n\n\ngp_Test + \n  theme(axis.ticks = element_line(color = \"red\", linewidth = 2),\n        axis.ticks.length = unit(2, \"mm\"))\n\n\n\n\nAxis tick marks in red, line width in 2, length in 2 mm\n\n\n\n\n\n\n\n\n\n3 Legend elements\nThe “legend” elements control the appearance of all legends. You can also modify the appearance of individual legends by modifying the same elements in guide_legend() or guide_colourbar() (Wickham 2009).\nThere are four main elements related to the legend:\n\nlegend.background: legend background\n\nlegend.margin: legend margin\n\nlegend.key: background of legend keys\n\nlegend.key.size: legend key size\nlegend.key.height: legend key height\nlegend.key.width: legend key width\n\nlegend.text: legend labels\nlegend.title: legend name\n\n\nlegend.backgroundlegend.keylegend.textlegend.title\n\n\n\ngp_Test + \n  theme(legend.background = element_rect(fill = \"red\"),\n        legend.margin = margin(10,10,10,10, \"mm\"))\n\n\n\n\nLegend background in red and margin in 10 mm\n\n\n\n\n\n\n\ngp_Test + \n  theme(legend.key = element_rect(fill = \"red\"),\n        legend.key.size = unit(10, \"mm\"))\n\n\n\n\nBackground of legend keys in red and legend keys size in 10 mm\n\n\n\n\n\n\n\ngp_Test + \n  theme(legend.text = element_text(color = \"red\", size = 15))\n\n\n\n\nLegend labels in red and font size in 15\n\n\n\n\n\n\n\ngp_Test + \n  theme(legend.title = element_text(color = \"red\", size = 15))\n\n\n\n\nLegend name in red and font size in 15\n\n\n\n\n\n\n\n\n\n4 Panel elements\nThe “panel” in a plot is the central area where the main data representation.\nThere are four main elements related to the panel:\n\npanel.background: panel background (under data)\npanel.border: panel border (over data)\npanel.grid.major (panel.grid.minor): major / minor grid lines\naspect.ratio: plot aspect ratio\n\n\npanel.backgroundpanel.borderpanel.gridaspect.ratio\n\n\n\ngp_Test + \n  theme(panel.background = element_rect(fill = \"red\"))\n\n\n\n\nLegend background in red and margin in 10 mm\n\n\n\n\n\n\n\ngp_Test + \n  theme(panel.border = element_rect(color = \"red\", fill = NA, linewidth = 2))\n\n\n\n\nPanel border in red and line width in 2\n\n\n\n\nIf the fill parameter is not set to NA (transparent), it will cover the main plot:\n\ngp_Test + \n  theme(panel.border = element_rect(color = \"red\", fill = \"green\", linewidth = 2))\n\n\n\n\nPanel border in red and line width in 2, but the area fill in green\n\n\n\n\n\n\n\ngp_Test + \n  theme(panel.grid.major = element_line(color = \"red\", linewidth = 2))\n\n\n\n\nMajor grid lines in red and line width in 15\n\n\n\n\n\n\n\ngp_Test + \n  theme(aspect.ratio = 2)\n\n\n\n\nAspect ratio in 1\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nWickham, Hadley. 2009. Ggplot2: Elegant Graphics for Data Analysis. New York, NY: Springer. https://doi.org/10.1007/978-0-387-98141-3.",
    "crumbs": [
      "Dataprocess",
      "Visualization",
      "The elements of a plot"
    ]
  },
  {
    "objectID": "dataset/hydro.html",
    "href": "dataset/hydro.html",
    "title": "Hydro",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/hydro.html#h2",
    "href": "dataset/hydro.html#h2",
    "title": "Hydro",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/meteo.html",
    "href": "dataset/meteo.html",
    "title": "Meteological",
    "section": "",
    "text": "About this site test1\n\n1 + 1\n\n[1] 2\n\n\n\n\nAbout this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataset/meteo.html#h2",
    "href": "dataset/meteo.html#h2",
    "title": "Meteological",
    "section": "",
    "text": "About this site test2\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "modelling/basic_concept.html",
    "href": "modelling/basic_concept.html",
    "title": "Concept of Modelling",
    "section": "",
    "text": "Within the process of hydrological modeling, there are fundamental concepts that play a crucial role. In this article, we will elucidate these concepts through illustrative figures, enhancing your comprehension. These concepts can be categorized into two main parts: the Data part, which encompasses aspects related to input data, parameters, and parameter ranges, and the Process part, which outlines the workflow of the entire modeling process. By exploring these concepts and their visual representations, you’ll gain a deeper understanding of hydrological modeling.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#boundary-condition-forcing-data",
    "href": "modelling/basic_concept.html#boundary-condition-forcing-data",
    "title": "Concept of Modelling",
    "section": "1.1 Boundary Condition (Forcing Data)",
    "text": "1.1 Boundary Condition (Forcing Data)\nFor hydrological modeling, you need data that describes the boundary conditions or forcing factors affecting the model. They define how water and other related variables enter or exit the modeled domain. This includes information on precipitation, temperature, humidity, and other meteorological variables.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#initial-condition-data-warm-up-time",
    "href": "modelling/basic_concept.html#initial-condition-data-warm-up-time",
    "title": "Concept of Modelling",
    "section": "1.2 Initial Condition Data & Warm-up Time",
    "text": "1.2 Initial Condition Data & Warm-up Time\nInitial conditions represent the state of the watershed or catchment at the beginning of the simulation.\nHowever, in many cases, the exact initial conditions are unknown or difficult to measure accurately. To address this uncertainty, hydrological models incorporate a warm-up period. This warm-up period refers to the initial phase of model simulation where the model runs to establish a stable or equilibrium state before commencing the actual simulation.\nIt’s crucial to ensure that the warm-up period is of sufficient duration to reach a stable state. Typically, this period should span at least two complete cycles of the dominant hydrological processes within the watershed. This requirement ensures that the model has the opportunity to capture the full range of variability associated with these processes.\nThe periodicity of hydrological processes is a crucial consideration when implementing a warm-up period. By assuming that the initial state during warm-up has only a minimal influence, we rely on the repeated cycles of hydrological events. During the second cycle, the system tends to reach its maximum or minimum state, which is indicative of equilibrium. Subsequently, the simulation becomes stable, and this equilibrium state serves as the initial condition for the formal simulation.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#parameter",
    "href": "modelling/basic_concept.html#parameter",
    "title": "Concept of Modelling",
    "section": "1.3 Parameter",
    "text": "1.3 Parameter\nParameters are essential components of hydrological models for several reasons. Firstly, hydrological models cannot simulate the entire complexity of the real world, so parameters are used to represent various physical characteristics and processes. Secondly, real-world measurements are often limited in scope and may not capture all relevant data across the entire watershed. Parameters help bridge these gaps by allowing models to make predictions based on available information.\nThere are three main categories of hydrological models based on their use of parameters:\n\nPhysical-based models (white box): These models aim to simulate hydrological processes with a high level of detail. They rely on deterministic formulas and aim to represent physical processes as accurately as possible. Ideally, physical-based models do not require the use of parameters, as all processes are simulated with detailed data and deterministic equations. However, in practice, some level of parameterization may still be necessary, especially for processes that are not well understood or difficult to represent mathematically.\nConceptual models (gray box): Conceptual models are the most commonly used hydrological models. They strike a balance between complexity and simplicity. These models use parameters to represent various physical characteristics of the watershed, such as soil properties, land use, and drainage patterns. Calibration, a process of adjusting parameter values to match observed data, is typically needed to make the model’s predictions consistent with real-world conditions.\nEmpirical and data-driven models (black box): Empirical models rely heavily on observed data and may not explicitly represent physical processes. Instead, they use statistical relationships to make predictions. These models often require fewer parameters than physical-based or conceptual models but may still involve parameter estimation based on data.\n\n\n1.3.1 Initial Parameters\nInitial parameters are the starting values for model parameters that are often suggested based on previous research or prior knowledge. These initial parameter values are used to test the fundamental functionality of the model and its applicability to the study area. While these initial parameters may provide a reasonable starting point, they may or may not be a good fit for the specific study area and objectives.\n\n\n1.3.2 Parameter Range\nParameter ranges define the range of allowable values that model parameters can assume within specified bounds. While a broad parameter range can provide a greater opportunity to find optimal parameter values, it also expands the search space, making it more challenging to identify the best-fitting parameters.\nParameter ranges can be categorized into two main types:\n\nPhysical Range: This refers to the range of parameter values that are physically meaningful and are constrained by the fundamental characteristics of the system being modeled. For example, hydraulic conductivity in groundwater models cannot be negative, so it has a physical lower bound of zero. Physical range limits ensure that parameter values are consistent with the underlying physical processes.\nRegional Range: In addition to the physical limits, parameters may have regional variations based on the specific characteristics of the study area. These regional variations account for local geological, climatic, or land-use differences that influence parameter values. Regional ranges help to capture the heterogeneity within a larger study domain and allow for parameterization that reflects local conditions.\n\nBalancing the scope of parameter ranges is essential in hydrological modeling. While broader ranges offer flexibility, they also increase the complexity of parameter estimation. The challenge lies in finding a balance that allows for the exploration of diverse parameter values while ensuring that the model remains physically meaningful and regionally applicable.\n\n\n1.3.3 Calibrated Parameters\nCalibrated parameters are the parameter set that has been adjusted and fine-tuned during the calibration process. These parameters represent the current best-fit values for the hydrological model in a specific study area. They are chosen to optimize the model’s performance and ensure that it provides accurate predictions and simulations based on observed data.\n\n\n1.3.4 Validated Parameters\nValidated parameters are parameters that have been **verified* through comparison with observed data to ensure that the model accurately represents the real-world system.\n\n\n1.3.5 Parameter Mapping with Groups\nParameters in hydrological modeling vary based on the physical characteristics of the system being studied. However, in reality, different regions within a study area often exhibit distinct physical characteristics. Consequently, when performing simulations, it becomes necessary to calibrate parameters for each Hydrological Response Unit (HRU), which represents a homogeneous area within the study domain.\nCalibrating parameters for every HRU can be a formidable task, as it would involve a substantial number of individual calibrations. To address this challenge, researchers often employ a strategy of grouping HRUs with similar characteristics. By doing so, they can map parameters to these groups, effectively reducing the parameter calibration space.\nOne of the most commonly used grouping criteria includes categorizing HRUs based on factors such as land use, soil type, soil class, or climate zone. These characteristics often play a significant role in shaping the hydrological behavior of a region.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#model-running",
    "href": "modelling/basic_concept.html#model-running",
    "title": "Concept of Modelling",
    "section": "2.1 Model Running",
    "text": "2.1 Model Running\nThe core step in hydrological modeling involves running the model, which treats the model as a function that calculates a process with the input data to produce output data.\n\nThis process utilizes the provided boundary conditions, initial conditions, and parameter values to simulate the hydrological processes within the watershed or catchment. Boundary conditions and initial conditions are often collectively referred to as forcing data or input data.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#evaluation",
    "href": "modelling/basic_concept.html#evaluation",
    "title": "Concept of Modelling",
    "section": "2.2 Evaluation",
    "text": "2.2 Evaluation\nAfter the model run, an evaluation process is conducted to assess the performance of the model. This involves comparing the model’s simulated output to observed data or reference values. Various performance metrics and statistical measures are used to determine how well the model simulates real-world conditions.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#uncertainty-and-sensitivity",
    "href": "modelling/basic_concept.html#uncertainty-and-sensitivity",
    "title": "Concept of Modelling",
    "section": "2.3 Uncertainty and sensitivity",
    "text": "2.3 Uncertainty and sensitivity\nUncertainty and sensitivity analysis are crucial components of hydrological modeling, helping us understand the reliability of model predictions and the influence of different input parameters. Uncertainty analysis assesses the overall uncertainty in model outputs, considering various sources of uncertainty, while sensitivity analysis identifies which input parameters have the most significant impact on model results. Together, they provide valuable insights into the robustness and reliability of hydrological models.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#calibration",
    "href": "modelling/basic_concept.html#calibration",
    "title": "Concept of Modelling",
    "section": "2.4 Calibration",
    "text": "2.4 Calibration\nCalibration is a critical step in hydrological modeling. It involves adjusting the model’s parameters to improve its accuracy and alignment with observed data. Optimization techniques are often used to find parameter values that minimize the difference between model output and observed data.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/basic_concept.html#validation",
    "href": "modelling/basic_concept.html#validation",
    "title": "Concept of Modelling",
    "section": "2.5 Validation",
    "text": "2.5 Validation\nOnce the model has been calibrated, it is essential to validate its performance. Validation involves testing the calibrated model against independent datasets or data from a different time period. This step ensures that the model’s performance is not solely tailored to the calibration data but remains reliable for a broader range of conditions.",
    "crumbs": [
      "Modelling",
      "Concept of Modelling"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html",
    "href": "modelling/model_caliLinearReser.html",
    "title": "Calibration Prozess",
    "section": "",
    "text": "In this article, we will learn how to manage the entire hydrological modeling process (mehr Concept Details in Concept of Modelling) with the minimal model Linear-Reservoir model, including automatic calibration.",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#load-experimental-data",
    "href": "modelling/model_caliLinearReser.html#load-experimental-data",
    "title": "Calibration Prozess",
    "section": "4.1 Load Experimental Data",
    "text": "4.1 Load Experimental Data\nIn this phase, we will utilize experimental data from a labor experiment. This dataset involves the physical simulation of a linear reservoir and provides the measured inflow and outflow data in liters per second (L/h).\nThe labor data is available on GitHub.\n\n# Load Labor Data\ndf_Labor &lt;- read_delim(\"https://raw.githubusercontent.com/HydroSimul/Web/main/data_share/tbl_LaborMess_LinearReservior.txt\", delim = \"\\t\")\n\n# Rename the data, in order to more flexible Manipulation\nnames(df_Labor) &lt;- c(\"t\", \"QZ\", \"QA\")\n\n# The first 10 Line\nhead(df_Labor)\n\n# A tibble: 6 × 3\n      t    QZ    QA\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0    30\n2     1     0    30\n3     2     0    29\n4     3     0    29\n5     4     0    29\n6     5     0    29",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#mapping-from-concept-to-proceeding",
    "href": "modelling/model_caliLinearReser.html#mapping-from-concept-to-proceeding",
    "title": "Calibration Prozess",
    "section": "4.2 Mapping from Concept to Proceeding",
    "text": "4.2 Mapping from Concept to Proceeding\nTo provide clarity, let’s map the conceptual understanding to the simulation before proceeding:\n\n\n1.1 -&gt; Q_In = df_Labor$QZ\n1.2 -&gt; Q_Out0 = 0\n1.3 -&gt; param_K = 50\nf1 -&gt; model_linearReservoir()\n2 -&gt; df_Labor$QZ\nf2 -&gt; NSE(), KGE()",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#running-the-model-with-forcing-data",
    "href": "modelling/model_caliLinearReser.html#running-the-model-with-forcing-data",
    "title": "Calibration Prozess",
    "section": "4.3 Running the Model with Forcing Data",
    "text": "4.3 Running the Model with Forcing Data\nAs a preliminary test, we can suggest certain parameter values, such as \\(K\\) at 90 and 60. After the simulation, we will store the results in variables num_Q_Sim and num_Q_Sim2 for further analysis.\n\n# run the model\n\nnum_Q_Sim &lt;- model_linearReservoir(df_Labor$QZ, 0, param_K =  90)\n\nnum_Q_Sim2 &lt;- model_linearReservoir(df_Labor$QZ, 0, param_K =  60)",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#visual-evaluation",
    "href": "modelling/model_caliLinearReser.html#visual-evaluation",
    "title": "Calibration Prozess",
    "section": "4.4 Visual Evaluation",
    "text": "4.4 Visual Evaluation\nBefore employing quantitative criteria, it’s beneficial to visually evaluate the simulation results using time series plots, which provide an initial sense of the model’s performance.\n\n# Visual Evaluation\nggLabor &lt;- ggplot(df_Labor) +\n  geom_line(aes(t, QZ, color = \"Inflow\")) +\n  geom_line(aes(t, num_Q_Sim, color = \"Simul1\")) +\n  geom_line(aes(t, num_Q_Sim2, color = \"Simul2\")) +\n  geom_line(aes(t, QA, color = \"Observ\")) +\n  scale_color_manual(values = c(Inflow = \"cyan\", Simul1 = \"red\", Simul2 = \"orange\", Observ = \"blue\"))+\n  labs(x = \"Time [s]\", y = \"In-/Outflow [L/h]\", color = \"Flow\")\nggplotly(ggLabor)",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#quantitative-evaluation",
    "href": "modelling/model_caliLinearReser.html#quantitative-evaluation",
    "title": "Calibration Prozess",
    "section": "4.5 Quantitative Evaluation",
    "text": "4.5 Quantitative Evaluation\nFor short or simplified time series, visual evaluation may suffice. However, when dealing with long-term data, we require standardized criteria to objectively assess the model’s performance.\nNSE and KGE are the most commonly used criteria in hydrological research. However, there are also additional criteria available in the hydroGOF package (use ?hydroGOF):\nQuantitative statistics included are: Mean Error (me), Mean Absolute Error (mae), Root Mean Square Error (rms), Normalized Root Mean Square Error (nrms), Pearson product-moment correlation coefficient (r), Spearman Correlation coefficient (r.Spearman), Coefficient of Determination (R2), Ratio of Standard Deviations (rSD), Nash-Sutcliffe efficiency (NSE), Modified Nash-Sutcliffe efficiency (mNSE), Relative Nash-Sutcliffe efficiency (rNSE), Index of Agreement (d), Modified Index of Agreement (md), Relative Index of Agreement (rd), Coefficient of Persistence (cp), Percent Bias (pbias), Kling-Gupta efficiency (KGE), the coef. of determination multiplied by the slope of the linear regression between ‘sim’ and ‘obs’ (bR2), and volumetric efficiency (VE).\n\nNSE(num_Q_Sim, df_Labor$QA)\n\n[1] 0.9234607\n\nNSE(num_Q_Sim2, df_Labor$QA)\n\n[1] 0.8752109\n\nKGE(num_Q_Sim, df_Labor$QA)\n\n[1] 0.8425622\n\nKGE(num_Q_Sim2, df_Labor$QA)\n\n[1] 0.9273187",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#create-the-fit-function",
    "href": "modelling/model_caliLinearReser.html#create-the-fit-function",
    "title": "Calibration Prozess",
    "section": "5.1 Create the Fit Function",
    "text": "5.1 Create the Fit Function\nBefore proceeding with automatic calibration, an important step is to create a function that the calibration algorithm will use. This function should take the parameter to be calibrated as an input. Thus, we need to modify our model and evaluation function into a “Fit Function.”\n\neva_fit &lt;- function(model_Param,\n                     model_Input,\n                     Q_Observ,\n                     fct_gof = NSE) {\n  Q_Simu &lt;- model_linearReservoir(model_Input, param_K = model_Param)\n  \n  - fct_gof(Q_Simu, Q_Observ)\n  \n}\n\neva_fit(60, df_Labor$QZ, df_Labor$QA)\n\n[1] -0.8752109\n\n\nThere is another critical point to consider when creating the Fit Function. Calibration algorithms need to know which criteria is better. Most calibration algorithms compare the current criteria value with the previous one (or several previous ones) and consider the minimum (or maximum) criteria value as the best. However, in the case of NSE and KGE, a better simulation results in a higher value. To handle this, we should set these criteria as negative values. By doing so, calibration algorithms like cali_UVS() can work effectively with them.",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "modelling/model_caliLinearReser.html#calibrating",
    "href": "modelling/model_caliLinearReser.html#calibrating",
    "title": "Calibration Prozess",
    "section": "5.2 Calibrating",
    "text": "5.2 Calibrating\nWith the fit function in place, we can choose a calibration algorithm to optimize our model parameter (in this case, parameter \\(K\\)).\n\nlst_Cali &lt;- cali_UVS(eva_fit, x_Min = 40, x_Max = 90, model_Input = df_Labor$QZ, Q_Observ = df_Labor$QA, fct_gof = KGE)\n\n\n  |                                                                            \n  |                                                                      |   0%",
    "crumbs": [
      "Modelling",
      "Calibration Prozess"
    ]
  },
  {
    "objectID": "dataprocess/timeserises_analyse.html#dataset-description",
    "href": "dataprocess/timeserises_analyse.html#dataset-description",
    "title": "Time Series Analyse",
    "section": "",
    "text": "For demonstration purposes, we will use a synthetic dataset derived from daily temperature data recorded at the Düsseldorf station of the DWD, covering the period from 2005 to 2024.\nThe following R packages are required and will be loaded below:\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(plotly)\nlibrary(xts)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(car)\n\nxts_Component &lt;- read_csv(\"https://raw.githubusercontent.com/HydroSimul/Web/refs/heads/main/data_share/df_TS_Component.csv\") |&gt; as.xts()\n\nThe components of the dataset from 2020 to 2024 are shown as follows:\n\n# --- 1. Fit a loess trend to the time series\n# Use a span of 1 (full smoothing window) to estimate the trend\ntrend_Fit100 &lt;- stats::loess(coredata(xts_Component) ~ as.numeric(index(xts_Component)), span = 1)\n\n# Extract the fitted trend values\ntrend_Loess100 &lt;- trend_Fit100$fitted\nxts_Trend &lt;- xts(trend_Loess100, order.by = index(xts_Component))\n# Detrend the original series by subtracting the fitted trend\nxts_Detrend &lt;- xts_Component - trend_Loess100\n\n# Extract the time index from the xts object\ntime_index &lt;- index(xts_Component)\n\n# --- 2. Compute seasonal component based on day of the year\n# Convert dates to day-of-year (1–365/366)\nseason_Day_Index &lt;- as.numeric(format(time_index, \"%j\"))\n\n# Compute the mean of the detrended series for each day-of-year\n# This gives the seasonal effect for each day\nseason_Day &lt;- ave(xts_Detrend, season_Day_Index, FUN = mean, na.rm = TRUE)\n\n# Compute the residual component (remainder) after removing trend and seasonal effects\nremainder_Day &lt;- xts_Detrend - season_Day\n\n# --- 3. Combine results into a tidy data frame for plotting\ndf_TS_Plot &lt;- data.frame(\n  date = index(xts_Component[\"2020-01-01/2024-12-31\"]),             # time index\n  Original = xts_Component[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),# original time series\n  Trend = xts_Trend[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),  # extracted trend\n  Seasonal = season_Day[\"2020-01-01/2024-12-31\"] |&gt; as.numeric(),   # extracted seasonal component\n  Residual = remainder_Day[\"2020-01-01/2024-12-31\"] |&gt; as.numeric() # residual component\n) |&gt;\n  pivot_longer(\n    cols = c(Original, Trend, Seasonal, Residual), # pivot components into long format\n    names_to = \"Component\",                        # column indicating component type\n    values_to = \"Value\"                            # column containing values\n  )\n\n# --- 4. Plot the time series decomposition using ggplot2\ngp_TS &lt;- ggplot(df_TS_Plot, aes(x = date, y = Value)) +\n  geom_line(aes(color = Component)) +                # plot lines colored by component\n  facet_wrap(~Component, ncol = 1, scales = \"free_y\") + # separate facet for each component, free y-scale\n  labs(\n    x = NULL, \n    y = \"Value\", \n    title = \"Time Series Decomposition\"\n  ) +\n  theme(\n    strip.text = element_text(face = \"bold\", size = 12), # bold facet labels\n    panel.grid.minor = element_blank()                  # remove minor grid lines\n  )\n\n# Convert ggplot object to interactive plotly plot\nggplotly(gp_TS)",
    "crumbs": [
      "Dataprocess",
      "Timeserises",
      "Time Series Analyse"
    ]
  }
]