---
title: ML Workflow with `tidymodels`
execute:
  warning: false
  error: false
sidebar:
  contents: auto
number-sections: true
---



# Libraries

In this article we use the `tidymodels` ecosystem to illustrate a complete machine learning workflow in a tidy, coherent, and reproducible manner.  
Throughout the article, we revisit the fundamental components of the ML workflow:

1. Data split 
2. Feature engineering  
3. Model building  
4. Model fitting 
5. Model prediction 
6. Model evaluation

We begin with the most basic workflow and later extend it using the `recipe` framework for more advanced preprocessing.

The material in this article is based on the case study provided at: [https://www.tidymodels.org/start/case-study/](https://www.tidymodels.org/start/case-study/).  All exercises use the dataset included in that example.

The following packages are required:

```{r}
library(tidyverse)
theme_set(theme_bw())

library(tidymodels)  # parsnip + other tidymodels packages

# Helper packages
library(readr)       # for importing data
library(broom.mixed) # for tidying Bayesian model output
library(dotwhisker)  # for visualizing regression results
library(skimr)       # for variable summaries
```




# Basic Model Building and Fitting

In the first case, we will focus on three steps:

1. **Exploring the data** – Understanding the structure, distributions, and relationships within the dataset to inform subsequent modeling decisions.  
2. **Specifying a model** – Selecting an appropriate algorithm and defining its parameters based on the task and data characteristics.  
3. **Fitting the model to the data** – Training the model using the available data to estimate its parameters and capture underlying patterns.

## Data

In this exercise we explore the relationship between:

* the outcome variable: **sea urchin width**
* feature 1: **initial volume** (continuous)
* feature 2: **food regime** (categorical)

A basic model for these variables can be written as:

```r
width ~ initial_volume * food_regime
```

```{r}
## Data import ----------
df_Urchins <-
  read_csv("https://tidymodels.org/start/models/urchins.csv") |>
  setNames(c("food_regime", "initial_volume", "width")) |>
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))

## Explore the relationships ----------
ggplot(df_Urchins,
       aes(x = initial_volume,
           y = width,
           group = food_regime,
           col = food_regime)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)
```



## Building the Model

We start with a simple linear regression model using `linear_reg()`.
The `tidymodels` ecosystem separates *model specification* (what model we want) from the *computational engine* (how it is estimated).
The `parsnip` package handles this abstraction.

Available engines for `linear_reg()` are listed here:
[https://parsnip.tidymodels.org/reference/linear_reg.html](https://parsnip.tidymodels.org/reference/linear_reg.html)

1. Choose the model type:

```{r}
linear_reg()
```

2. Choose the engine:

```{r}
linear_reg() |>
  set_engine("lm")
```

3. Save the model specification:

```{r}
mdl_Linear <- linear_reg()
```



## Training (Fitting) the Model

To train the model, we use the `fit()` function, which requires:

1. the model specification
2. a formula
3. the training dataset

```{r}
fit_Linear <-
  mdl_Linear |>
  fit(width ~ initial_volume * food_regime, data = df_Urchins)

fit_Linear
```

We can inspect the fitted model coefficients:

```{r}
tidy(fit_Linear)
```



## Predicting with the Fitted Model

Once a model is fitted, we can use it for prediction.
To make predictions we need:

1. a dataset containing the feature variables (with matching names)
2. the fitted model object

By default, `predict()` returns the mean prediction:

```{r}
df_NewData <- expand.grid(
  initial_volume = 20,
  food_regime = c("Initial", "Low", "High")
)

pred_Mean <- predict(fit_Linear, new_data = df_NewData)
pred_Mean
```

We can also request confidence intervals using `type = "conf_int"`:

```{r}
pred_Conf <- predict(
  fit_Linear,
  new_data = df_NewData,
  type = "conf_int"
)
pred_Conf
```




# Data Processing with `recipes`

The `recipes` package provides a structured and extensible framework for data preprocessing and feature engineering within the `tidymodels` ecosystem.  
It allows us to define all preprocessing steps in a reproducible, model-agnostic workflow before model fitting.

In this exercise, we will focus on the following steps:

1. **Data splitting** – Dividing the dataset into training, validation, and testing sets.  
2. **Recipe defining**  
   2.1 Basic definition – Specifying the target variable, predictors, and initial preprocessing steps.  
   2.2 Feature engineering – Applying transformations, scaling, encoding, and other preprocessing techniques to enhance model performance.  
3. **Model building** – Selecting and configuring the appropriate algorithm for the task.  
4. **Combining recipe and model into a workflow** – Integrating preprocessing steps with the model into a unified pipeline.  
5. **Model fitting** – Training the model on the training data using the defined workflow.  
6. **Prediction** – Generating predictions on new or unseen data.  
7. **Evaluation** – Assessing model performance using suitable metrics and interpreting the results to inform potential improvements.







## Data

In this example we work with flight information from the United States (package `nycflights13`).  
Our goal is to predict whether an arriving flight is **late** (arrival delay ≥ 30 minutes) or **on time**.

The modeling relationship can be summarized as:

```r
arr_delay ~ (date, airport, distance, ...)
```

```{r}
library(nycflights13)
set.seed(123)

df_Flight <- 
  flights |> 
  mutate(
    arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
    arr_delay = factor(arr_delay),
    date = lubridate::as_date(time_hour)
  ) |> 
  inner_join(weather, by = c("origin", "time_hour")) |> 
  select(dep_time, flight, origin, dest, air_time, distance,
         carrier, date, arr_delay, time_hour) |> 
  na.omit() |> 
  mutate_if(is.character, as.factor)

df_Flight |> 
  skimr::skim(dest, carrier)
```



## Data Split

As usual, we divide the dataset into **training** and **validation** sets.
This allows us to fit the model on one portion of the data and evaluate performance on unseen observations.

Under the `tidymodels` framework:

1. `initial_split()` performs the random split.
2. `training()` and `testing()` extract the corresponding sets.

```{r}
set.seed(222)
split_Flight <- initial_split(df_Flight, prop = 3/4)

df_Train <- training(split_Flight)
df_Validate <- testing(split_Flight)
```



## Creating a Recipe



### Basical Recipe

A **recipe** in `tidymodels` is a structured, pre-defined plan for **data preprocessing and feature engineering**.  
It allows us to declare, in advance, all the steps needed to transform raw data into model-ready features.

When creating a recipe, two fundamental components must be specified:

1. **Formula (`~`)**:  
   The variable on the left-hand side is the **target/outcome**, and the variables on the right-hand side are the **predictors/features**.

2. **Data**:  
   The dataset is used to identify variable names and types. At this stage, no transformations are applied.

```{r}
# Create a basic recipe
rcp_Flight <- recipe(arr_delay ~ ., data = df_Train)
```

Using `.` on the right-hand side indicates that **all remaining variables** in the dataset should be used as predictors.




### Roles

In addition to the standard roles (**outcome** and **predictor**), `recipes` allows defining custom roles using `update_role()`.

Common roles include:

* `predictor`: variables used to predict the outcome
* `outcome`: target variable
* `ID`: identifiers not used for modeling
* `case_weights`: observation weights
* `datetime`, `latitude`, `longitude`: for temporal or spatial feature engineering

```{r}
rcp_Flight <- 
  recipe(arr_delay ~ ., data = df_Train) |> 
  update_role(flight, time_hour, new_role = "ID")

summary(rcp_Flight)
```



## Feature Engineering with `step_*`

### Date Features

Feature engineering transforms raw data into model-ready variables.
`recipes` expresses each transformation as a `step_*()` function.

Below, we extract:

- **day of week** (`dow`)
- **month**
- **holiday indicators** (using U.S. holidays)

```{r}
rcp_Flight <- 
  recipe(arr_delay ~ ., data = df_Train) |> 
  update_role(flight, time_hour, new_role = "ID") |> 
  step_date(date, features = c("dow", "month")) |>               
  step_holiday(
    date,
    holidays = timeDate::listHolidays("US"),
    keep_original_cols = FALSE
  )
```

This replaces the original date with engineered calendar features.



### Categorical Variables (Dummy Encoding)

Most models require converting categorical variables into **dummy/one-hot encoded** predictors.
`recipes` provides:

- `step_novel()`: handles unseen factor levels at prediction time
- `step_dummy()`: converts categorical predictors into numerical dummy variables

```{r}
rcp_Flight <- 
  recipe(arr_delay ~ ., data = df_Train) |> 
  update_role(flight, time_hour, new_role = "ID") |> 
  step_date(date, features = c("dow", "month")) |>               
  step_holiday(
    date, 
    holidays = timeDate::listHolidays("US"), 
    keep_original_cols = FALSE
  ) |> 
  step_novel(all_nominal_predictors()) |>  
  step_dummy(all_nominal_predictors())
```



### Removing Zero-Variance Predictors

Predictors with constant values provide no information and may cause issues in some algorithms.
We remove them using `step_zv()`:

```{r}
rcp_Flight <- 
  recipe(arr_delay ~ ., data = df_Train) |> 
  update_role(flight, time_hour, new_role = "ID") |> 
  step_date(date, features = c("dow", "month")) |>               
  step_holiday(
    date, 
    holidays = timeDate::listHolidays("US"), 
    keep_original_cols = FALSE
  ) |> 
  step_novel(all_nominal_predictors()) |>  
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors())
```



## Combining the Recipe with a Model

### Workflow Definition

In this section, we formalize the integration of data preprocessing and model specification by constructing a **workflow**. This approach ensures that all preprocessing operations defined in the recipe are applied consistently during model training and prediction, thereby maintaining a coherent pipeline.


A `workflow` allows bundling:

1. the **recipe** (preprocessing)
2. the **model** (statistical/ML algorithm)

```{r}
mdl_LogReg <- 
  logistic_reg() |> 
  set_engine("glm")

wflow_Flight <- 
  workflow() |> 
  add_model(mdl_LogReg) |> 
  add_recipe(rcp_Flight)
```



## Model Fitting

We proceed by fitting the complete preprocessing and modeling pipeline to the training data using the `fit()` function.

```{r}
fit_Flight <- 
  wflow_Flight |> 
  fit(data = df_Train)
```

Inspect model coefficients:

```{r}
fit_Flight |> 
  extract_fit_parsnip() |> 
  tidy()
```



## Prediction

Upon fitting, the workflow is employed to generate predictions on the validation dataset.

```{r}
pred_Flight <- predict(fit_Flight, df_Validate)
```

For evaluation we need both class predictions and class probabilities:

```{r}
pred_Flight <- 
  predict(fit_Flight, df_Validate, type = "prob") |> 
  bind_cols(predict(fit_Flight, df_Validate, type = "class")) |> 
  bind_cols(df_Validate |> select(arr_delay))
```



## Evaluation


The predictive performance of the model is evaluated using appropriate metrics, including ROC AUC and accuracy. Additionally, visualization of the ROC curve facilitates an assessment of the classifier’s discriminative capability.

```{r}
pred_Flight |> 
  roc_auc(truth = arr_delay, .pred_late)

pred_Flight |> 
  accuracy(truth = arr_delay, .pred_class)

aug_Flight <- augment(fit_Flight, df_Validate)

aug_Flight |> 
  roc_curve(truth = arr_delay, .pred_late) |> 
  autoplot()
```



## Clean Version of the Code

```{r}
#| eval: false
set.seed(222)  # Set seed for reproducibility

# Split the data into 75% training and 25% validation
split_Flight <- initial_split(df_Flight, prop = 3/4)
df_Train <- training(split_Flight)      # Extract training set
df_Validate <- testing(split_Flight)    # Extract validation set

# -------------------------------
# Create a preprocessing recipe
# -------------------------------
rcp_Flight <- 
  recipe(arr_delay ~ ., data = df_Train) |>    # Define target and predictors
  update_role(flight, time_hour, new_role = "ID") |>  # Assign ID role to identifier columns
  step_date(date, features = c("dow", "month")) |>    # Extract day of week & month from date
  step_holiday(
    date, 
    holidays = timeDate::listHolidays("US"), 
    keep_original_cols = FALSE
  ) |>                                             # Add holiday indicator features
  step_novel(all_nominal_predictors()) |>         # Handle new factor levels in prediction
  step_dummy(all_nominal_predictors()) |>         # Convert categorical predictors to dummy variables
  step_zv(all_predictors())                       # Remove predictors with zero variance

# -------------------------------
# Define the logistic regression model
# -------------------------------
mdl_LogReg <- 
  logistic_reg() |> 
  set_engine("glm")  # Use generalized linear model as backend

# -------------------------------
# Combine recipe and model into a workflow
# -------------------------------
wflow_Flight <- 
  workflow() |> 
  add_model(mdl_LogReg) |> 
  add_recipe(rcp_Flight)

# -------------------------------
# Fit the workflow on the training data
# -------------------------------
fit_Flight <- 
  wflow_Flight |> 
  fit(data = df_Train)

# -------------------------------
# Predict on the validation set (class labels)
# -------------------------------
pred_Flight <- predict(fit_Flight, df_Validate)

# -------------------------------
# Predict on validation set (probabilities and class labels)
# -------------------------------
pred_Flight <- 
  predict(fit_Flight, df_Validate, type = "prob") |>   # Predicted probabilities
  bind_cols(predict(fit_Flight, df_Validate, type = "class")) |>  # Predicted classes
  bind_cols(df_Validate |> select(arr_delay))          # Add true labels for evaluation

# -------------------------------
# Evaluate model performance
# -------------------------------
pred_Flight |> 
  roc_auc(truth = arr_delay, .pred_late)   # ROC AUC for "late" class

pred_Flight |> 
  accuracy(truth = arr_delay, .pred_class) # Classification accuracy

# -------------------------------
# Visualize ROC curve
# -------------------------------
aug_Flight <- augment(fit_Flight, df_Validate)  # Add predictions and residuals to dataset

aug_Flight |> 
  roc_curve(truth = arr_delay, .pred_late) |>  # Compute ROC curve
  autoplot()                                   # Plot ROC curve

```




# Modelling with Resampling

In contrast to the previous exercise, this section introduces model evaluation using resampling techniques, specifically **cross-validation**. This approach incorporates an additional step in which the training dataset is repeatedly partitioned into multiple training–validation splits. By fitting and evaluating the model across these resampled subsets, cross-validation provides a more reliable and robust estimate of the model’s generalization performance than a single train–validate split.

## Evaluating Training and Validation Sets

A fitted model can be evaluated on both the **training** dataset and the **validation** (test) dataset.  
After generating predictions, we can compute performance metrics such as `roc_auc` and `accuracy`.  
For both metrics, **higher values indicate better predictive performance**.

```{r}
library(modeldata) 

# ----------------------------------
# Dataset
# ----------------------------------

## Split the data
set.seed(123)
split_Cell <- initial_split(
  modeldata::cells |> select(-case), 
  strata = class
)

df_Train_Cell <- training(split_Cell)
df_Validate_Cell  <- testing(split_Cell)

# ----------------------------------
# Model
# ----------------------------------

## Specify a random forest classifier
mdl_RandomForest <- 
  rand_forest(trees = 1000) |> 
  set_engine("ranger") |> 
  set_mode("classification")

## Fit the model
set.seed(234)
fit_RandomForest <- 
  mdl_RandomForest |> 
  fit(class ~ ., data = df_Train_Cell)
fit_RandomForest

# ----------------------------------
# Predictions
# ----------------------------------

## Training set predictions
pred_Train_Cell <- 
  predict(fit_RandomForest, df_Train_Cell) |> 
  bind_cols(predict(fit_RandomForest, df_Train_Cell, type = "prob")) |> 
  bind_cols(df_Train_Cell |> select(class))

## Validation set predictions
pred_Validate_Cell <- 
  predict(fit_RandomForest, df_Validate_Cell) |> 
  bind_cols(predict(fit_RandomForest, df_Validate_Cell, type = "prob")) |> 
  bind_cols(df_Validate_Cell |> select(class))

# ----------------------------------
# Performance
# ----------------------------------

## Training
pred_Train_Cell |> roc_auc(truth = class, .pred_PS)
pred_Train_Cell |> accuracy(truth = class, .pred_class)

## Validation
pred_Validate_Cell |> roc_auc(truth = class, .pred_PS)
pred_Validate_Cell |> accuracy(truth = class, .pred_class)
```

The results typically show a **large gap** between training and validation performance.
This is normal: the model tends to overfit the training data, leading to reduced predictive ability on new, unseen data.
This is precisely why a simple train/test split is often insufficient.
To obtain more reliable and stable performance estimates, we use **resampling methods**.



## Resampling (Cross-Validation)

Cross-validation improves model evaluation by repeatedly splitting the training data into several **folds**.
Each fold is used once as an **assessment** (test) set, while the remaining folds form the **analysis** (training) set.

This process reduces the variance in performance estimation and helps detect overfitting.

```{r}
set.seed(345)
folds_Cell <- vfold_cv(df_Train_Cell, v = 10)

wflow_CrossValidate <- 
  workflow() |>
  add_model(mdl_RandomForest) |>
  add_formula(class ~ .)

set.seed(456)
fit_CrossValidate <- 
  wflow_CrossValidate |> 
  fit_resamples(folds_Cell)

collect_metrics(fit_CrossValidate)
```

Compared with normal model fitting, resampling uses the specialized function:

* `fit_resamples()`
  which fits the model on each fold and aggregates the performance results.

Cross-validation therefore provides a more robust estimate of the model’s generalization ability and reduces the risk of overfitting, compared with a single train/test split.













